## Baseline RAG Configuration
## 이 파일은 모든 실험의 기준점이 되는 베이스라인 설정입니다.

experiment_name: "baseline_rag"
description: "Baseline RAG pipeline with fixed chunking and OpenAI embeddings"

# ===========================================
# 데이터 설정 - 전체 데이터 사용 (원천+라벨링 통합)
# ===========================================
data:
  use_real_data: true           # 실제 형사법 데이터 사용
  path: "data/processed/v1/"    # 전처리된 데이터 경로
  version: "v1"

  # 원천데이터 + 라벨링데이터 통합 사용
  use_source: true              # 원천데이터 사용 (TS_법령, TS_판결문 등)
  use_labeled: false            # 라벨링데이터 사용 안 함 (인덱싱용은 원천만)

  # 데이터 스플릿 설정
  split: 'all'                  # 'training', 'validation', 'all' (권장: all로 지식 최대화)

  # 사용할 데이터 타입 (null = 전체 사용)
  source_types: ['법령', '판결문', '결정례', '해석례']
  labeled_types: ['법령_QA', '판결문_QA', '결정례_QA', '해석례_QA', '판결문_SUM', '결정례_SUM', '해석례_SUM']

  # 타입별 최대 문서 수 (null = 제한 없음, 전체 사용)
  # 예: 100 → 각 타입에서 최대 100개씩만 로드
  # 예: null → 모든 데이터 로드 (법령_QA 34,919개 + 판결문_QA 7,025개 등)
  max_per_type: 10            # 기본: 각 타입별 최대 10개 문서 로드

  # 전체 데이터 규모:
  # - 법령_QA: 34,919개
  # - 판결문_QA: 7,025개
  # - 결정례_QA: 5,426개
  # - 해석례_QA: 64개
  # + 원천데이터 (법령, 판결문, 결정례, 해석례)

# 청킹 전략
chunking:
  strategy: "fixed"  # fixed | semantic | recursive | sliding_window
  chunk_size: 512    # 토큰 또는 문자 수
  overlap: 50        # 청크 간 겹침
  use_token_count: false  # true면 토큰 기준, false면 문자 기준

# 임베딩 모델
embedding:
  type: "huggingface"  # openai | huggingface
  # model: "text-embedding-ada-002"  # OpenAI (유료, API 필요)
  model: "BAAI/bge-m3"  # 2024-2025 최고 다국어 모델 (한국어 우수)
  # model: "dragonkue/BGE-m3-ko"  # 한국어 특화 버전 (선택사항)
  # api_key: null  # .env 파일에서 OPENAI_API_KEY 읽음
  device: "mps"  # Mac M1/M4는 "mps", Windows/Linux는 "cpu", NVIDIA GPU는 "cuda"
  batch_size: 8  # M1 Mac MPS 안정성을 위해 배치 크기 축소 (기본 32 → 8)

# 벡터 저장소
vector_store:
  type: "faiss"  # faiss | chroma
  index_type: "flat"  # FAISS: flat | ivf
  persist_directory: "experiments/baseline/vector_store/"
  collection_name: "legal_docs"  # Chroma용
  index_path: "experiments/indexed_data/"  # 인덱스 저장 경로

# 캐시 설정 (NEW!)
cache:
  enabled: true         # 캐시 시스템 활성화
  auto_use: true        # 자동으로 호환 캐시 사용
  force_rebuild: false  # 캐시 무시하고 재인덱싱
  cache_dir: "experiments/indexed_data/"

# 검색 전략
retrieval:
  method: "similarity"  # similarity | mmr | hybrid
  top_k: 5  # 검색할 문서 수
  lambda_mult: 0.5  # MMR용: 관련성 vs 다양성 균형 (0-1)
  bm25_weight: 0.3  # Hybrid용: BM25 가중치

# 생성 모델
generation:
  provider: "openai"  # openai | gemini
  model: "gpt-3.5-turbo"
  temperature: 0.7
  max_tokens: 500
  # api_key: null  # .env에서 읽음

# 평가 설정
evaluation:
  metrics: ["precision@k", "recall@k", "mrr", "latency", "cost"]
  eval_set: "data/evaluation/test_qa.json"
  k_values: [1, 3, 5]  # Precision@k, Recall@k를 계산할 k 값들

# 실험 설정
experiment:
  seed: 42
  verbose: true
  save_results: true
  results_dir: "experiments/baseline/results/"