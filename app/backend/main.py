"""
LawLaw Backend Server
FastAPI ê¸°ë°˜ ë°±ì—”ë“œ ì„œë²„ - Ollamaë¥¼ í†µí•œ ë¡œì»¬ LLM ì—°ë™
RAG + Constitutional AI í†µí•©
"""

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import httpx
import json
import logging
from datetime import datetime
import sys
from pathlib import Path
import uuid
import shutil
import asyncio

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€
BASE_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(BASE_DIR))

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from app.backend.core.llm.llm_client import create_llm_client
from app.backend.core.embeddings.embedder import KoreanLegalEmbedder
from app.backend.core.embeddings.vectordb import ChromaVectorDB
from app.backend.core.retrieval.retriever import LegalDocumentRetriever
from app.backend.core.retrieval.bm25_index import BM25Index
from app.backend.core.retrieval.hybrid_retriever import HybridRetriever
from app.backend.core.llm.constitutional_chatbot import ConstitutionalLawChatbot
from app.backend.core.llm.adapter_chatbot import AdapterChatbot
from app.backend.services.file_parser import FileParser
from app.backend.services.case_analyzer import CaseAnalyzer
from app.backend.services.scenario_detector import ScenarioDetector
from app.backend.services.document_generator import DocumentGenerator
from configs.config import config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LawLaw Backend API",
    description="í˜•ì‚¬ë²• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ ë°±ì—”ë“œ API",
    version="0.1.0"
)

# CORS ì„¤ì • (Electron ì•±ê³¼ì˜ í†µì‹ ì„ ìœ„í•´)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "file://"],  # Electronê³¼ React dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
embedder = None
vectordb = None
bm25_index = None
hybrid_retriever = None
constitutional_chatbot = None

try:
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedder = KoreanLegalEmbedder()
    logger.info("Embedder initialized successfully")

    # ë²¡í„° DB ì´ˆê¸°í™” (ê¸°ì¡´ ë°ì´í„° ë¡œë“œ)
    vectordb = ChromaVectorDB(
        persist_directory=str(BASE_DIR / "data" / "vectordb" / "chroma"),
        collection_name="criminal_law_docs"
    )
    logger.info(f"Vector DB loaded with {vectordb.get_count()} documents")

    # BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    bm25_index_path = BASE_DIR / "data" / "vectordb" / "bm25"
    if bm25_index_path.exists():
        bm25_index = BM25Index()
        bm25_index.load(str(bm25_index_path / "bm25_index.pkl"))
        logger.info(f"BM25 index loaded with {bm25_index.get_count()} documents")

    # Semantic Retriever ì´ˆê¸°í™”
    semantic_retriever = LegalDocumentRetriever(embedder=embedder, vectordb=vectordb)

    # Hybrid Retriever ì´ˆê¸°í™” (Semantic + BM25)
    if bm25_index:
        hybrid_retriever = HybridRetriever(
            semantic_retriever=semantic_retriever,
            bm25_index=bm25_index,
            fusion_method='rrf',
            semantic_weight=0.5,
            enable_adaptive_weighting=True
        )
        logger.info("Hybrid Retriever initialized successfully")
    else:
        hybrid_retriever = semantic_retriever
        logger.info("Using Semantic Retriever only (BM25 index not found)")

except Exception as e:
    logger.error(f"Failed to initialize RAG system: {e}")
    logger.info("Will use fallback mode without RAG")

# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI ì‚¬ìš©)
llm_client = None
OPENAI_API_KEY = config.llm.openai_api_key
MODEL_NAME = "gpt-4-turbo-preview"

try:
    llm_client = create_llm_client(
        provider="openai",
        api_key=OPENAI_API_KEY,
        model=MODEL_NAME,
        temperature=0.1,  # ë²•ë¥  ë‹µë³€ì€ ë‚®ì€ temperature
        max_tokens=2000
    )
    logger.info(f"OpenAI LLM client initialized successfully (model={MODEL_NAME})")

    # Constitutional AI ì±—ë´‡ ì´ˆê¸°í™” (Adapter ì§€ì›)
    if hybrid_retriever and llm_client:
        constitutional_chatbot = AdapterChatbot(
            retriever=hybrid_retriever,
            llm_client=llm_client,
            enable_self_critique=True,  # Self-Critique í™œì„±í™”
            critique_threshold=0.5
        )
        logger.info("Adapter-enabled Constitutional AI Chatbot initialized successfully")

except Exception as e:
    logger.warning(f"Failed to initialize Ollama client: {e}")
    logger.info("API will run without LLM support")

# Request/Response ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    context: Optional[str] = None
    temperature: Optional[float] = 0.7

class ChatResponse(BaseModel):
    response: str
    timestamp: str
    model: str

class SearchRequest(BaseModel):
    query: str
    filters: Optional[Dict[str, Any]] = None
    limit: Optional[int] = 10

class SearchResult(BaseModel):
    id: str
    title: str
    type: str
    summary: str
    date: str
    relevance: float
    citation: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    model_status: str
    timestamp: str

@app.get("/")
async def root():
    """API ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "name": "LawLaw Backend API",
        "version": "0.1.0",
        "status": "running"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ ë° ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        # OpenAI API í‚¤ ë° LLM í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ í™•ì¸
        if llm_client and OPENAI_API_KEY:
            model_available = True
        else:
            model_available = False

        return HealthResponse(
            status="healthy" if model_available else "degraded",
            model_status="available" if model_available else "not_configured",
            timestamp=datetime.now().isoformat()
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return HealthResponse(
            status="unhealthy",
            model_status="error",
            timestamp=datetime.now().isoformat()
        )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Constitutional AI ê¸°ë°˜ ë²•ë¥  ì±—ë´‡"""
    if constitutional_chatbot:
        # Constitutional AI ì±—ë´‡ ì‚¬ìš© (RAG + Self-Critique)
        try:
            result = constitutional_chatbot.chat(
                query=request.message,
                top_k=5,
                include_critique_log=False
            )

            # ì¶œì²˜ ì •ë³´ í¬í•¨í•œ ì‘ë‹µ êµ¬ì„±
            response_text = result['answer']
            if result.get('sources'):
                response_text += "\n\nğŸ“š ì°¸ê³  ìë£Œ:\n"
                for i, source in enumerate(result['sources'][:3], 1):
                    metadata = source.get('metadata', {})
                    response_text += f"{i}. {metadata.get('source', 'Unknown')} - {metadata.get('date', '')}\n"

            return ChatResponse(
                response=response_text,
                timestamp=datetime.now().isoformat(),
                model="GPT-4 + Constitutional AI + RAG"
            )

        except Exception as e:
            logger.error(f"Constitutional AI chat error: {e}")
            # Fallback to simple LLM
            if llm_client:
                response_text = llm_client.generate(
                    prompt=request.message,
                    temperature=request.temperature
                )
                return ChatResponse(
                    response=response_text,
                    timestamp=datetime.now().isoformat(),
                    model="GPT-4"
                )
            raise HTTPException(status_code=500, detail=str(e))

    elif llm_client:
        # RAG ì—†ì´ LLMë§Œ ì‚¬ìš© (Fallback)
        try:
            response_text = llm_client.generate(
                prompt=request.message,
                temperature=request.temperature
            )

            return ChatResponse(
                response=response_text,
                timestamp=datetime.now().isoformat(),
                model="GPT-4"
            )

        except Exception as e:
            logger.error(f"Chat error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    else:
        raise HTTPException(status_code=503, detail="Chat service not available")

@app.post("/search", response_model=List[SearchResult])
async def search_legal_documents(request: SearchRequest):
    """Hybrid Search ê¸°ë°˜ ë²•ë¥  ë¬¸ì„œ ê²€ìƒ‰"""
    if hybrid_retriever:
        try:
            # Hybrid Search ì‹¤í–‰ (Semantic + BM25)
            results = hybrid_retriever.retrieve(
                query=request.query,
                top_k=request.limit or 10,
                filter_metadata=request.filters
            )

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_results = []
            for i, result in enumerate(results):
                metadata = result.get('metadata', {})

                # ë¬¸ì„œ íƒ€ì… ê²°ì •
                doc_type = metadata.get('type', 'unknown')
                if 'íŒë¡€' in metadata.get('source', ''):
                    doc_type = 'case'
                elif 'ë²•ë ¹' in metadata.get('source', ''):
                    doc_type = 'law'
                elif 'í•´ì„ë¡€' in metadata.get('source', ''):
                    doc_type = 'interpretation'

                search_results.append(SearchResult(
                    id=str(i + 1),
                    title=metadata.get('title', f"ë¬¸ì„œ {i + 1}"),
                    type=doc_type,
                    summary=result.get('text', '')[:200],  # ìš”ì•½ì€ ì²˜ìŒ 200ì
                    date=metadata.get('date', ''),
                    relevance=min(100, int(result.get('score', 0) * 100)),  # ì ìˆ˜ë¥¼ ë°±ë¶„ìœ¨ë¡œ
                    citation=metadata.get('citation', metadata.get('source', ''))
                ))

            logger.info(f"Search returned {len(search_results)} results for query: {request.query}")
            return search_results

        except Exception as e:
            logger.error(f"Search error: {e}")
            # Fallback to mock data
            return _get_mock_search_results(request.query, request.limit)

    else:
        # RAG ì‹œìŠ¤í…œì´ ì—†ì„ ë•Œ Mock ë°ì´í„° ë°˜í™˜
        return _get_mock_search_results(request.query, request.limit)

def _get_mock_search_results(query: str, limit: int) -> List[SearchResult]:
    """Mock ê²€ìƒ‰ ê²°ê³¼ (Fallback)"""
    mock_results = [
        SearchResult(
            id="1",
            title="ëŒ€ë²•ì› 2023ë„1234 íŒê²°",
            type="case",
            summary="ìœ„ë²•ìˆ˜ì§‘ì¦ê±°ì˜ ì¦ê±°ëŠ¥ë ¥ì— ê´€í•œ íŒë‹¨ ê¸°ì¤€ì„ ì œì‹œí•œ ì‚¬ë¡€",
            date="2023-12-15",
            relevance=95.0,
            citation="ëŒ€ë²•ì› 2023. 12. 15. ì„ ê³  2023ë„1234 íŒê²°"
        ),
        SearchResult(
            id="2",
            title="í˜•ì‚¬ì†Œì†¡ë²• ì œ308ì¡°ì˜2",
            type="law",
            summary="ìœ„ë²•ìˆ˜ì§‘ì¦ê±°ì˜ ë°°ì œ - ì ë²•í•œ ì ˆì°¨ì— ë”°ë¥´ì§€ ì•„ë‹ˆí•˜ê³  ìˆ˜ì§‘í•œ ì¦ê±°ëŠ” ì¦ê±°ë¡œ í•  ìˆ˜ ì—†ë‹¤",
            date="2007-06-01",
            relevance=90.0,
            citation="í˜•ì‚¬ì†Œì†¡ë²• ì œ308ì¡°ì˜2"
        )
    ]
    return mock_results[:limit]

class AnalyzeRequest(BaseModel):
    content: str
    document_type: Optional[str] = None

class AnalyzeResponse(BaseModel):
    analysis: str
    sources: List[Dict[str, Any]]
    timestamp: str

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_document(request: AnalyzeRequest):
    """ë²•ë¥  ë¬¸ì„œ ë¶„ì„ (Constitutional AI ì ìš©)"""
    if constitutional_chatbot:
        try:
            # Constitutional AI ì±—ë´‡ìœ¼ë¡œ ë¬¸ì„œ ë¶„ì„
            analysis_query = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë²•ì  ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{request.content}

ë¶„ì„ í˜•ì‹:
1. ë¬¸ì„œ ìš”ì•½
2. ì£¼ìš” ë²•ì  ìŸì 
3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
4. ì‹¤ë¬´ì  ì‹œì‚¬ì """

            result = constitutional_chatbot.chat(
                query=analysis_query,
                top_k=5,
                include_critique_log=False
            )

            return AnalyzeResponse(
                analysis=result['answer'],
                sources=result.get('sources', []),
                timestamp=datetime.now().isoformat()
            )

        except Exception as e:
            logger.error(f"Constitutional AI analysis error: {e}")
            # Fallback to simple LLM
            if llm_client:
                analysis_text = llm_client.generate(
                    prompt=analysis_query,
                    temperature=0.1
                )
                return AnalyzeResponse(
                    analysis=analysis_text,
                    sources=[],
                    timestamp=datetime.now().isoformat()
                )
            raise HTTPException(status_code=500, detail=str(e))

    elif llm_client:
        # RAG ì—†ì´ LLMë§Œ ì‚¬ìš© (Fallback)
        try:
            prompt = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë²•ì  ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{request.content}

ë¶„ì„ í˜•ì‹:
1. ë¬¸ì„œ ìš”ì•½
2. ì£¼ìš” ë²•ì  ìŸì 
3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
4. ì‹¤ë¬´ì  ì‹œì‚¬ì """

            analysis_text = llm_client.generate(
                prompt=prompt,
                temperature=0.1
            )

            return AnalyzeResponse(
                analysis=analysis_text,
                sources=[],
                timestamp=datetime.now().isoformat()
            )

        except Exception as e:
            logger.error(f"Document analysis error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    else:
        raise HTTPException(status_code=503, detail="Analysis service not available")

# ============================================
# File Upload & Case Management Endpoints
# ============================================

# ì—…ë¡œë“œëœ íŒŒì¼ ì„ì‹œ ì €ì¥ ë””ë ‰í† ë¦¬
UPLOAD_DIR = BASE_DIR / "data" / "uploads"
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# CaseAnalyzer ì´ˆê¸°í™”
case_analyzer = None
if llm_client:
    case_analyzer = CaseAnalyzer(llm_client=llm_client, retriever=hybrid_retriever)
    logger.info("CaseAnalyzer initialized successfully")

# DocumentGenerator ì´ˆê¸°í™”
document_generator = None
if llm_client:
    document_generator = DocumentGenerator(llm_client=llm_client)
    logger.info("DocumentGenerator initialized successfully")

class CaseAnalysisResponse(BaseModel):
    case_id: str
    summary: str
    document_types: List[str]
    issues: List[str]
    key_dates: Dict[str, str]
    parties: Dict[str, str]
    related_cases: List[Dict[str, Any]]
    suggested_case_name: str
    suggested_next_steps: List[str]
    uploaded_files: List[Dict[str, str]]
    scenario: Dict[str, Any]  # ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´

@app.post("/cases/upload")
async def upload_case_files(files: List[UploadFile] = File(...)):
    """
    ì‚¬ê±´ íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„

    Args:
        files: ì—…ë¡œë“œí•  íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (PDF, DOCX, TXT)

    Returns:
        {
            "case_id": "uuid",
            "summary": "ì‚¬ê±´ ìš”ì•½",
            "document_types": ["íŒê²°ë¬¸", "ê³„ì•½ì„œ"],
            "issues": ["ìŸì 1", "ìŸì 2"],
            "key_dates": {"ì„ ê³ ì¼": "2024-01-15"},
            "parties": {"ì›ê³ ": "í™ê¸¸ë™", "í”¼ê³ ": "ê¹€ì² ìˆ˜"},
            "related_cases": [...],
            "suggested_case_name": "AI ì œì•ˆ ì‚¬ê±´ëª…",
            "suggested_next_steps": ["ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"],
            "uploaded_files": [{"filename": "file.pdf", "size": 1024}]
        }
    """
    if not case_analyzer:
        raise HTTPException(status_code=503, detail="Case analyzer not available")

    if not files:
        raise HTTPException(status_code=400, detail="No files uploaded")

    try:
        # ì‚¬ê±´ ID ìƒì„±
        case_id = str(uuid.uuid4())
        case_dir = UPLOAD_DIR / case_id
        case_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ ì €ì¥ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        filenames = []
        file_info = []

        for file in files:
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            file_ext = Path(file.filename).suffix.lower()
            if file_ext not in ['.pdf', '.docx', '.doc', '.txt']:
                raise HTTPException(
                    status_code=400,
                    detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file.filename} (ì§€ì›: PDF, DOCX, TXT)"
                )

            # íŒŒì¼ ì €ì¥
            file_path = case_dir / file.filename
            with open(file_path, "wb") as f:
                content = await file.read()
                f.write(content)

            # íŒŒì¼ ì •ë³´ ì €ì¥
            file_info.append({
                "filename": file.filename,
                "size": len(content),
                "path": str(file_path)
            })

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                text = FileParser.parse_file(str(file_path))
                texts.append(text)
                filenames.append(file.filename)
                logger.info(f"Successfully parsed {file.filename}: {len(text)} characters")
            except Exception as e:
                logger.error(f"Failed to parse {file.filename}: {e}")
                raise HTTPException(
                    status_code=500,
                    detail=f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {file.filename} - {str(e)}"
                )

        # AI ë¶„ì„ ì‹¤í–‰
        logger.info(f"Analyzing {len(texts)} documents for case {case_id}")
        analysis = await case_analyzer.analyze_documents(texts, filenames)

        # ì‹œë‚˜ë¦¬ì˜¤ ìë™ ê°ì§€
        scenario_info = ScenarioDetector.detect_scenario(analysis, filenames)
        logger.info(f"Detected scenario: {scenario_info['scenario_name']} (confidence: {scenario_info['confidence']})")

        # ì‘ë‹µ êµ¬ì„±
        response_dict = {
            "case_id": case_id,
            "summary": analysis.get('summary', ''),
            "document_types": analysis.get('document_types', []),
            "issues": analysis.get('issues', []),
            "key_dates": analysis.get('key_dates', {}),
            "parties": analysis.get('parties', {}),
            "related_cases": analysis.get('related_cases', []),
            "suggested_case_name": analysis.get('suggested_case_name', f"ì‚¬ê±´_{case_id[:8]}"),
            "suggested_next_steps": analysis.get('suggested_next_steps', []),
            "uploaded_files": [{"filename": f["filename"], "size": f["size"]} for f in file_info],
            "scenario": scenario_info  # ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ ì¶”ê°€
        }

        response = CaseAnalysisResponse(**response_dict)

        # ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        analysis_path = case_dir / "analysis.json"
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(response_dict, f, ensure_ascii=False, indent=2)

        logger.info(f"Case analysis completed: {case_id}")
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Case upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/cases/{case_id}")
async def get_case_analysis(case_id: str):
    """
    ì €ì¥ëœ ì‚¬ê±´ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ

    Args:
        case_id: ì‚¬ê±´ ID

    Returns:
        ì €ì¥ëœ ì‚¬ê±´ ë¶„ì„ ê²°ê³¼
    """
    try:
        case_dir = UPLOAD_DIR / case_id
        analysis_path = case_dir / "analysis.json"

        if not analysis_path.exists():
            raise HTTPException(status_code=404, detail="Case not found")

        with open(analysis_path, "r", encoding="utf-8") as f:
            analysis = json.load(f)

        return analysis

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get case error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/cases/{case_id}")
async def delete_case(case_id: str):
    """
    ì‚¬ê±´ ì‚­ì œ

    Args:
        case_id: ì‚¬ê±´ ID

    Returns:
        ì‚­ì œ ê²°ê³¼
    """
    try:
        case_dir = UPLOAD_DIR / case_id

        if not case_dir.exists():
            raise HTTPException(status_code=404, detail="Case not found")

        # ë””ë ‰í† ë¦¬ ì‚­ì œ
        shutil.rmtree(case_dir)

        logger.info(f"Case deleted: {case_id}")
        return {"success": True, "message": f"Case {case_id} deleted successfully"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete case error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/cases")
async def list_cases():
    """
    ëª¨ë“  ì‚¬ê±´ ëª©ë¡ ì¡°íšŒ

    Returns:
        ì‚¬ê±´ ëª©ë¡
    """
    try:
        cases = []

        for case_dir in UPLOAD_DIR.iterdir():
            if case_dir.is_dir():
                analysis_path = case_dir / "analysis.json"
                if analysis_path.exists():
                    with open(analysis_path, "r", encoding="utf-8") as f:
                        analysis = json.load(f)
                    cases.append({
                        "case_id": case_dir.name,
                        "case_name": analysis.get("suggested_case_name", "Unknown"),
                        "summary": analysis.get("summary", "")[:200],
                        "document_count": len(analysis.get("uploaded_files", [])),
                        "created_at": analysis_path.stat().st_ctime
                    })

        # ìƒì„± ì‹œê°„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬
        cases.sort(key=lambda x: x["created_at"], reverse=True)

        return {"cases": cases, "total": len(cases)}

    except Exception as e:
        logger.error(f"List cases error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# Template & Document Generation Endpoints
# ============================================

class DocumentGenerationRequest(BaseModel):
    case_id: str
    template_name: str
    generation_mode: Optional[str] = "quick"  # "quick" or "custom"
    custom_fields: Optional[Dict[str, str]] = None
    user_instructions: Optional[str] = None

class DocumentGenerationResponse(BaseModel):
    document_id: str
    title: str
    content: str
    template_used: str
    metadata: Dict[str, Any]

@app.post("/documents/generate", response_model=DocumentGenerationResponse)
async def generate_document(request: DocumentGenerationRequest):
    """
    í…œí”Œë¦¿ ê¸°ë°˜ ë²•ë¥  ë¬¸ì„œ ìƒì„±

    Args:
        case_id: ì‚¬ê±´ ID
        template_name: í…œí”Œë¦¿ ì´ë¦„ (ì˜ˆ: "ì†Œì¥", "ë‹µë³€ì„œ", "ê³ ì†Œì¥")
        user_instructions: ì‚¬ìš©ì ì¶”ê°€ ì§€ì‹œì‚¬í•­ (ì„ íƒ)

    Returns:
        ìƒì„±ëœ ë¬¸ì„œ ì •ë³´
    """
    if not document_generator:
        raise HTTPException(status_code=503, detail="Document generator not available")

    try:
        # ì‚¬ê±´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        case_dir = UPLOAD_DIR / request.case_id
        analysis_path = case_dir / "analysis.json"

        if not analysis_path.exists():
            raise HTTPException(status_code=404, detail="Case not found")

        with open(analysis_path, "r", encoding="utf-8") as f:
            case_analysis = json.load(f)

        # ë¬¸ì„œ ìƒì„±
        logger.info(f"Generating document '{request.template_name}' for case {request.case_id} (mode: {request.generation_mode})")
        document = await document_generator.generate_document(
            template_name=request.template_name,
            case_analysis=case_analysis,
            generation_mode=request.generation_mode,
            custom_fields=request.custom_fields,
            user_instructions=request.user_instructions
        )

        # ë¬¸ì„œ ID ìƒì„± ë° ì €ì¥
        document_id = str(uuid.uuid4())
        documents_dir = case_dir / "documents"
        documents_dir.mkdir(exist_ok=True)

        document_path = documents_dir / f"{document_id}.json"
        document_with_id = {
            "document_id": document_id,
            "created_at": datetime.now().isoformat(),
            **document
        }

        with open(document_path, "w", encoding="utf-8") as f:
            json.dump(document_with_id, f, ensure_ascii=False, indent=2)

        logger.info(f"Document generated and saved: {document_id}")

        return DocumentGenerationResponse(
            document_id=document_id,
            title=document["title"],
            content=document["content"],
            template_used=document["template_used"],
            metadata=document["metadata"]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{case_id}/{document_id}")
async def get_generated_document(case_id: str, document_id: str):
    """
    ìƒì„±ëœ ë¬¸ì„œ ì¡°íšŒ

    Args:
        case_id: ì‚¬ê±´ ID
        document_id: ë¬¸ì„œ ID

    Returns:
        ìƒì„±ëœ ë¬¸ì„œ ì •ë³´
    """
    try:
        document_path = UPLOAD_DIR / case_id / "documents" / f"{document_id}.json"

        if not document_path.exists():
            raise HTTPException(status_code=404, detail="Document not found")

        with open(document_path, "r", encoding="utf-8") as f:
            document = json.load(f)

        return document

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{case_id}")
async def list_generated_documents(case_id: str):
    """
    ì‚¬ê±´ì˜ ëª¨ë“  ìƒì„± ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ

    Args:
        case_id: ì‚¬ê±´ ID

    Returns:
        ìƒì„±ëœ ë¬¸ì„œ ëª©ë¡
    """
    try:
        documents_dir = UPLOAD_DIR / case_id / "documents"

        if not documents_dir.exists():
            return {"documents": [], "total": 0}

        documents = []
        for doc_file in documents_dir.glob("*.json"):
            with open(doc_file, "r", encoding="utf-8") as f:
                doc = json.load(f)
                documents.append({
                    "document_id": doc.get("document_id"),
                    "title": doc.get("title"),
                    "template_used": doc.get("template_used"),
                    "created_at": doc.get("created_at")
                })

        # ìƒì„± ì‹œê°„ ì—­ìˆœ ì •ë ¬
        documents.sort(key=lambda x: x.get("created_at", ""), reverse=True)

        return {"documents": documents, "total": len(documents)}

    except Exception as e:
        logger.error(f"List documents error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{case_id}/{document_id}")
async def delete_generated_document(case_id: str, document_id: str):
    """
    ìƒì„±ëœ ë¬¸ì„œ ì‚­ì œ

    Args:
        case_id: ì‚¬ê±´ ID
        document_id: ë¬¸ì„œ ID

    Returns:
        ì‚­ì œ ê²°ê³¼
    """
    try:
        document_path = UPLOAD_DIR / case_id / "documents" / f"{document_id}.json"

        if not document_path.exists():
            raise HTTPException(status_code=404, detail="Document not found")

        document_path.unlink()

        logger.info(f"Document deleted: {document_id}")
        return {"success": True, "message": f"Document {document_id} deleted successfully"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/scenarios")
async def list_scenarios():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤ ë° í…œí”Œë¦¿ ëª©ë¡ ì¡°íšŒ

    Returns:
        ì‹œë‚˜ë¦¬ì˜¤ ë° í…œí”Œë¦¿ ëª©ë¡
    """
    return {"scenarios": ScenarioDetector.SCENARIOS}

# ============================================
# Adapter Management Endpoints (QDoRA)
# ============================================

class AdapterLoadRequest(BaseModel):
    adapter_name: str

class AdapterInfoResponse(BaseModel):
    current_adapter: Optional[str]
    is_adapter_loaded: bool
    available_adapters: List[str]
    metrics: Dict[str, Any]

@app.post("/adapter/load")
async def load_adapter(request: AdapterLoadRequest):
    """
    QDoRA Adapter ë¡œë“œ

    Args:
        adapter_name: Adapter ì´ë¦„ (ì˜ˆ: "traffic", "criminal")

    Returns:
        {
            "success": bool,
            "message": str,
            "current_adapter": str
        }
    """
    if not constitutional_chatbot:
        raise HTTPException(status_code=503, detail="Chatbot not available")

    if not isinstance(constitutional_chatbot, AdapterChatbot):
        raise HTTPException(status_code=400, detail="Adapter feature not supported")

    try:
        success = constitutional_chatbot.load_adapter(request.adapter_name)

        if success:
            return {
                "success": True,
                "message": f"Adapter '{request.adapter_name}' loaded successfully",
                "current_adapter": request.adapter_name
            }
        else:
            return {
                "success": False,
                "message": f"Failed to load adapter '{request.adapter_name}'. Check if it exists in Ollama.",
                "current_adapter": None
            }

    except Exception as e:
        logger.error(f"Adapter load error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/adapter/unload")
async def unload_adapter():
    """Adapter ì–¸ë¡œë“œ (Base Modelë¡œ ë³µê·€)"""
    if not constitutional_chatbot:
        raise HTTPException(status_code=503, detail="Chatbot not available")

    if not isinstance(constitutional_chatbot, AdapterChatbot):
        raise HTTPException(status_code=400, detail="Adapter feature not supported")

    try:
        constitutional_chatbot.unload_adapter()

        return {
            "success": True,
            "message": "Adapter unloaded, returned to base model",
            "current_adapter": None
        }

    except Exception as e:
        logger.error(f"Adapter unload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/adapter/list")
async def list_adapters():
    """ì‚¬ìš© ê°€ëŠ¥í•œ Adapter ëª©ë¡ ì¡°íšŒ"""
    if not constitutional_chatbot:
        raise HTTPException(status_code=503, detail="Chatbot not available")

    if not isinstance(constitutional_chatbot, AdapterChatbot):
        return []

    try:
        adapters = constitutional_chatbot.list_available_adapters()
        return adapters

    except Exception as e:
        logger.error(f"List adapters error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/adapter/info", response_model=AdapterInfoResponse)
async def get_adapter_info():
    """í˜„ì¬ Adapter ì •ë³´ ë° ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    if not constitutional_chatbot:
        raise HTTPException(status_code=503, detail="Chatbot not available")

    if not isinstance(constitutional_chatbot, AdapterChatbot):
        return AdapterInfoResponse(
            current_adapter=None,
            is_adapter_loaded=False,
            available_adapters=[],
            metrics={}
        )

    try:
        info = constitutional_chatbot.get_adapter_info()
        return AdapterInfoResponse(**info)

    except Exception as e:
        logger.error(f"Get adapter info error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)