"""
Chat & Search Router
ì±—ë´‡ ë° ê²€ìƒ‰ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["chat"])


class ChatRequest(BaseModel):
    message: str
    context: Optional[str] = None
    temperature: Optional[float] = 0.7


class ChatResponse(BaseModel):
    response: str
    timestamp: str
    model: str


class SearchRequest(BaseModel):
    query: str
    filters: Optional[Dict[str, Any]] = None
    limit: Optional[int] = 10


class SearchResult(BaseModel):
    id: str
    title: str
    type: str
    summary: str
    date: str
    relevance: float
    citation: Optional[str] = None


class AnalyzeRequest(BaseModel):
    content: str
    document_type: Optional[str] = None


class AnalyzeResponse(BaseModel):
    analysis: str
    sources: List[Dict[str, Any]]
    timestamp: str


class RAGChatRequest(BaseModel):
    """RAG ê¸°ë°˜ ì±—ë´‡ ìš”ì²­"""
    query: str
    top_k: Optional[int] = 5
    include_sources: Optional[bool] = True


class RAGChatResponse(BaseModel):
    """RAG ê¸°ë°˜ ì±—ë´‡ ì‘ë‹µ"""
    answer: str
    sources: List[Dict[str, Any]]
    query: str
    model: str
    timestamp: str
    revised: bool = False


def setup_chat_routes(
    constitutional_chatbot,
    llm_client,
    hybrid_retriever,
    openlaw_client=None
):
    """ì±—ë´‡ ë° ê²€ìƒ‰ ë¼ìš°íŠ¸ ì„¤ì •"""

    @router.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """Constitutional AI ê¸°ë°˜ ë²•ë¥  ì±—ë´‡"""
        if constitutional_chatbot:
            try:
                result = constitutional_chatbot.chat(
                    query=request.message,
                    top_k=5,
                    include_critique_log=False
                )

                response_text = result['answer']
                if result.get('sources'):
                    response_text += "\n\nğŸ“š ì°¸ê³  ìë£Œ:\n"
                    for i, source in enumerate(result['sources'][:3], 1):
                        metadata = source.get('metadata', {})
                        response_text += f"{i}. {metadata.get('source', 'Unknown')} - {metadata.get('date', '')}\n"

                return ChatResponse(
                    response=response_text,
                    timestamp=datetime.now().isoformat(),
                    model="GPT-4 + Constitutional AI + RAG"
                )

            except Exception as e:
                logger.error(f"Constitutional AI chat error: {e}")
                if llm_client:
                    response_text = llm_client.generate(
                        prompt=request.message,
                        temperature=request.temperature
                    )
                    return ChatResponse(
                        response=response_text,
                        timestamp=datetime.now().isoformat(),
                        model="GPT-4"
                    )
                raise HTTPException(status_code=500, detail=str(e))

        elif llm_client:
            try:
                response_text = llm_client.generate(
                    prompt=request.message,
                    temperature=request.temperature
                )

                return ChatResponse(
                    response=response_text,
                    timestamp=datetime.now().isoformat(),
                    model="GPT-4"
                )

            except Exception as e:
                logger.error(f"Chat error: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(status_code=503, detail="Chat service not available")

    @router.post("/chat-with-rag", response_model=RAGChatResponse)
    async def chat_with_rag(request: RAGChatRequest):
        """ChromaDB RAG + Constitutional AI ê¸°ë°˜ ë²•ë¥  ì±—ë´‡

        - Hybrid Search (Semantic + BM25, RRF k=60, Adaptive Weighting)
        - Constitutional AI (6 principles + Self-Critique + 3-shot Learning)
        - 388,767ê°œ í˜•ì‚¬ë²• ë¬¸ì„œ ê¸°ë°˜ RAG
        """
        if not constitutional_chatbot:
            raise HTTPException(
                status_code=503,
                detail="Constitutional AI chatbot not available"
            )

        try:
            logger.info(f"RAG Chat request: '{request.query}' (top_k={request.top_k})")

            # Constitutional AI + Hybrid RAGë¡œ ë‹µë³€ ìƒì„±
            result = constitutional_chatbot.chat(
                query=request.query,
                top_k=request.top_k,
                include_critique_log=False
            )

            # ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ…
            sources = []
            if request.include_sources and result.get('sources'):
                for i, source in enumerate(result['sources'], 1):
                    metadata = source.get('metadata', {})
                    sources.append({
                        'rank': i,
                        'source': metadata.get('source', 'Unknown'),
                        'type': metadata.get('type', 'unknown'),
                        'title': metadata.get('title', ''),
                        'case_number': metadata.get('case_number', ''),
                        'date': metadata.get('date', ''),
                        'citation': metadata.get('citation', ''),
                        'text_snippet': source.get('text', '')[:200],
                        'score': source.get('score', 0.0)
                    })

            # Constitutional AIê°€ self-critiqueë¥¼ í†µí•´ ë‹µë³€ì„ ìˆ˜ì •í–ˆëŠ”ì§€ í™•ì¸
            revised = result.get('revised', False)

            return RAGChatResponse(
                answer=result['answer'],
                sources=sources,
                query=request.query,
                model="GPT-4 Turbo + Constitutional AI + Hybrid RAG (388K docs)",
                timestamp=datetime.now().isoformat(),
                revised=revised
            )

        except Exception as e:
            logger.error(f"RAG chat error: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"RAG ì±—ë´‡ ì˜¤ë¥˜: {str(e)}"
            )

    @router.post("/search", response_model=List[SearchResult])
    async def search_legal_documents(request: SearchRequest):
        """Hybrid Search: ì‹¤ì‹œê°„ OpenLaw API + ë¡œì»¬ RAG ê²€ìƒ‰"""
        search_results = []

        # 1. OpenLaw APIë¡œ ì‹¤ì‹œê°„ íŒë¡€ ê²€ìƒ‰
        if openlaw_client:
            try:
                logger.info(f"Searching OpenLaw API for: {request.query}")
                api_precedents = openlaw_client.fetch_supreme_court_precedents(
                    search_keyword=request.query,
                    limit=request.limit or 10
                )

                for i, prec in enumerate(api_precedents):
                    # Use precedent_serial as ID so we can fetch details from OpenLaw API later
                    precedent_id = prec.get('precedent_serial', f"api_{i}")

                    search_results.append(SearchResult(
                        id=precedent_id,
                        title=prec.get('title', prec.get('case_number', '')),
                        type='case',
                        summary=prec.get('summary', '')[:200] if prec.get('summary') else 'íŒë¡€ ìš”ì•½ ì •ë³´ ì—†ìŒ',
                        date=prec.get('decision_date', datetime.now()).strftime('%Y-%m-%d') if isinstance(prec.get('decision_date'), datetime) else str(prec.get('decision_date', '')),
                        relevance=95.0 - (i * 2),  # API ê²°ê³¼ëŠ” ë†’ì€ relevance
                        citation=prec.get('case_number', '')
                    ))

                logger.info(f"OpenLaw API returned {len(api_precedents)} results")
            except Exception as e:
                logger.error(f"OpenLaw API search error: {e}")

        # 2. ë¡œì»¬ RAG ê²€ìƒ‰ (ë³´ì™„)
        if hybrid_retriever:
            try:
                logger.info(f"Searching local RAG for: {request.query}")
                rag_results = hybrid_retriever.retrieve(
                    query=request.query,
                    top_k=5,  # RAGì—ì„œëŠ” 5ê°œë§Œ
                    filter_metadata=request.filters
                )

                for i, result in enumerate(rag_results):
                    metadata = result.get('metadata', {})

                    doc_type = metadata.get('type', 'unknown')
                    if 'íŒë¡€' in metadata.get('source', ''):
                        doc_type = 'case'
                    elif 'ë²•ë ¹' in metadata.get('source', ''):
                        doc_type = 'law'
                    elif 'í•´ì„ë¡€' in metadata.get('source', ''):
                        doc_type = 'interpretation'

                    search_results.append(SearchResult(
                        id=f"rag_{i}",
                        title=metadata.get('title', f"ë¬¸ì„œ {i + 1}"),
                        type=doc_type,
                        summary=result.get('text', '')[:200],
                        date=metadata.get('date', ''),
                        relevance=min(85.0, int(result.get('score', 0) * 100)),  # RAG ê²°ê³¼ëŠ” ì•½ê°„ ë‚®ì€ relevance
                        citation=metadata.get('citation', metadata.get('source', ''))
                    ))

                logger.info(f"Local RAG returned {len(rag_results)} results")
            except Exception as e:
                logger.error(f"RAG search error: {e}")

        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ mock ë°ì´í„°
        if not search_results:
            logger.warning("No results from API or RAG, returning mock data")
            search_results = _get_mock_search_results(request.query, request.limit)

        logger.info(f"Total search results: {len(search_results)}")
        return search_results[:request.limit or 10]

    @router.post("/analyze", response_model=AnalyzeResponse)
    async def analyze_document(request: AnalyzeRequest):
        """ë²•ë¥  ë¬¸ì„œ ë¶„ì„ (Constitutional AI ì ìš©)"""
        if constitutional_chatbot:
            try:
                analysis_query = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë²•ì  ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{request.content}

ë¶„ì„ í˜•ì‹:
1. ë¬¸ì„œ ìš”ì•½
2. ì£¼ìš” ë²•ì  ìŸì 
3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
4. ì‹¤ë¬´ì  ì‹œì‚¬ì """

                result = constitutional_chatbot.chat(
                    query=analysis_query,
                    top_k=5,
                    include_critique_log=False
                )

                return AnalyzeResponse(
                    analysis=result['answer'],
                    sources=result.get('sources', []),
                    timestamp=datetime.now().isoformat()
                )

            except Exception as e:
                logger.error(f"Constitutional AI analysis error: {e}")
                if llm_client:
                    analysis_text = llm_client.generate(
                        prompt=analysis_query,
                        temperature=0.1
                    )
                    return AnalyzeResponse(
                        analysis=analysis_text,
                        sources=[],
                        timestamp=datetime.now().isoformat()
                    )
                raise HTTPException(status_code=500, detail=str(e))

        elif llm_client:
            try:
                prompt = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë²•ì  ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{request.content}

ë¶„ì„ í˜•ì‹:
1. ë¬¸ì„œ ìš”ì•½
2. ì£¼ìš” ë²•ì  ìŸì 
3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
4. ì‹¤ë¬´ì  ì‹œì‚¬ì """

                analysis_text = llm_client.generate(
                    prompt=prompt,
                    temperature=0.1
                )

                return AnalyzeResponse(
                    analysis=analysis_text,
                    sources=[],
                    timestamp=datetime.now().isoformat()
                )

            except Exception as e:
                logger.error(f"Document analysis error: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(status_code=503, detail="Analysis service not available")

    return router


def _get_mock_search_results(query: str, limit: int) -> List[SearchResult]:
    """Mock ê²€ìƒ‰ ê²°ê³¼ (Fallback)"""
    mock_results = [
        SearchResult(
            id="1",
            title="ëŒ€ë²•ì› 2023ë„1234 íŒê²°",
            type="case",
            summary="ìœ„ë²•ìˆ˜ì§‘ì¦ê±°ì˜ ì¦ê±°ëŠ¥ë ¥ì— ê´€í•œ íŒë‹¨ ê¸°ì¤€ì„ ì œì‹œí•œ ì‚¬ë¡€",
            date="2023-12-15",
            relevance=95.0,
            citation="ëŒ€ë²•ì› 2023. 12. 15. ì„ ê³  2023ë„1234 íŒê²°"
        ),
        SearchResult(
            id="2",
            title="í˜•ì‚¬ì†Œì†¡ë²• ì œ308ì¡°ì˜2",
            type="law",
            summary="ìœ„ë²•ìˆ˜ì§‘ì¦ê±°ì˜ ë°°ì œ - ì ë²•í•œ ì ˆì°¨ì— ë”°ë¥´ì§€ ì•„ë‹ˆí•˜ê³  ìˆ˜ì§‘í•œ ì¦ê±°ëŠ” ì¦ê±°ë¡œ í•  ìˆ˜ ì—†ë‹¤",
            date="2007-06-01",
            relevance=90.0,
            citation="í˜•ì‚¬ì†Œì†¡ë²• ì œ308ì¡°ì˜2"
        )
    ]
    return mock_results[:limit]
