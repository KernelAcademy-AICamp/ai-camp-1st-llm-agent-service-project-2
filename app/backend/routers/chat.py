"""
Chat & Search Router
ì±—ë´‡ ë° ê²€ìƒ‰ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from datetime import datetime
import logging
from sqlalchemy.ext.asyncio import AsyncSession

from app.backend.database import get_db
from app.backend.core.retrieval.feedback_filter import get_excluded_precedent_ids

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["chat"])


class ChatRequest(BaseModel):
    message: str
    context: Optional[str] = None
    temperature: Optional[float] = 0.7


class ChatResponse(BaseModel):
    response: str
    timestamp: str
    model: str


class SearchRequest(BaseModel):
    query: str
    filters: Optional[Dict[str, Any]] = None
    limit: Optional[int] = 10


class SearchResult(BaseModel):
    id: str
    title: str
    type: str
    summary: str
    date: str
    relevance: float
    citation: Optional[str] = None


class AnalyzeRequest(BaseModel):
    content: str
    document_type: Optional[str] = None


class AnalyzeResponse(BaseModel):
    analysis: str
    sources: List[Dict[str, Any]]
    timestamp: str


class RAGChatRequest(BaseModel):
    """RAG ê¸°ë°˜ ì±—ë´‡ ìš”ì²­"""
    query: str
    top_k: Optional[int] = 5
    include_sources: Optional[bool] = True


class RAGChatResponse(BaseModel):
    """RAG ê¸°ë°˜ ì±—ë´‡ ì‘ë‹µ"""
    answer: str
    sources: List[Dict[str, Any]]
    query: str
    model: str
    timestamp: str
    revised: bool = False


def setup_chat_routes(
    constitutional_chatbot,
    llm_client,
    hybrid_retriever,
    openlaw_client=None
):
    """ì±—ë´‡ ë° ê²€ìƒ‰ ë¼ìš°íŠ¸ ì„¤ì •"""

    @router.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """Constitutional AI ê¸°ë°˜ ë²•ë¥  ì±—ë´‡"""
        if constitutional_chatbot:
            try:
                result = constitutional_chatbot.chat(
                    query=request.message,
                    top_k=5,
                    include_critique_log=False
                )

                response_text = result['answer']
                if result.get('sources'):
                    response_text += "\n\nğŸ“š ì°¸ê³  ìë£Œ:\n"
                    for i, source in enumerate(result['sources'][:3], 1):
                        metadata = source.get('metadata', {})
                        response_text += f"{i}. {metadata.get('source', 'Unknown')} - {metadata.get('date', '')}\n"

                return ChatResponse(
                    response=response_text,
                    timestamp=datetime.now().isoformat(),
                    model="GPT-4 + Constitutional AI + RAG"
                )

            except Exception as e:
                logger.error(f"Constitutional AI chat error: {e}")
                if llm_client:
                    response_text = llm_client.generate(
                        prompt=request.message,
                        temperature=request.temperature
                    )
                    return ChatResponse(
                        response=response_text,
                        timestamp=datetime.now().isoformat(),
                        model="GPT-4"
                    )
                raise HTTPException(status_code=500, detail=str(e))

        elif llm_client:
            try:
                response_text = llm_client.generate(
                    prompt=request.message,
                    temperature=request.temperature
                )

                return ChatResponse(
                    response=response_text,
                    timestamp=datetime.now().isoformat(),
                    model="GPT-4"
                )

            except Exception as e:
                logger.error(f"Chat error: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(status_code=503, detail="Chat service not available")

    @router.post("/chat-with-rag", response_model=RAGChatResponse)
    async def chat_with_rag(request: RAGChatRequest, db: AsyncSession = Depends(get_db)):
        """ChromaDB RAG + Constitutional AI ê¸°ë°˜ ë²•ë¥  ì±—ë´‡

        - Hybrid Search (Semantic + BM25, RRF k=60, Adaptive Weighting)
        - Constitutional AI (6 principles + Self-Critique + 3-shot Learning)
        - 388,767ê°œ í˜•ì‚¬ë²• ë¬¸ì„œ ê¸°ë°˜ RAG
        - ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ í•„í„°ë§ (ì‹«ì–´ìš”ê°€ ë§ì€ íŒë¡€ ì œì™¸)
        """
        if not constitutional_chatbot:
            raise HTTPException(
                status_code=503,
                detail="Constitutional AI chatbot not available"
            )

        try:
            logger.info(f"RAG Chat request: '{request.query}' (top_k={request.top_k})")

            # ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì œì™¸ íŒë¡€ ID ì¡°íšŒ
            excluded_ids = await get_excluded_precedent_ids(db)
            if excluded_ids:
                logger.info(f"Filtering out {len(excluded_ids)} precedents based on user feedback")

            # Constitutional AI + Hybrid RAGë¡œ ë‹µë³€ ìƒì„±
            # top_kë¥¼ ì¦ê°€ì‹œì¼œì„œ ì¤‘ë³µ ì œê±° ë° í•„í„°ë§ í›„ì—ë„ ì¶©ë¶„í•œ ë¬¸ì„œ í™•ë³´
            # ì²­í¬ ì¤‘ë³µì„ ê³ ë ¤í•˜ì—¬ 2ë°°ë¡œ ê²€ìƒ‰ (ì˜ˆ: top_k=5 -> ê²€ìƒ‰ 10ê°œ -> ì¤‘ë³µ ì œê±° í›„ ~5ê°œ)
            retrieval_top_k = request.top_k * 2
            if excluded_ids:
                retrieval_top_k += len(excluded_ids)
                logger.info(f"Retrieving {retrieval_top_k} chunks (accounting for {len(excluded_ids)} excluded + deduplication)")

            result = constitutional_chatbot.chat(
                query=request.query,
                top_k=retrieval_top_k,
                include_critique_log=False
            )

            # í”¼ë“œë°± ê¸°ë°˜ í•„í„°ë§ ì ìš©
            filtered_sources = result.get('sources', [])
            if excluded_ids and filtered_sources:
                original_count = len(filtered_sources)
                # metadata.sourceë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
                filtered_sources = [
                    s for s in filtered_sources
                    if s.get('metadata', {}).get('source') not in excluded_ids
                ]
                if len(filtered_sources) < original_count:
                    logger.info(f"Filtered {original_count - len(filtered_sources)} sources based on feedback")

            # ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ… (ì²­í¬ ì¤‘ë³µ ì œê±°)
            sources = []
            seen_sources = set()  # ì´ë¯¸ ì²˜ë¦¬ëœ source ID ì¶”ì 

            if request.include_sources and filtered_sources:
                rank = 1
                for source in filtered_sources:
                    metadata = source.get('metadata', {})
                    source_id = metadata.get('source', 'Unknown')

                    # ê°™ì€ íŒë¡€ì˜ ì²­í¬ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ì²­í¬ë§Œ ìœ ì§€)
                    if source_id in seen_sources:
                        logger.debug(f"Skipping duplicate source: {source_id}")
                        continue

                    seen_sources.add(source_id)

                    # Source ID íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ì¶”ë¡ 
                    doc_type = metadata.get('type', 'unknown')
                    source_str = metadata.get('source', '')

                    # Source IDë‚˜ ë©”íƒ€ë°ì´í„°ë¡œ íƒ€ì… ì¶”ë¡ 
                    if '_P_' in source_str or 'íŒë¡€' in source_str:
                        doc_type = 'case'
                    elif 'ë²•ë ¹' in source_str:
                        doc_type = 'law'
                    elif 'í•´ì„ë¡€' in source_str:
                        doc_type = 'interpretation'
                    elif doc_type == 'unknown':
                        # ê¸°ë³¸ê°’: í˜•ì‚¬ë²• ë°ì´í„°ëŠ” ëŒ€ë¶€ë¶„ íŒë¡€
                        doc_type = 'case'

                    sources.append({
                        'rank': rank,
                        'source': source_id,
                        'type': doc_type,
                        'title': metadata.get('title', ''),
                        'case_number': metadata.get('case_number', ''),
                        'date': metadata.get('date', ''),
                        'citation': metadata.get('citation', ''),
                        'text_snippet': source.get('text', '')[:200],
                        'score': source.get('score', 0.0)
                    })
                    rank += 1

                    # top_k ê°œìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    if len(sources) >= request.top_k:
                        break

                logger.info(f"Deduplicated sources: {len(filtered_sources)} chunks -> {len(sources)} unique precedents")

            # Constitutional AIê°€ self-critiqueë¥¼ í†µí•´ ë‹µë³€ì„ ìˆ˜ì •í–ˆëŠ”ì§€ í™•ì¸
            revised = result.get('revised', False)

            return RAGChatResponse(
                answer=result['answer'],
                sources=sources,
                query=request.query,
                model="GPT-4 Turbo + Constitutional AI + Hybrid RAG (388K docs)",
                timestamp=datetime.now().isoformat(),
                revised=revised
            )

        except Exception as e:
            logger.error(f"RAG chat error: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"RAG ì±—ë´‡ ì˜¤ë¥˜: {str(e)}"
            )

    @router.post("/search", response_model=List[SearchResult])
    async def search_legal_documents(request: SearchRequest):
        """Hybrid Search: ì‹¤ì‹œê°„ OpenLaw API + ë¡œì»¬ RAG ê²€ìƒ‰"""
        search_results = []

        # 1. OpenLaw APIë¡œ ì‹¤ì‹œê°„ íŒë¡€ ê²€ìƒ‰
        if openlaw_client:
            try:
                logger.info(f"Searching OpenLaw API for: {request.query}")
                api_precedents = openlaw_client.fetch_supreme_court_precedents(
                    search_keyword=request.query,
                    limit=request.limit or 10
                )

                for i, prec in enumerate(api_precedents):
                    # Use precedent_serial as ID so we can fetch details from OpenLaw API later
                    precedent_id = prec.get('precedent_serial', f"api_{i}")

                    search_results.append(SearchResult(
                        id=precedent_id,
                        title=prec.get('title', prec.get('case_number', '')),
                        type='case',
                        summary=prec.get('summary', '')[:200] if prec.get('summary') else 'íŒë¡€ ìš”ì•½ ì •ë³´ ì—†ìŒ',
                        date=prec.get('decision_date', datetime.now()).strftime('%Y-%m-%d') if isinstance(prec.get('decision_date'), datetime) else str(prec.get('decision_date', '')),
                        relevance=95.0 - (i * 2),  # API ê²°ê³¼ëŠ” ë†’ì€ relevance
                        citation=prec.get('case_number', '')
                    ))

                logger.info(f"OpenLaw API returned {len(api_precedents)} results")
            except Exception as e:
                logger.error(f"OpenLaw API search error: {e}")

        # 2. ë¡œì»¬ RAG ê²€ìƒ‰ (ë³´ì™„)
        if hybrid_retriever:
            try:
                logger.info(f"Searching local RAG for: {request.query}")
                rag_results = hybrid_retriever.retrieve(
                    query=request.query,
                    top_k=5,  # RAGì—ì„œëŠ” 5ê°œë§Œ
                    filter_metadata=request.filters
                )

                for i, result in enumerate(rag_results):
                    metadata = result.get('metadata', {})

                    doc_type = metadata.get('type', 'unknown')
                    if 'íŒë¡€' in metadata.get('source', ''):
                        doc_type = 'case'
                    elif 'ë²•ë ¹' in metadata.get('source', ''):
                        doc_type = 'law'
                    elif 'í•´ì„ë¡€' in metadata.get('source', ''):
                        doc_type = 'interpretation'

                    search_results.append(SearchResult(
                        id=f"rag_{i}",
                        title=metadata.get('title', f"ë¬¸ì„œ {i + 1}"),
                        type=doc_type,
                        summary=result.get('text', '')[:200],
                        date=metadata.get('date', ''),
                        relevance=min(85.0, int(result.get('score', 0) * 100)),  # RAG ê²°ê³¼ëŠ” ì•½ê°„ ë‚®ì€ relevance
                        citation=metadata.get('citation', metadata.get('source', ''))
                    ))

                logger.info(f"Local RAG returned {len(rag_results)} results")
            except Exception as e:
                logger.error(f"RAG search error: {e}")

        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ mock ë°ì´í„°
        if not search_results:
            logger.warning("No results from API or RAG, returning mock data")
            search_results = _get_mock_search_results(request.query, request.limit)

        logger.info(f"Total search results: {len(search_results)}")
        return search_results[:request.limit or 10]

    @router.get("/document/{source_id}")
    async def get_document_detail(source_id: str):
        """ë¬¸ì„œ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì „ì²´ í…ìŠ¤íŠ¸ í¬í•¨)

        Args:
            source_id: ë¬¸ì„œ ì†ŒìŠ¤ ID (ì˜ˆ: HS_P_338200)

        Returns:
            ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° (ëª¨ë“  ì²­í¬ ê²°í•©)
        """
        if not hybrid_retriever:
            raise HTTPException(
                status_code=503,
                detail="Document retrieval service not available"
            )

        try:
            logger.info(f"Fetching document detail for: {source_id}")

            # ChromaDBì—ì„œ í•´ë‹¹ source_idë¥¼ ê°€ì§„ ëª¨ë“  ì²­í¬ ì¡°íšŒ
            results = hybrid_retriever.semantic_retriever.vectordb.collection.get(
                where={"source": source_id},
                include=["documents", "metadatas"]
            )

            if not results or not results['ids']:
                # source_idê°€ ì •í™•í•˜ì§€ ì•Šìœ¼ë©´ ë¶€ë¶„ ê²€ìƒ‰ ì‹œë„
                all_results = hybrid_retriever.semantic_retriever.vectordb.collection.get(
                    include=["documents", "metadatas"]
                )

                # source_idê°€ í¬í•¨ëœ ë¬¸ì„œ ì°¾ê¸°
                matching_chunks = []
                for i, metadata in enumerate(all_results['metadatas']):
                    if source_id in metadata.get('source', ''):
                        matching_chunks.append({
                            'id': all_results['ids'][i],
                            'document': all_results['documents'][i],
                            'metadata': metadata
                        })

                if not matching_chunks:
                    raise HTTPException(
                        status_code=404,
                        detail=f"Document not found: {source_id}"
                    )

                # ì²­í¬ ì •ë ¬ ë° ê²°í•©
                matching_chunks.sort(key=lambda x: int(x['metadata'].get('chunk_id', 0)))
                full_text = '\n\n'.join([chunk['document'] for chunk in matching_chunks])

                return {
                    'id': matching_chunks[0]['id'],
                    'source': matching_chunks[0]['metadata'].get('source', ''),
                    'title': matching_chunks[0]['metadata'].get('title', ''),
                    'type': matching_chunks[0]['metadata'].get('type', 'unknown'),
                    'case_number': matching_chunks[0]['metadata'].get('case_number', ''),
                    'date': matching_chunks[0]['metadata'].get('date', ''),
                    'citation': matching_chunks[0]['metadata'].get('citation', ''),
                    'full_text': full_text,
                    'metadata': matching_chunks[0]['metadata']
                }

            # ëª¨ë“  ì²­í¬ë¥¼ chunk_idë¡œ ì •ë ¬
            chunks = []
            for i in range(len(results['ids'])):
                chunks.append({
                    'id': results['ids'][i],
                    'document': results['documents'][i],
                    'metadata': results['metadatas'][i]
                })

            # chunk_idë¡œ ì •ë ¬ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼)
            chunks.sort(key=lambda x: int(x['metadata'].get('chunk_id', 0)))

            # ëª¨ë“  ì²­í¬ì˜ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©
            full_text = '\n\n'.join([chunk['document'] for chunk in chunks])

            # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš© (ëª¨ë“  ì²­í¬ê°€ ê°™ì€ ë©”íƒ€ë°ì´í„° ê³µìœ )
            metadata = chunks[0]['metadata']

            logger.info(f"Combined {len(chunks)} chunks for document: {source_id}")

            return {
                'id': chunks[0]['id'],
                'source': metadata.get('source', ''),
                'title': metadata.get('title', ''),
                'type': metadata.get('type', 'unknown'),
                'case_number': metadata.get('case_number', ''),
                'date': metadata.get('date', ''),
                'citation': metadata.get('citation', ''),
                'full_text': full_text,
                'metadata': metadata
            }

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Document detail error: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"ë¬¸ì„œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"
            )

    @router.post("/analyze", response_model=AnalyzeResponse)
    async def analyze_document(request: AnalyzeRequest):
        """ë²•ë¥  ë¬¸ì„œ ë¶„ì„ (Constitutional AI ì ìš©)"""
        if constitutional_chatbot:
            try:
                analysis_query = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë²•ì  ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{request.content}

ë¶„ì„ í˜•ì‹:
1. ë¬¸ì„œ ìš”ì•½
2. ì£¼ìš” ë²•ì  ìŸì 
3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
4. ì‹¤ë¬´ì  ì‹œì‚¬ì """

                result = constitutional_chatbot.chat(
                    query=analysis_query,
                    top_k=5,
                    include_critique_log=False
                )

                return AnalyzeResponse(
                    analysis=result['answer'],
                    sources=result.get('sources', []),
                    timestamp=datetime.now().isoformat()
                )

            except Exception as e:
                logger.error(f"Constitutional AI analysis error: {e}")
                if llm_client:
                    analysis_text = llm_client.generate(
                        prompt=analysis_query,
                        temperature=0.1
                    )
                    return AnalyzeResponse(
                        analysis=analysis_text,
                        sources=[],
                        timestamp=datetime.now().isoformat()
                    )
                raise HTTPException(status_code=500, detail=str(e))

        elif llm_client:
            try:
                prompt = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ë²•ì  ìŸì ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{request.content}

ë¶„ì„ í˜•ì‹:
1. ë¬¸ì„œ ìš”ì•½
2. ì£¼ìš” ë²•ì  ìŸì 
3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
4. ì‹¤ë¬´ì  ì‹œì‚¬ì """

                analysis_text = llm_client.generate(
                    prompt=prompt,
                    temperature=0.1
                )

                return AnalyzeResponse(
                    analysis=analysis_text,
                    sources=[],
                    timestamp=datetime.now().isoformat()
                )

            except Exception as e:
                logger.error(f"Document analysis error: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(status_code=503, detail="Analysis service not available")

    return router


def _get_mock_search_results(query: str, limit: int) -> List[SearchResult]:
    """Mock ê²€ìƒ‰ ê²°ê³¼ (Fallback)"""
    mock_results = [
        SearchResult(
            id="1",
            title="ëŒ€ë²•ì› 2023ë„1234 íŒê²°",
            type="case",
            summary="ìœ„ë²•ìˆ˜ì§‘ì¦ê±°ì˜ ì¦ê±°ëŠ¥ë ¥ì— ê´€í•œ íŒë‹¨ ê¸°ì¤€ì„ ì œì‹œí•œ ì‚¬ë¡€",
            date="2023-12-15",
            relevance=95.0,
            citation="ëŒ€ë²•ì› 2023. 12. 15. ì„ ê³  2023ë„1234 íŒê²°"
        ),
        SearchResult(
            id="2",
            title="í˜•ì‚¬ì†Œì†¡ë²• ì œ308ì¡°ì˜2",
            type="law",
            summary="ìœ„ë²•ìˆ˜ì§‘ì¦ê±°ì˜ ë°°ì œ - ì ë²•í•œ ì ˆì°¨ì— ë”°ë¥´ì§€ ì•„ë‹ˆí•˜ê³  ìˆ˜ì§‘í•œ ì¦ê±°ëŠ” ì¦ê±°ë¡œ í•  ìˆ˜ ì—†ë‹¤",
            date="2007-06-01",
            relevance=90.0,
            citation="í˜•ì‚¬ì†Œì†¡ë²• ì œ308ì¡°ì˜2"
        )
    ]
    return mock_results[:limit]
