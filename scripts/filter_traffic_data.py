"""
êµí†µ ê´€ë ¨ ë²•ë¥  ë°ì´í„° í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/filter_traffic_data.py

ê¸°ëŠ¥:
    - ì›ë³¸ í˜•ì‚¬ë²• ë°ì´í„°ì—ì„œ êµí†µ ê´€ë ¨ ë°ì´í„°ë§Œ ì¶”ì¶œ
    - í´ë” êµ¬ì¡° ìœ ì§€
    - CSV(ì›ì²œë°ì´í„°)ì™€ JSON(ë¼ë²¨ë§ë°ì´í„°) ëª¨ë‘ ì²˜ë¦¬
"""

import os
import csv
import json
import shutil
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm

# ============================================================================
# ì„¤ì •: ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤
# ============================================================================

# êµí†µ ê´€ë ¨ í‚¤ì›Œë“œ (í•„ìš”ì— ë”°ë¼ ì¶”ê°€/ì‚­ì œ)
TRAFFIC_KEYWORDS = [
    'êµí†µ', 'ë„ë¡œêµí†µë²•', 'ë„ë¡œ', 'ìš´ì „', 'ìë™ì°¨', 'ì°¨ëŸ‰', 'ìŒì£¼ìš´ì „',
    'ë¬´ë©´í—ˆ', 'ì‹ í˜¸ìœ„ë°˜', 'ì†ë„ìœ„ë°˜', 'ì£¼ì •ì°¨', 'êµí†µì‚¬ê³ ', 'ë‚œí­ìš´ì „',
    'í†µí–‰', 'ì£¼ì°¨', 'ì •ì°¨', 'ë³´í–‰ì', 'íš¡ë‹¨ë³´ë„', 'ì‹ í˜¸ë“±', 'ë©´í—ˆ',
    'ì‚¬ê³ ', 'ì¶©ëŒ', 'ì ‘ì´‰', 'ê³¼ì†', 'ì•ˆì „ê±°ë¦¬', 'ì°¨ì„ ', 'ì¤‘ì•™ì„ ',
    'ì¶”ì›”', 'ê¸‰ì •ê±°', 'ê¸‰ì¶œë°œ', 'ì •ì§€ì„ ', 'ì¼ì‹œì •ì§€', 'ìŒì£¼', 'í˜ˆì¤‘ì•Œì½”ì˜¬',
    'ìš´ì „ë©´í—ˆ', 'ì´ë¥œì°¨', 'ì˜¤í† ë°”ì´', 'ë²„ìŠ¤', 'íŠ¸ëŸ­', 'í™”ë¬¼ì°¨',
    'ê²¬ì¸', 'ì£¼í–‰', 'ìš´í–‰', 'ì •ì°¨ìœ„ë°˜', 'ì£¼ì°¨ìœ„ë°˜', 'ì†ë„ì œí•œ',
]

# ì›ë³¸ ë°ì´í„° ê²½ë¡œ
SOURCE_BASE = "04.í˜•ì‚¬ë²• LLM ì‚¬ì „í•™ìŠµ ë° Instruction Tuning ë°ì´í„°/3.ê°œë°©ë°ì´í„°/1.ë°ì´í„°"

# ì¶œë ¥ ë°ì´í„° ê²½ë¡œ
OUTPUT_BASE = "êµí†µë²• ë°ì´í„°/3.ê°œë°©ë°ì´í„°/1.ë°ì´í„°"

# ============================================================================
# í•„í„°ë§ ë¡œì§
# ============================================================================

def contains_traffic_keyword(text):
    """í…ìŠ¤íŠ¸ì— êµí†µ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    text = str(text).lower()
    return any(keyword.lower() in text for keyword in TRAFFIC_KEYWORDS)


def filter_csv_files(source_dir, output_dir, data_type):
    """CSV íŒŒì¼ì—ì„œ êµí†µ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§ (ë¬¸ì„œ ë‹¨ìœ„)"""

    source_path = Path(source_dir) / data_type
    output_path = Path(output_dir) / data_type

    if not source_path.exists():
        print(f"âš ï¸  ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")
        return

    output_path.mkdir(parents=True, exist_ok=True)

    csv_files = list(source_path.glob("*.csv"))
    print(f"\nğŸ“„ {data_type} ì²˜ë¦¬ ì¤‘... (ì´ {len(csv_files)}ê°œ íŒŒì¼)")

    total_docs = 0
    filtered_docs = 0
    skipped_files = 0

    for csv_file in tqdm(csv_files, desc=f"  {data_type}"):
        try:
            # CSV íŒŒì¼ ì½ê¸° ë° ë¬¸ì„œë³„ ê·¸ë£¹í™”
            document_groups = defaultdict(list)

            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # ìˆ˜ì •: íŒê²°ë¬¸ì€ 'íŒë¡€ì¼ë ¨ë²ˆí˜¸' ì‚¬ìš©
                    doc_id = (row.get('ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸') or
                             row.get('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸') or
                             row.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸') or  # íŒê²°ë¬¸ìš© (ìˆ˜ì •ë¨!)
                             row.get('í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸'))
                    if doc_id:
                        document_groups[doc_id].append(row)

            # ë¬¸ì„œ IDë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê²½ê³ 
            if not document_groups:
                skipped_files += 1
                continue

            # êµí†µ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§
            traffic_docs = {}
            for doc_id, rows in document_groups.items():
                total_docs += 1
                # ë¬¸ì„œ ë‚´ ëª¨ë“  í–‰ì˜ ë‚´ìš©ì„ ê²€ì‚¬
                full_text = ' '.join([row.get('ë‚´ìš©', '') for row in rows])
                if contains_traffic_keyword(full_text):
                    traffic_docs[doc_id] = rows
                    filtered_docs += 1

            # í•„í„°ë§ëœ ë¬¸ì„œë§Œ ìƒˆ CSVì— ì €ì¥
            if traffic_docs:
                output_file = output_path / csv_file.name
                with open(output_file, 'w', encoding='utf-8', newline='') as f:
                    if traffic_docs:
                        fieldnames = list(list(traffic_docs.values())[0][0].keys())
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        for rows in traffic_docs.values():
                            writer.writerows(rows)

        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {csv_file.name} - {e}")
            skipped_files += 1

    if skipped_files > 0:
        print(f"  âš ï¸  ìŠ¤í‚µëœ íŒŒì¼: {skipped_files}ê°œ")
    print(f"  âœ… ì™„ë£Œ: {filtered_docs}/{total_docs} ë¬¸ì„œ ì¶”ì¶œ ({filtered_docs/total_docs*100:.1f}%)" if total_docs > 0 else "  âœ… ì™„ë£Œ: 0/0 ë¬¸ì„œ")


def filter_json_files(source_dir, output_dir, data_type):
    """JSON íŒŒì¼ì—ì„œ êµí†µ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§"""

    source_path = Path(source_dir) / data_type
    output_path = Path(output_dir) / data_type

    if not source_path.exists():
        print(f"âš ï¸  ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")
        return

    output_path.mkdir(parents=True, exist_ok=True)

    json_files = list(source_path.glob("*.json"))
    print(f"\nğŸ“„ {data_type} ì²˜ë¦¬ ì¤‘... (ì´ {len(json_files)}ê°œ íŒŒì¼)")

    filtered_count = 0

    for json_file in tqdm(json_files, desc=f"  {data_type}"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # ì—¬ëŸ¬ í•„ë“œì—ì„œ êµí†µ í‚¤ì›Œë“œ ê²€ìƒ‰
            search_fields = []

            # info í•„ë“œ
            if 'info' in data:
                search_fields.extend([
                    data['info'].get('caseName', ''),
                    data['info'].get('caseNum', ''),
                ])

            # label í•„ë“œ
            if 'label' in data:
                search_fields.extend([
                    data['label'].get('instruction', ''),
                    data['label'].get('input', ''),
                    data['label'].get('output', ''),
                ])

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            full_text = ' '.join(search_fields)
            if contains_traffic_keyword(full_text):
                output_file = output_path / json_file.name
                shutil.copy2(json_file, output_file)
                filtered_count += 1

        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {json_file.name} - {e}")

    print(f"  âœ… ì™„ë£Œ: {filtered_count}/{len(json_files)} íŒŒì¼ ì¶”ì¶œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 80)
    print("ğŸš— êµí†µ ê´€ë ¨ ë²•ë¥  ë°ì´í„° í•„í„°ë§ ì‹œì‘")
    print("=" * 80)
    print(f"\nğŸ“‚ ì›ë³¸ ê²½ë¡œ: {SOURCE_BASE}")
    print(f"ğŸ“‚ ì¶œë ¥ ê²½ë¡œ: {OUTPUT_BASE}")
    print(f"\nğŸ” í•„í„°ë§ í‚¤ì›Œë“œ ({len(TRAFFIC_KEYWORDS)}ê°œ):")
    print(f"   {', '.join(TRAFFIC_KEYWORDS[:10])}...")
    print("\n" + "=" * 80)

    # Training ë°ì´í„° ì²˜ë¦¬
    print("\nğŸ“š Training ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("-" * 80)

    source_training = os.path.join(SOURCE_BASE, "Training")
    output_training = os.path.join(OUTPUT_BASE, "Training")

    # ì›ì²œë°ì´í„° (CSV)
    print("\n[1/2] ì›ì²œë°ì´í„° (CSV) ì²˜ë¦¬")
    source_csv = os.path.join(source_training, "01.ì›ì²œë°ì´í„°")
    output_csv = os.path.join(output_training, "01.ì›ì²œë°ì´í„°")

    for data_type in ['TS_ê²°ì •ë¡€', 'TS_ë²•ë ¹', 'TS_íŒê²°ë¬¸', 'TS_í•´ì„ë¡€']:
        filter_csv_files(source_csv, output_csv, data_type)

    # ë¼ë²¨ë§ë°ì´í„° (JSON)
    print("\n[2/2] ë¼ë²¨ë§ë°ì´í„° (JSON) ì²˜ë¦¬")
    source_json = os.path.join(source_training, "02.ë¼ë²¨ë§ë°ì´í„°")
    output_json = os.path.join(output_training, "02.ë¼ë²¨ë§ë°ì´í„°")

    for data_type in ['TL_ê²°ì •ë¡€_QA', 'TL_ê²°ì •ë¡€_SUM', 'TL_ë²•ë ¹_QA',
                      'TL_íŒê²°ë¬¸_QA', 'TL_íŒê²°ë¬¸_SUM', 'TL_í•´ì„ë¡€_QA', 'TL_í•´ì„ë¡€_SUM']:
        filter_json_files(source_json, output_json, data_type)

    # Validation ë°ì´í„° ì²˜ë¦¬
    print("\n" + "=" * 80)
    print("\nğŸ“š Validation ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("-" * 80)

    source_validation = os.path.join(SOURCE_BASE, "Validation")
    output_validation = os.path.join(OUTPUT_BASE, "Validation")

    if os.path.exists(source_validation):
        # ì›ì²œë°ì´í„° (CSV) - Validationì€ VS_ ì ‘ë‘ì‚¬ ì‚¬ìš©
        print("\n[1/2] ì›ì²œë°ì´í„° (CSV) ì²˜ë¦¬")
        source_csv = os.path.join(source_validation, "01.ì›ì²œë°ì´í„°")
        output_csv = os.path.join(output_validation, "01.ì›ì²œë°ì´í„°")

        for data_type in ['VS_ê²°ì •ë¡€', 'VS_ë²•ë ¹', 'VS_íŒê²°ë¬¸', 'VS_í•´ì„ë¡€']:
            filter_csv_files(source_csv, output_csv, data_type)

        # ë¼ë²¨ë§ë°ì´í„° (JSON) - Validationì€ VL_ ì ‘ë‘ì‚¬ ì‚¬ìš©
        print("\n[2/2] ë¼ë²¨ë§ë°ì´í„° (JSON) ì²˜ë¦¬")
        source_json = os.path.join(source_validation, "02.ë¼ë²¨ë§ë°ì´í„°")
        output_json = os.path.join(output_validation, "02.ë¼ë²¨ë§ë°ì´í„°")

        for data_type in ['VL_ê²°ì •ë¡€_QA', 'VL_ê²°ì •ë¡€_SUM', 'VL_ë²•ë ¹_QA',
                          'VL_íŒê²°ë¬¸_QA', 'VL_íŒê²°ë¬¸_SUM', 'VL_í•´ì„ë¡€_QA', 'VL_í•´ì„ë¡€_SUM']:
            filter_json_files(source_json, output_json, data_type)
    else:
        print("âš ï¸  Validation ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 80)
    print("âœ… êµí†µ ê´€ë ¨ ë°ì´í„° í•„í„°ë§ ì™„ë£Œ!")
    print(f"ğŸ“‚ ê²°ê³¼ í™•ì¸: {OUTPUT_BASE}")
    print("=" * 80)


if __name__ == "__main__":
    main()
