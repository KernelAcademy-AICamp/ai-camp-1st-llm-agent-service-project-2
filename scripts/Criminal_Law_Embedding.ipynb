{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# í˜•ì‚¬ë²• ë°ì´í„° ChromaDB ì„ë² ë”©\n",
    "\n",
    "**ëª©í‘œ**: 388,767ê°œì˜ í˜•ì‚¬ë²• íŒë¡€ ë° ë²•ë ¹ ì²­í¬ë¥¼ ChromaDBì— ì„ë² ë”©\n",
    "\n",
    "**ë°ì´í„°**:\n",
    "- íŒë¡€ (HS_P): 32,525ê°œ íŒŒì¼\n",
    "- ë²•ë ¹ (HS_B): 798ê°œ íŒŒì¼\n",
    "- ì´ ì²­í¬: 388,767ê°œ\n",
    "- íŒŒì¼ í¬ê¸°: 456MB\n",
    "\n",
    "**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: ~30-60ë¶„ (GPU ì‚¬ìš© ì‹œ)"
   ],
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ],
   "metadata": {
    "id": "install"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": "# ChromaDBì™€ ì˜ì¡´ì„± ì„¤ì¹˜ (ì—ëŸ¬ ë¬´ì‹œ)\n!pip install -q chromadb sentence-transformers tqdm 2>&1 | grep -v \"dependency conflicts\" || true\nprint(\"âœ… Packages installed (warnings ignored)\")"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. íŒŒì¼ ì—…ë¡œë“œ\n",
    "\n",
    "`criminal_law_chunks.jsonl` íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (456MB)"
   ],
   "metadata": {
    "id": "upload"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# íŒŒì¼ ì—…ë¡œë“œ\n",
    "print(\"ğŸ“ Please upload criminal_law_chunks.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ì—…ë¡œë“œëœ íŒŒì¼ í™•ì¸\n",
    "if 'criminal_law_chunks.jsonl' in uploaded:\n",
    "    file_size = os.path.getsize('criminal_law_chunks.jsonl') / (1024 * 1024)\n",
    "    print(f\"âœ… File uploaded successfully: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"âŒ File not found. Please upload criminal_law_chunks.jsonl\")"
   ],
   "metadata": {
    "id": "upload-file"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. ì„ë² ë”© ì‹¤í–‰"
   ],
   "metadata": {
    "id": "embed"
   }
  },
  {
   "cell_type": "code",
   "source": "import json\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict\nimport chromadb\nfrom chromadb.config import Settings\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nimport torch\nimport gc\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass OptimizedCriminalLawEmbedder:\n    \"\"\"ìµœì í™”ëœ í˜•ì‚¬ë²• ë°ì´í„° ChromaDB ì„ë² ë”© (ìŠ¤íŠ¸ë¦¬ë° + ì²´í¬í¬ì¸íŠ¸)\"\"\"\n\n    def __init__(\n        self,\n        jsonl_file: str,\n        chroma_db_path: str = \"./chroma_db\",\n        collection_name: str = \"criminal_law_docs\",\n        model_name: str = \"jhgan/ko-sroberta-multitask\",\n        checkpoint_interval: int = 10000\n    ):\n        self.jsonl_file = Path(jsonl_file)\n        self.chroma_db_path = Path(chroma_db_path)\n        self.collection_name = collection_name\n        self.checkpoint_interval = checkpoint_interval\n        self.checkpoint_file = Path(\"embedding_checkpoint.txt\")\n        \n        # GPU ê°ì§€ ë° ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if self.device == \"cuda\":\n            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n            if gpu_mem > 30:  # V100, A100\n                self.batch_size = 512\n            elif gpu_mem > 14:  # T4\n                self.batch_size = 256\n            else:\n                self.batch_size = 128\n        else:\n            self.batch_size = 64\n        \n        logger.info(f\"ğŸ–¥ï¸  Device: {self.device.upper()}\")\n        logger.info(f\"ğŸ“¦ Batch size: {self.batch_size}\")\n\n        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n        logger.info(f\"Loading embedding model: {model_name}\")\n        self.model = SentenceTransformer(model_name, device=self.device)\n        logger.info(\"âœ… Embedding model loaded\")\n\n        # ChromaDB ì´ˆê¸°í™”\n        logger.info(f\"Initializing ChromaDB at {chroma_db_path}\")\n        self.client = chromadb.PersistentClient(\n            path=str(self.chroma_db_path),\n            settings=Settings(anonymized_telemetry=False)\n        )\n\n        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±\n        try:\n            self.collection = self.client.get_collection(name=self.collection_name)\n            logger.info(f\"ğŸ“‚ Existing collection found: {self.collection.count()} documents\")\n        except:\n            self.collection = self.client.create_collection(\n                name=self.collection_name,\n                metadata={\"description\": \"í˜•ì‚¬ë²• íŒë¡€ ë° ë²•ë ¹ ë¬¸ì„œ\"}\n            )\n            logger.info(f\"âœ… New collection created: {self.collection_name}\")\n\n    def clean_metadata(self, metadata: Dict) -> Dict:\n        \"\"\"ë©”íƒ€ë°ì´í„°ì—ì„œ None ê°’ ì œê±° ë° íƒ€ì… ë³€í™˜\"\"\"\n        cleaned = {}\n        for key, value in metadata.items():\n            if value is None:\n                continue  # None ê°’ ìŠ¤í‚µ\n            elif isinstance(value, (str, int, float, bool)):\n                cleaned[key] = value\n            elif isinstance(value, list):\n                # ë¦¬ìŠ¤íŠ¸ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜\n                cleaned[key] = str(value)\n            else:\n                # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜\n                cleaned[key] = str(value)\n        return cleaned\n\n    def count_total_lines(self) -> int:\n        \"\"\"JSONL íŒŒì¼ì˜ ì´ ë¼ì¸ ìˆ˜ ê³„ì‚°\"\"\"\n        with open(self.jsonl_file, 'r', encoding='utf-8') as f:\n            return sum(1 for _ in f)\n\n    def load_checkpoint(self) -> int:\n        \"\"\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ ì¸ë±ìŠ¤ ë¡œë“œ\"\"\"\n        if self.checkpoint_file.exists():\n            with open(self.checkpoint_file, 'r') as f:\n                last_index = int(f.read().strip())\n                logger.info(f\"ğŸ“ Resuming from checkpoint: {last_index}\")\n                return last_index\n        return 0\n\n    def save_checkpoint(self, index: int):\n        \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥\"\"\"\n        with open(self.checkpoint_file, 'w') as f:\n            f.write(str(index))\n\n    def embed_and_store_streaming(self):\n        \"\"\"ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  ChromaDBì— ì €ì¥\"\"\"\n        # ì´ ë¼ì¸ ìˆ˜ ê³„ì‚°\n        total_chunks = self.count_total_lines()\n        logger.info(f\"ğŸ“Š Total chunks to process: {total_chunks:,}\")\n        \n        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n        start_index = self.load_checkpoint()\n        if start_index > 0:\n            logger.info(f\"â­ï¸  Skipping first {start_index:,} chunks (already processed)\")\n\n        # ë°°ì¹˜ ë²„í¼\n        batch_texts = []\n        batch_ids = []\n        batch_metadatas = []\n        processed = start_index\n        failed_chunks = []\n\n        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬\n        with open(self.jsonl_file, 'r', encoding='utf-8') as f:\n            # ì´ë¯¸ ì²˜ë¦¬ëœ ë¼ì¸ ìŠ¤í‚µ\n            for _ in range(start_index):\n                next(f)\n            \n            # tqdm ì§„í–‰ë¥  í‘œì‹œ\n            with tqdm(total=total_chunks - start_index, initial=0, desc=\"Embedding\") as pbar:\n                for line_num, line in enumerate(f, start=start_index):\n                    try:\n                        chunk = json.loads(line)\n                        batch_texts.append(chunk['text'])\n                        batch_ids.append(chunk['id'])\n                        # ë©”íƒ€ë°ì´í„° í´ë¦¬ë‹\n                        batch_metadatas.append(self.clean_metadata(chunk['metadata']))\n                        \n                        # ë°°ì¹˜ê°€ ì°¼ìœ¼ë©´ ì„ë² ë”© ë° ì €ì¥\n                        if len(batch_texts) >= self.batch_size:\n                            self._process_batch(batch_texts, batch_ids, batch_metadatas)\n                            processed += len(batch_texts)\n                            \n                            # ë°°ì¹˜ ì´ˆê¸°í™”\n                            batch_texts = []\n                            batch_ids = []\n                            batch_metadatas = []\n                            \n                            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸\n                            pbar.update(self.batch_size)\n                            \n                            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n                            if processed % self.checkpoint_interval == 0:\n                                self.save_checkpoint(processed)\n                                logger.info(f\"ğŸ’¾ Checkpoint saved at {processed:,} chunks\")\n                                \n                                # ë©”ëª¨ë¦¬ ì •ë¦¬\n                                gc.collect()\n                                if self.device == \"cuda\":\n                                    torch.cuda.empty_cache()\n                    \n                    except Exception as e:\n                        logger.warning(f\"âš ï¸  Failed to process chunk at line {line_num}: {str(e)[:200]}\")\n                        failed_chunks.append(line_num)\n                        continue\n                \n                # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬\n                if batch_texts:\n                    self._process_batch(batch_texts, batch_ids, batch_metadatas)\n                    processed += len(batch_texts)\n                    pbar.update(len(batch_texts))\n        \n        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n        self.save_checkpoint(processed)\n        logger.info(f\"âœ… Embedding complete! Total processed: {processed:,}\")\n        \n        if failed_chunks:\n            logger.warning(f\"âš ï¸  Failed chunks: {len(failed_chunks)}\")\n            with open('failed_chunks.txt', 'w') as f:\n                f.write('\\n'.join(map(str, failed_chunks)))\n\n    def _process_batch(self, texts: List[str], ids: List[str], metadatas: List[Dict]):\n        \"\"\"ë°°ì¹˜ ì„ë² ë”© ë° ì €ì¥\"\"\"\n        try:\n            # GPUë¡œ ì§ì ‘ ì„ë² ë”© ìƒì„±\n            embeddings = self.model.encode(\n                texts,\n                convert_to_tensor=True,\n                show_progress_bar=False,\n                device=self.device\n            )\n            \n            # CPUë¡œ ë³€í™˜ (ChromaDB ì €ì¥ìš©)\n            if isinstance(embeddings, torch.Tensor):\n                embeddings = embeddings.cpu().numpy()\n            \n            # ChromaDBì— ì €ì¥\n            self.collection.add(\n                ids=ids,\n                embeddings=embeddings.tolist(),\n                documents=texts,\n                metadatas=metadatas\n            )\n        except Exception as e:\n            logger.error(f\"âŒ Batch processing error: {e}\")\n            raise\n\n    def verify_embedding(self):\n        \"\"\"ì„ë² ë”© ê²°ê³¼ ê²€ì¦\"\"\"\n        count = self.collection.count()\n        logger.info(f\"ğŸ“Š Total documents in ChromaDB: {count:,}\")\n\n        # ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n        test_queries = [\n            \"ìŒì£¼ìš´ì „ ì²˜ë²Œ\",\n            \"ì ˆë„ì£„ êµ¬ì„±ìš”ê±´\",\n            \"ìœ„ë²•ìˆ˜ì§‘ì¦ê±° ë°°ì œ\"\n        ]\n\n        for test_query in test_queries:\n            logger.info(f\"\\nğŸ” Test query: '{test_query}'\")\n            \n            query_embedding = self.model.encode([test_query], convert_to_tensor=True, device=self.device)\n            if isinstance(query_embedding, torch.Tensor):\n                query_embedding = query_embedding.cpu().numpy()\n            \n            results = self.collection.query(\n                query_embeddings=query_embedding.tolist(),\n                n_results=3\n            )\n\n            logger.info(\"Top 3 results:\")\n            for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n                doc_type = metadata.get('type', 'unknown')\n                source = metadata.get('source', 'unknown')\n                case_num = metadata.get('case_number', 'N/A')\n                logger.info(f\"\\n  {i+1}. [{doc_type}] {source}\")\n                logger.info(f\"     Case: {case_num}\")\n                logger.info(f\"     {doc[:150]}...\")\n\n    def run(self):\n        \"\"\"ì „ì²´ ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\"\"\"\n        logger.info(\"ğŸš€ Starting optimized embedding process...\")\n        \n        # ìŠ¤íŠ¸ë¦¬ë° ì„ë² ë”©\n        self.embed_and_store_streaming()\n        \n        # ê²€ì¦\n        self.verify_embedding()\n        \n        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ (ì™„ë£Œë¨)\n        if self.checkpoint_file.exists():\n            self.checkpoint_file.unlink()\n            logger.info(\"ğŸ—‘ï¸  Checkpoint file removed (embedding complete)\")\n        \n        logger.info(f\"\\nâœ… All done! ChromaDB saved at: {self.chroma_db_path}\")\n        logger.info(f\"ğŸ“¦ You can now download the '{self.chroma_db_path}' folder\")\n\n\n# ì‹¤í–‰\nembedder = OptimizedCriminalLawEmbedder(\n    jsonl_file=\"criminal_law_chunks.jsonl\",\n    chroma_db_path=\"./chroma_criminal_law\",\n    checkpoint_interval=10000  # 10,000ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n)\n\nembedder.run()",
   "metadata": {
    "id": "embed-code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. ChromaDB ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "ì„ë² ë”©ì´ ì™„ë£Œë˜ë©´ `chroma_criminal_law` í´ë”ë¥¼ ì••ì¶•í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
   ],
   "metadata": {
    "id": "download"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "# ChromaDB í´ë” ì••ì¶•\n",
    "print(\"ğŸ“¦ Compressing ChromaDB folder...\")\n",
    "shutil.make_archive('chroma_criminal_law', 'zip', '.', 'chroma_criminal_law')\n",
    "\n",
    "# ì••ì¶• íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "zip_size = os.path.getsize('chroma_criminal_law.zip') / (1024 * 1024)\n",
    "print(f\"âœ… Compressed file size: {zip_size:.2f} MB\")\n",
    "\n",
    "# ë‹¤ìš´ë¡œë“œ\n",
    "print(\"â¬‡ï¸ Downloading...\")\n",
    "files.download('chroma_criminal_law.zip')\n",
    "print(\"âœ… Download complete!\")"
   ],
   "metadata": {
    "id": "download-code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "ë‹¤ìš´ë¡œë“œí•œ `chroma_criminal_law.zip` íŒŒì¼ì„ ë¡œì»¬ í”„ë¡œì íŠ¸ì˜ `data/vectordb/` ê²½ë¡œì— ì••ì¶• í•´ì œí•˜ì„¸ìš”.\n",
    "\n",
    "```bash\n",
    "# ë¡œì»¬ í„°ë¯¸ë„ì—ì„œ\n",
    "cd /path/to/lawlaw/data/vectordb\n",
    "unzip chroma_criminal_law.zip\n",
    "```\n",
    "\n",
    "ê·¸ëŸ¬ë©´ `/research/ai` í˜ì´ì§€ì—ì„œ í˜•ì‚¬ë²• RAG ê²€ìƒ‰ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤!"
   ],
   "metadata": {
    "id": "usage"
   }
  }
 ]
}