"""
Criminal Law Data Loader
ì›ì²œë°ì´í„° + ë¼ë²¨ë§ë°ì´í„° í†µí•© ë¡œë”
Training/Validation split ì§€ì›
"""

import os
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm


class CriminalLawDataLoader:
    """í˜•ì‚¬ë²• ë°ì´í„° ë¡œë” - ì›ì²œë°ì´í„° + ë¼ë²¨ë§ë°ì´í„° í†µí•©"""

    def __init__(self, base_path: str = None):
        """
        Initialize data loader.

        Args:
            base_path: ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ
        """
        if base_path is None:
            # ê¸°ë³¸ ê²½ë¡œ ìë™ íƒì§€
            project_root = Path(__file__).parent.parent
            base_path = project_root / "04.í˜•ì‚¬ë²• LLM ì‚¬ì „í•™ìŠµ ë° Instruction Tuning ë°ì´í„°/3.ê°œë°©ë°ì´í„°/1.ë°ì´í„°"

        self.base_path = Path(base_path)
        self.training_path = self.base_path / "Training"
        self.validation_path = self.base_path / "Validation"

        # ê²½ë¡œ ì„¤ì •
        self.source_path = self.training_path / "01.ì›ì²œë°ì´í„°"
        self.labeled_path = self.training_path / "02.ë¼ë²¨ë§ë°ì´í„°"

        self.stats = {}

    def load_dataset(self,
                    use_source: bool = True,
                    use_labeled: bool = True,
                    source_types: List[str] = None,
                    labeled_types: List[str] = None,
                    max_per_type: int = None,
                    split: str = 'training') -> List[Dict[str, Any]]:
        """
        í†µí•© ë°ì´í„°ì…‹ ë¡œë“œ.

        Args:
            use_source: ì›ì²œë°ì´í„° ì‚¬ìš© ì—¬ë¶€
            use_labeled: ë¼ë²¨ë§ë°ì´í„° ì‚¬ìš© ì—¬ë¶€
            source_types: ì›ì²œë°ì´í„° íƒ€ì… ['ë²•ë ¹', 'íŒê²°ë¬¸', 'ê²°ì •ë¡€', 'í•´ì„ë¡€']
            labeled_types: ë¼ë²¨ë§ë°ì´í„° íƒ€ì… ['ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'íŒê²°ë¬¸_SUM', etc]
            max_per_type: íƒ€ì…ë³„ ìµœëŒ€ ë¬¸ì„œ ìˆ˜
            split: ë°ì´í„° ìŠ¤í”Œë¦¿ 'training', 'validation', 'all'

        Returns:
            í†µí•© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        all_documents = []

        # ê¸°ë³¸ê°’ ì„¤ì •
        if source_types is None:
            source_types = ['ë²•ë ¹', 'íŒê²°ë¬¸', 'ê²°ì •ë¡€', 'í•´ì„ë¡€']
        if labeled_types is None:
            labeled_types = ['ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'ê²°ì •ë¡€_QA', 'í•´ì„ë¡€_QA']

        # splitì— ë”°ë¼ ë¡œë“œí•  ë°ì´í„° ê²°ì •
        splits_to_load = []
        if split == 'all':
            splits_to_load = ['training', 'validation']
        elif split in ['training', 'validation']:
            splits_to_load = [split]
        else:
            raise ValueError(f"Invalid split: {split}. Must be 'training', 'validation', or 'all'")

        # ê° splitì—ì„œ ë°ì´í„° ë¡œë“œ
        for current_split in splits_to_load:
            # ì›ì²œë°ì´í„° ë¡œë“œ
            if use_source:
                source_docs = self.load_source_data(source_types, max_per_type, current_split)
                all_documents.extend(source_docs)
                print(f"âœ“ {current_split} ì›ì²œë°ì´í„°: {len(source_docs)}ê°œ ë¡œë“œ")

            # ë¼ë²¨ë§ë°ì´í„° ë¡œë“œ
            if use_labeled:
                labeled_docs = self.load_labeled_data(labeled_types, max_per_type, current_split)
                all_documents.extend(labeled_docs)
                print(f"âœ“ {current_split} ë¼ë²¨ë§ë°ì´í„°: {len(labeled_docs)}ê°œ ë¡œë“œ")

        # í†µê³„
        print(f"\nğŸ“Š ì „ì²´ ë¡œë“œ í†µê³„:")
        print(f"  ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}")
        for key, value in self.stats.items():
            print(f"  - {key}: {value}")

        return all_documents

    def load_source_data(self,
                        types: List[str] = None,
                        max_per_type: int = None,
                        split: str = 'training') -> List[Dict[str, Any]]:
        """
        ì›ì²œë°ì´í„° ë¡œë“œ.

        Args:
            types: ë¡œë“œí•  íƒ€ì… ë¦¬ìŠ¤íŠ¸ ['ë²•ë ¹', 'íŒê²°ë¬¸', 'ê²°ì •ë¡€', 'í•´ì„ë¡€']
            max_per_type: íƒ€ì…ë³„ ìµœëŒ€ ë¬¸ì„œ ìˆ˜
            split: 'training' or 'validation'

        Returns:
            ì›ì²œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if types is None:
            types = ['ë²•ë ¹', 'íŒê²°ë¬¸', 'ê²°ì •ë¡€', 'í•´ì„ë¡€']

        # splitì— ë”°ë¥¸ ê²½ë¡œ ì„¤ì •
        if split == 'training':
            base_path = self.training_path / "01.ì›ì²œë°ì´í„°"
            prefix = "TS"
        else:  # validation
            base_path = self.validation_path / "01.ì›ì²œë°ì´í„°"
            prefix = "VS"

        all_docs = []

        for doc_type in types:
            folder_name = f"{prefix}_{doc_type}"
            folder_path = base_path / folder_name

            if not folder_path.exists():
                print(f"âš ï¸ ì›ì²œë°ì´í„° í´ë” ì—†ìŒ: {folder_path}")
                continue

            # ëª¨ë“  ì›ì²œë°ì´í„°ëŠ” CSV íŒŒì¼
            docs = self._load_source_csv(folder_path, doc_type, max_per_type)

            all_docs.extend(docs)
            self.stats[f'{split}_source_{doc_type}'] = len(docs)

        return all_docs

    def load_labeled_data(self,
                         types: List[str] = None,
                         max_per_type: int = None,
                         split: str = 'training') -> List[Dict[str, Any]]:
        """
        ë¼ë²¨ë§ë°ì´í„° ë¡œë“œ.

        Args:
            types: ë¡œë“œí•  íƒ€ì… ë¦¬ìŠ¤íŠ¸ ['ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', etc]
            max_per_type: íƒ€ì…ë³„ ìµœëŒ€ ë¬¸ì„œ ìˆ˜
            split: 'training' or 'validation'

        Returns:
            ë¼ë²¨ë§ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if types is None:
            types = ['ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'ê²°ì •ë¡€_QA', 'í•´ì„ë¡€_QA']

        # splitì— ë”°ë¥¸ ê²½ë¡œ ì„¤ì •
        if split == 'training':
            base_path = self.training_path / "02.ë¼ë²¨ë§ë°ì´í„°"
            prefix = "TL"
        else:  # validation
            base_path = self.validation_path / "02.ë¼ë²¨ë§ë°ì´í„°"
            prefix = "VL"

        all_docs = []

        for doc_type in types:
            folder_name = f"{prefix}_{doc_type}"
            folder_path = base_path / folder_name

            if not folder_path.exists():
                print(f"âš ï¸ ë¼ë²¨ë§ë°ì´í„° í´ë” ì—†ìŒ: {folder_path}")
                continue

            docs = self._load_labeled_json(folder_path, doc_type, max_per_type)
            all_docs.extend(docs)
            self.stats[f'{split}_labeled_{doc_type}'] = len(docs)

        return all_docs

    def _load_source_csv(self, folder_path: Path, doc_type: str, limit: int = None) -> List[Dict[str, Any]]:
        """ì›ì²œë°ì´í„° CSV íŒŒì¼ ë¡œë“œ (ë²•ë ¹, íŒê²°ë¬¸, ê²°ì •ë¡€, í•´ì„ë¡€)."""
        documents = []
        csv_files = list(folder_path.glob("*.csv"))

        if limit:
            csv_files = csv_files[:limit]

        print(f"ğŸ“š {doc_type} ì›ì²œ CSV íŒŒì¼ {len(csv_files)}ê°œ ë¡œë“œ ì¤‘...")

        for csv_file in tqdm(csv_files, desc=f"{doc_type} ë¡œë“œ"):
            try:
                df = pd.read_csv(csv_file, encoding='utf-8')
                df = df.fillna('')  # NaN ì²˜ë¦¬

                doc_content = []
                doc_id = None

                for _, row in df.iterrows():
                    # ì²« ë²ˆì§¸ ì¹¼ëŸ¼ì„ IDë¡œ ì‚¬ìš©
                    if doc_id is None and len(df.columns) > 0:
                        doc_id = row[df.columns[0]]

                    # 'ë‚´ìš©' ì¹¼ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ëª¨ë“  ì¹¼ëŸ¼ í•©ì¹˜ê¸°
                    if 'ë‚´ìš©' in df.columns:
                        content = str(row['ë‚´ìš©']).strip()
                    else:
                        content = ' '.join([str(val).strip() for val in row.values if str(val).strip()])

                    if content:
                        doc_content.append(content)

                full_content = "\n".join(doc_content)

                if full_content:  # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    documents.append({
                        'content': full_content,
                        'metadata': {
                            'source': f'ì›ì²œ_{doc_type}',
                            'doc_id': doc_id,
                            'file': csv_file.name,
                            'type': f'source_{doc_type}'
                        }
                    })

            except Exception as e:
                # ì¡°ìš©íˆ ìŠ¤í‚µ
                continue

        return documents

    def _load_labeled_json(self, folder_path: Path, doc_type: str, limit: int = None) -> List[Dict[str, Any]]:
        """ë¼ë²¨ë§ JSON íŒŒì¼ ë¡œë“œ (QA, SUM)."""
        documents = []
        json_files = list(folder_path.glob("*.json"))

        if limit:
            json_files = json_files[:limit]

        print(f"ğŸ“š {doc_type} ë¼ë²¨ë§ íŒŒì¼ {len(json_files)}ê°œ ë¡œë“œ ì¤‘...")

        for json_file in tqdm(json_files, desc=f"{doc_type} ë¡œë“œ"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                label_data = data.get('label', {})

                # QA ë°ì´í„°
                if '_QA' in doc_type:
                    input_text = label_data.get('input', '')
                    output_text = label_data.get('output', '')
                    instruction = label_data.get('instruction', '')

                    if input_text and output_text:
                        # QA ìŒìœ¼ë¡œ ì €ì¥
                        content = f"Q: {input_text}\nA: {output_text}"

                        documents.append({
                            'content': content,
                            'metadata': {
                                'source': f'ë¼ë²¨ë§_{doc_type}',
                                'input': input_text,
                                'output': output_text,
                                'instruction': instruction,
                                'file': json_file.name,
                                'type': f'labeled_{doc_type}'
                            }
                        })

                # SUM ë°ì´í„°
                elif '_SUM' in doc_type:
                    summary_text = label_data.get('output', '')
                    instruction = label_data.get('instruction', '')

                    if summary_text:
                        documents.append({
                            'content': summary_text,
                            'metadata': {
                                'source': f'ë¼ë²¨ë§_{doc_type}',
                                'instruction': instruction,
                                'file': json_file.name,
                                'type': f'labeled_{doc_type}'
                            }
                        })

            except Exception as e:
                continue

        return documents
