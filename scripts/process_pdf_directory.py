"""
ì´ë¯¸ì§€í™”ëœ PDF íŒŒì¼ ì²˜ë¦¬ V2 (ê°œì„ ëœ í’ˆì§ˆ í‰ê°€ + ì„ íƒì  ì „ì²˜ë¦¬)
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.ocr.pdf_extractor import PDFTextExtractor
from core.ocr.document_structurer import DocumentStructurer
from core.ocr.ocr_processor import extract_pdf_with_preprocessing
from core.ocr.postprocessor import apply_ocr_postprocessing
import json
from datetime import datetime
import logging
import fitz

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """ì´ë¯¸ì§€í™”ëœ PDF ë””ë ‰í† ë¦¬ ì²˜ë¦¬ (V2 - ê°œì„ ëœ í’ˆì§ˆ í‰ê°€)"""
    # imaged_PDF ë””ë ‰í† ë¦¬
    test_dir = Path("/Users/nw_mac/Documents/Github_crawling/ai-camp-1st-llm-agent-service-project-2/OCR_test/imaged_PDF")

    if not test_dir.exists():
        logger.error(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_dir}")
        return

    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = sorted(list(test_dir.glob("*.pdf")))

    if not pdf_files:
        logger.error("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("="*70)
    logger.info("ì´ë¯¸ì§€í™”ëœ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ V2 (ê°œì„ ëœ í’ˆì§ˆ í‰ê°€ + ì„ íƒì  ì „ì²˜ë¦¬)")
    logger.info("="*70)
    logger.info(f"ì´ {len(pdf_files)}ê°œ íŒŒì¼")
    logger.info(f"ë””ë ‰í† ë¦¬: {test_dir}")
    logger.info("")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path("imaged_pdf_output_v2")
    output_dir.mkdir(exist_ok=True)

    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    all_results = []
    processing_stats = {
        'pymupdf': 0,
        'ocr': 0,
        'failed': 0
    }

    for idx, pdf_file in enumerate(pdf_files, 1):
        logger.info(f"\n[{idx}/{len(pdf_files)}] {pdf_file.name}")
        logger.info("="*70)

        try:
            # [1ë‹¨ê³„] PyMuPDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            pymupdf_result = PDFTextExtractor.extract_text_with_pymupdf(pdf_file)
            is_extractable = PDFTextExtractor.is_text_extractable(pymupdf_result)

            if is_extractable:
                # PyMuPDF í…ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥
                logger.info("  [ê²½ë¡œ] PyMuPDF í…ìŠ¤íŠ¸ â†’ ë¬¸ì„œ íƒ€ì…ë³„ êµ¬ì¡°í™”")

                extraction_method = 'pymupdf'
                full_text = pymupdf_result['text']
                metadata = {
                    'extraction_method': extraction_method,
                    'char_count': pymupdf_result['char_count'],
                    'page_count': pymupdf_result['page_count'],
                    'extraction_rate': pymupdf_result['extraction_rate']
                }

                processing_stats['pymupdf'] += 1

            else:
                # OCR í•„ìš”
                logger.info("  [ê²½ë¡œ] í’ˆì§ˆ í‰ê°€ â†’ ì„ íƒì  ì „ì²˜ë¦¬ â†’ OCR â†’ í›„ì²˜ë¦¬")
                logger.info(f"  [2ë‹¨ê³„] OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ì ì‘í˜• ì „ì²˜ë¦¬)...")

                # V2: ê°œì„ ëœ í’ˆì§ˆ í‰ê°€ ë° ì„ íƒì  ì „ì²˜ë¦¬
                ocr_result = extract_pdf_with_preprocessing(
                    pdf_file,
                    dpi=300,
                    preset='standard',  # ê¸°ë³¸ê°’ (adaptive=Trueì´ë©´ ë¬´ì‹œë¨)
                    adaptive=True       # ì ì‘í˜• ëª¨ë“œ í™œì„±í™”
                )

                # OCR í…ìŠ¤íŠ¸ ë³‘í•©
                full_text = '\n'.join([page['text'] for page in ocr_result['pages']])

                # OCR í›„ì²˜ë¦¬ ì ìš©
                logger.info(f"  [2.5ë‹¨ê³„] OCR í›„ì²˜ë¦¬ (ì˜¤ì¸ì‹ ë‹¨ì–´ êµì •)...")
                full_text = apply_ocr_postprocessing(full_text, verbose=False)

                extraction_method = 'ocr_v2'
                metadata = {
                    'extraction_method': extraction_method,
                    'char_count': ocr_result['total_chars'],
                    'page_count': ocr_result['page_count'],
                    'avg_confidence': ocr_result['avg_confidence'],
                    'preprocessing': 'adaptive_selective',
                    'preset_usage': ocr_result.get('preset_usage', {}),
                    'quality_info': 'improved_weighted_scoring'
                }

                processing_stats['ocr'] += 1

            # [3ë‹¨ê³„] ë¬¸ì„œ íƒ€ì…ë³„ êµ¬ì¡°í™”
            logger.info(f"  [3ë‹¨ê³„] ë¬¸ì„œ íƒ€ì…ë³„ êµ¬ì¡°í™”...")
            structurer = DocumentStructurer(full_text, pdf_file.name)
            structured_data = structurer.structure()

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            structured_data['ì¶”ì¶œë°©ë²•'] = extraction_method
            structured_data['ì¶”ì¶œë©”íƒ€ë°ì´í„°'] = metadata
            structured_data['ì²˜ë¦¬ì‹œê°'] = datetime.now().isoformat()

            logger.info("="*70)
            logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            logger.info(f"  ë¬¸ì„œ íƒ€ì…: {structured_data.get('ë°ì´í„°íƒ€ì…', 'Unknown')}")
            logger.info(f"  ì¶”ì¶œ ë°©ë²•: {extraction_method.upper()}")
            logger.info(f"  ê¸€ì ìˆ˜: {metadata['char_count']}ì")
            if extraction_method.startswith('ocr'):
                logger.info(f"  OCR ì‹ ë¢°ë„: {metadata['avg_confidence']:.1f}%")
                logger.info(f"  ì „ì²˜ë¦¬: {metadata.get('preprocessing', 'N/A')}")
            logger.info("="*70)

            all_results.append(structured_data)

        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_file.name} - {e}")
            processing_stats['failed'] += 1
            import traceback
            traceback.print_exc()
            continue

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # í†µí•© JSON ì €ì¥
    output_file = output_dir / f"imaged_pdf_result_v2_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "ì²˜ë¦¬ì‹œê°": datetime.now().isoformat(),
            "ì´ê±´ìˆ˜": len(all_results),
            "ë²„ì „": "V2 - ê°œì„ ëœ í’ˆì§ˆ í‰ê°€ + ì„ íƒì  ì „ì²˜ë¦¬",
            "ë¬¸ì„œíƒ€ì…ë³„": {},
            "ì¼€ì´ìŠ¤": all_results
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ… í†µí•© ê²°ê³¼ ì €ì¥: {output_file}")

    # ê°œë³„ íŒŒì¼ ì €ì¥
    for result in all_results:
        doc_type = result.get("ë°ì´í„°íƒ€ì…", "unknown")
        filename = result.get("íŒŒì¼ëª…", "unknown")

        # íŒŒì¼ëª…ì—ì„œ .pdf ì œê±°
        base_name = filename.replace('.pdf', '').replace('_converted', '')
        individual_file = output_dir / f"{doc_type}_{base_name}.json"

        with open(individual_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ê°œë³„ íŒŒì¼ ì €ì¥: {individual_file}")

    # ìš”ì•½ í†µê³„
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š ì²˜ë¦¬ ìš”ì•½")
    logger.info("="*70)
    logger.info(f"ì´ ì²˜ë¦¬: {len(all_results)}ê±´")
    logger.info(f"ì‹¤íŒ¨: {processing_stats['failed']}ê±´")

    # ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„
    doc_types = {}
    for result in all_results:
        doc_type = result.get('ë°ì´í„°íƒ€ì…', 'Unknown')
        doc_types[doc_type] = doc_types.get(doc_type, 0) + 1

    logger.info(f"\në¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬:")
    for doc_type, count in doc_types.items():
        logger.info(f"  {doc_type}: {count}ê±´")

    # ì¶”ì¶œ ë°©ë²•ë³„ í†µê³„
    logger.info(f"\nì¶”ì¶œ ë°©ë²•ë³„ ë¶„í¬:")
    logger.info(f"  PYMUPDF: {processing_stats['pymupdf']}ê±´")
    logger.info(f"  OCR V2 (ê°œì„ ): {processing_stats['ocr']}ê±´")

    # OCR ì‹ ë¢°ë„ í†µê³„
    ocr_confidences = []
    quality_scores = []

    for result in all_results:
        if result.get('ì¶”ì¶œë°©ë²•', '').startswith('ocr'):
            metadata = result.get('ì¶”ì¶œë©”íƒ€ë°ì´í„°', {})
            confidence = metadata.get('avg_confidence', 0)
            ocr_confidences.append(confidence)

            # í’ˆì§ˆ ì •ë³´
            logger.info(f"\n  ğŸ“„ {result.get('íŒŒì¼ëª…', 'Unknown')}")
            logger.info(f"     ì‹ ë¢°ë„: {confidence:.1f}%")
            logger.info(f"     ì „ì²˜ë¦¬: {metadata.get('preprocessing', 'N/A')}")
            logger.info(f"     ê¸€ì ìˆ˜: {metadata.get('char_count', 0)}ì")
            preset_usage = metadata.get('preset_usage', {})
            if preset_usage:
                logger.info(f"     ì „ì²˜ë¦¬ ì „ëµ ì‚¬ìš©:")
                for preset, count in preset_usage.items():
                    logger.info(f"       - {preset}: {count}í˜ì´ì§€")

    if ocr_confidences:
        avg_confidence = sum(ocr_confidences) / len(ocr_confidences)
        logger.info(f"\nOCR í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.1f}%")
        logger.info(f"OCR ìµœì†Œ ì‹ ë¢°ë„: {min(ocr_confidences):.1f}%")
        logger.info(f"OCR ìµœëŒ€ ì‹ ë¢°ë„: {max(ocr_confidences):.1f}%")

    logger.info("="*70)

    return all_results, output_file


if __name__ == "__main__":
    main()
