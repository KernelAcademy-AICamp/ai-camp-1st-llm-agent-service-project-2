"""
PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
PyMuPDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„  â†’ OCR ì „í™˜ (í’ˆì§ˆ ê¸°ë°˜ ì „ì²˜ë¦¬ í¬í•¨)
"""

import json
from pathlib import Path
from datetime import datetime
import logging
import fitz  # PyMuPDF
import unicodedata
import re

# ì‚­ì œëœ import (pdf_extractorëŠ” ë…ë¦½ì ìœ¼ë¡œ ë™ì‘)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PDFTextExtractor:
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""

    @staticmethod
    def extract_text_with_pymupdf(pdf_path: Path) -> dict:
        """
        PyMuPDFë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„

        Returns:
            dict: {
                'success': bool,
                'text': str,
                'page_count': int,
                'char_count': int,
                'extraction_rate': float  # í˜ì´ì§€ë‹¹ í‰ê·  ê¸€ì ìˆ˜
            }
        """
        logger.info(f"  [1ë‹¨ê³„] PyMuPDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„...")

        try:
            doc = fitz.open(pdf_path)
            pages_text = []
            total_chars = 0

            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()

                # Unicode ì •ê·œí™”
                text = unicodedata.normalize('NFC', text)

                pages_text.append(text)
                total_chars += len(text.strip())

            doc.close()

            # ì „ì²´ í…ìŠ¤íŠ¸ ë³‘í•©
            full_text = '\n'.join(pages_text)

            # ì¶”ì¶œë¥  ê³„ì‚°
            extraction_rate = total_chars / len(pages_text) if pages_text else 0

            result = {
                'success': True,
                'text': full_text,
                'page_count': len(pages_text),
                'char_count': total_chars,
                'extraction_rate': extraction_rate
            }

            logger.info(f"    PyMuPDF ì¶”ì¶œ ì™„ë£Œ: {total_chars}ì ({len(pages_text)}í˜ì´ì§€)")
            logger.info(f"    í˜ì´ì§€ë‹¹ í‰ê· : {extraction_rate:.1f}ì")

            return result

        except Exception as e:
            logger.error(f"    PyMuPDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'text': '',
                'page_count': 0,
                'char_count': 0,
                'extraction_rate': 0,
                'error': str(e)
            }

    @staticmethod
    def is_text_extractable(extraction_result: dict, min_chars_per_page: int = 100) -> bool:
        """
        í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨

        Args:
            extraction_result: extract_text_with_pymupdf() ê²°ê³¼
            min_chars_per_page: í˜ì´ì§€ë‹¹ ìµœì†Œ ê¸€ì ìˆ˜ (ê¸°ë³¸ 100ì)

        Returns:
            bool: Trueì´ë©´ PyMuPDF í…ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥, Falseì´ë©´ OCR í•„ìš”
        """
        if not extraction_result['success']:
            logger.info(f"    íŒë‹¨: OCR í•„ìš” (ì¶”ì¶œ ì‹¤íŒ¨)")
            return False

        # ìµœì†Œ ê¸€ì ìˆ˜ í™•ì¸
        if extraction_result['char_count'] < 50:
            logger.info(f"    íŒë‹¨: OCR í•„ìš” (ì´ ê¸€ì ìˆ˜ ë¶€ì¡±: {extraction_result['char_count']}ì)")
            return False

        # í˜ì´ì§€ë‹¹ í‰ê·  ê¸€ì ìˆ˜ í™•ì¸
        if extraction_result['extraction_rate'] < min_chars_per_page:
            logger.info(f"    íŒë‹¨: OCR í•„ìš” (í˜ì´ì§€ë‹¹ í‰ê·  ë¶€ì¡±: {extraction_result['extraction_rate']:.1f}ì)")
            return False

        # ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ í™•ì¸ (í•œê¸€/ì˜ì–´ ë¹„ìœ¨)
        text = extraction_result['text']
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_meaningful = korean_chars + english_chars

        meaningful_ratio = total_meaningful / extraction_result['char_count'] if extraction_result['char_count'] > 0 else 0

        if meaningful_ratio < 0.3:  # ì˜ë¯¸ ìˆëŠ” ê¸€ìê°€ 30% ë¯¸ë§Œ
            logger.info(f"    íŒë‹¨: OCR í•„ìš” (ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶€ì¡±: {meaningful_ratio:.1%})")
            return False

        logger.info(f"    íŒë‹¨: PyMuPDF í…ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥ âœ“")
        logger.info(f"      - ì´ ê¸€ì: {extraction_result['char_count']}ì")
        logger.info(f"      - í˜ì´ì§€ë‹¹ í‰ê· : {extraction_result['extraction_rate']:.1f}ì")
        logger.info(f"      - ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸: {meaningful_ratio:.1%}")
        return True


class PDFProcessingPipeline:
    """PDF ì²˜ë¦¬ í†µí•© íŒŒì´í”„ë¼ì¸"""

    def __init__(self, search_keyword: str = "êµí†µì‚¬ê³ "):
        self.search_keyword = search_keyword

    def process_pdf(self, pdf_path: Path) -> dict:
        """
        PDF íŒŒì¼ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            dict: êµ¬ì¡°í™”ëœ ë°ì´í„°
        """
        logger.info("="*70)
        logger.info(f"PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {pdf_path.name}")
        logger.info("="*70)

        # [1ë‹¨ê³„] PyMuPDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        pymupdf_result = PDFTextExtractor.extract_text_with_pymupdf(pdf_path)
        is_extractable = PDFTextExtractor.is_text_extractable(pymupdf_result)

        if is_extractable:
            # PyMuPDF í…ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥ â†’ ë°”ë¡œ êµ¬ì¡°í™”
            logger.info("  [ê²½ë¡œ] PyMuPDF í…ìŠ¤íŠ¸ â†’ êµ¬ì¡°í™”")

            extraction_method = 'pymupdf'
            full_text = pymupdf_result['text']
            metadata = {
                'extraction_method': extraction_method,
                'char_count': pymupdf_result['char_count'],
                'page_count': pymupdf_result['page_count'],
                'extraction_rate': pymupdf_result['extraction_rate']
            }

        else:
            # OCR í•„ìš”
            logger.info("  [ê²½ë¡œ] OCR ì¶”ì¶œ (í’ˆì§ˆ í‰ê°€ â†’ ì „ì²˜ë¦¬ â†’ OCR)")

            # [2ë‹¨ê³„] í’ˆì§ˆ ê¸°ë°˜ OCR ì¶”ì¶œ (adaptive ëª¨ë“œ)
            logger.info(f"  [2ë‹¨ê³„] OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì ì‘í˜• ì „ì²˜ë¦¬)...")

            ocr_result = extract_pdf_with_preprocessing(
                pdf_path,
                dpi=300,
                preset='standard',
                adaptive=True  # í’ˆì§ˆ ê¸°ë°˜ ìë™ ì „ì²˜ë¦¬ ì„ íƒ
            )

            # OCR í…ìŠ¤íŠ¸ ë³‘í•©
            full_text = '\n'.join([page['text'] for page in ocr_result['pages']])

            extraction_method = 'ocr'
            metadata = {
                'extraction_method': extraction_method,
                'char_count': ocr_result['total_chars'],
                'page_count': ocr_result['page_count'],
                'avg_confidence': ocr_result['avg_confidence'],
                'preprocessing': ocr_result['preprocessing'],
                'preset_usage': ocr_result.get('preset_usage', {})
            }

        # [3ë‹¨ê³„] í…ìŠ¤íŠ¸ íŒŒì‹±
        logger.info(f"  [3ë‹¨ê³„] í…ìŠ¤íŠ¸ íŒŒì‹±...")
        parser = OCRTextParser(full_text, pdf_path.name)
        parsed_data = parser.parse()

        # [4ë‹¨ê³„] ë°ì´í„° êµ¬ì¡°í™”
        logger.info(f"  [4ë‹¨ê³„] ë°ì´í„° êµ¬ì¡°í™”...")
        structurer = OCRDataStructurer(parsed_data, self.search_keyword)
        structured_data = structurer.structure()

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        structured_data['ì¶”ì¶œë°©ë²•'] = extraction_method
        structured_data['ì¶”ì¶œë©”íƒ€ë°ì´í„°'] = metadata
        structured_data['ì²˜ë¦¬ì‹œê°'] = datetime.now().isoformat()

        logger.info("="*70)
        logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        logger.info(f"  ì¶”ì¶œ ë°©ë²•: {extraction_method.upper()}")
        logger.info(f"  ê¸€ì ìˆ˜: {metadata['char_count']}ì")
        logger.info(f"  ì‚¬ê±´ë²ˆí˜¸: {structured_data.get('ì‚¬ê±´ë²ˆí˜¸', 'N/A')}")
        logger.info(f"  ë²•ì›ëª…: {structured_data.get('ë²•ì›ëª…', 'N/A')}")
        logger.info("="*70)

        return structured_data


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # PDF ë””ë ‰í† ë¦¬
    pdf_dir = Path("raw_pdf_data")

    if not pdf_dir.exists():
        logger.error(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_dir}")
        return

    # PDF íŒŒì¼ ì°¾ê¸° (converted íŒŒì¼ ìš°ì„ )
    pdf_files = []
    for pdf_file in pdf_dir.glob("*.pdf"):
        normalized_name = unicodedata.normalize('NFC', pdf_file.name)
        if 'ë³€í™˜' in normalized_name or 'converted' in normalized_name:
            pdf_files.append(pdf_file)

    if not pdf_files:
        # converted ì—†ìœ¼ë©´ ëª¨ë“  PDF
        pdf_files = list(pdf_dir.glob("*.pdf"))

    if not pdf_files:
        logger.error("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("="*70)
    logger.info("PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    logger.info("="*70)
    logger.info(f"ì´ {len(pdf_files)}ê°œ íŒŒì¼")
    logger.info("")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path("pipeline_output")
    output_dir.mkdir(exist_ok=True)

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = PDFProcessingPipeline(search_keyword="êµí†µì‚¬ê³ ")

    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    all_results = []

    for idx, pdf_file in enumerate(pdf_files, 1):
        logger.info(f"\n[{idx}/{len(pdf_files)}] {pdf_file.name}")

        try:
            result = pipeline.process_pdf(pdf_file)
            all_results.append(result)

        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_file.name} - {e}")
            import traceback
            traceback.print_exc()
            continue

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # í†µí•© JSON ì €ì¥
    output_file = output_dir / f"pipeline_result_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "ê²€ìƒ‰ì–´": "êµí†µì‚¬ê³ ",
            "ë°ì´í„°íƒ€ì…": "íŒë¡€",
            "ì²˜ë¦¬ì‹œê°": datetime.now().isoformat(),
            "ì´ê±´ìˆ˜": len(all_results),
            "ì¼€ì´ìŠ¤": all_results
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ… í†µí•© ê²°ê³¼ ì €ì¥: {output_file}")

    # ê°œë³„ íŒŒì¼ ì €ì¥
    for result in all_results:
        case_id = result.get("íŒë¡€ì¼ë ¨ë²ˆí˜¸", "unknown")
        individual_file = output_dir / f"case_{case_id}.json"

        with open(individual_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ê°œë³„ íŒŒì¼ ì €ì¥: {individual_file}")

    # ìš”ì•½ í†µê³„
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š íŒŒì´í”„ë¼ì¸ ìš”ì•½")
    logger.info("="*70)
    logger.info(f"ì´ ì²˜ë¦¬: {len(all_results)}ê±´")

    # ì¶”ì¶œ ë°©ë²•ë³„ í†µê³„
    extraction_methods = {}
    for result in all_results:
        method = result.get('ì¶”ì¶œë°©ë²•', 'unknown')
        extraction_methods[method] = extraction_methods.get(method, 0) + 1

    logger.info(f"\nì¶”ì¶œ ë°©ë²•ë³„ ë¶„í¬:")
    for method, count in extraction_methods.items():
        logger.info(f"  {method.upper()}: {count}ê±´")

    # ì‚¬ê±´ì¢…ë¥˜ë³„ í†µê³„
    case_types = {}
    for result in all_results:
        case_type = result.get('ì‚¬ê±´ì¢…ë¥˜ëª…', 'Unknown')
        case_types[case_type] = case_types.get(case_type, 0) + 1

    logger.info(f"\nì‚¬ê±´ì¢…ë¥˜ë³„ ë¶„í¬:")
    for case_type, count in case_types.items():
        logger.info(f"  {case_type}: {count}ê±´")

    logger.info("="*70)


if __name__ == "__main__":
    main()
