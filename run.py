#!/usr/bin/env python3
"""
RAG Pipeline CLI Tool v2
ê°œì„ ëœ ì‹¤í—˜ ì›Œí¬í”Œë¡œìš°: index, search, generate ë¶„ë¦¬ ì‹¤í–‰
ìºì‹œë¥¼ í†µí•œ ì¸ë±ìŠ¤ ì¬ì‚¬ìš© ì§€ì›
"""

import os
import sys
import json
import yaml
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
import subprocess
import time

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Add current directory to path
sys.path.append(str(Path(__file__).parent))

from core.rag.pipeline import (
    RAGPipeline,
    ComponentInitializationError,
    IndexingError,
    RetrievalError,
    GenerationError
)
from core.rag.cache_manager import IndexCacheManager
from core.config.validator import validate_config
from core.evaluation.metrics import evaluate_answer, calculate_retrieval_metrics
from core.evaluation.advanced_metrics import (
    calculate_ndcg,
    calculate_hit_rate,
    calculate_token_usage,
    calculate_ragas_metrics
)


class RAGExperimentCLI:
    """ê°œì„ ëœ RAG ì‹¤í—˜ ê´€ë¦¬ CLI"""

    def __init__(self):
        self.project_root = Path(__file__).parent
        self.experiments_dir = self.project_root / "experiments"
        self.experiment_log_path = self.experiments_dir / "experiment_log.json"
        self.leaderboard_path = self.experiments_dir / "leaderboard.md"
        self.cache_manager = IndexCacheManager()

    def index_command(self, config_path: str, force: bool = False):
        """
        ì¸ë±ì‹±ë§Œ ìˆ˜í–‰í•˜ëŠ” ëª…ë ¹ì–´.
        ìºì‹œê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±.

        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            force: Trueë©´ ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ì¸ë±ì‹±
        """
        config_path = Path(config_path)
        if not config_path.exists():
            print(f"âŒ Config file not found: {config_path}")
            return

        print(f"\nğŸ“š Indexing with config: {config_path}")
        print("="*60)

        # Load config
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # Validate config
        print("\nğŸ” Validating configuration...")
        is_valid, errors, warnings = validate_config(config, verbose=True)

        if not is_valid:
            print("\nâŒ Configuration validation failed. Please fix the errors above.")
            return None

        # Generate cache key
        cache_key = self.cache_manager.generate_cache_key(config)
        print(f"ğŸ”‘ Cache Key: {cache_key}")

        # Check cache
        if not force and self.cache_manager.exists(cache_key):
            print(f"âœ… Using cached index: {cache_key}")
            cache_info = self.cache_manager.get_cache_info(cache_key)
            if cache_info:
                stats = cache_info.get('stats', {})
                print(f"   Chunks: {stats.get('total_chunks', 'N/A')}")
                print(f"   Created: {cache_info.get('created_at', 'N/A')}")
            return cache_key

        # Need to create new index
        print("ğŸ”¨ Creating new index...")

        try:
            # Initialize pipeline for indexing only
            pipeline = RAGPipeline(config, verbose=True)

        except ComponentInitializationError as e:
            print(f"\nâŒ Failed to initialize pipeline: {e}")
            return None
        except Exception as e:
            print(f"\nâŒ Unexpected initialization error: {e}")
            import traceback
            traceback.print_exc()
            return None

        try:
            # Load documents
            documents = self.load_data_from_config(config)
            print(f"ğŸ“„ Loaded {len(documents)} documents")

            # Index documents
            start_time = time.time()
            doc_contents = [doc['content'] for doc in documents]
            doc_metadata = [doc.get('metadata', {}) for doc in documents]
            stats = pipeline.index_documents(doc_contents, doc_metadata)

            indexing_time = time.time() - start_time
            stats['indexing_time'] = indexing_time

            print(f"âœ… Indexing complete: {stats.get('total_chunks')} chunks in {indexing_time:.1f}s")

        except IndexingError as e:
            print(f"\nâŒ Indexing failed: {e}")
            return None

        try:
            # Save vector store to disk first
            cache_path = self.cache_manager.get_cache_path(cache_key)
            cache_path.mkdir(parents=True, exist_ok=True)
            pipeline.save_index(cache_path)

            # Save cache metadata
            cache_info = {
                'cache_key': cache_key,
                'created_at': datetime.now().isoformat(),
                'config': config,
                'stats': stats,
                'path': str(cache_path)
            }

            with open(cache_path / "cache_config.json", 'w', encoding='utf-8') as f:
                json.dump(cache_info, f, ensure_ascii=False, indent=2)

            # Update metadata
            self.cache_manager.metadata[cache_key] = cache_info
            self.cache_manager._save_metadata()

            print(f"âœ… Cache saved: {cache_key}")

            return cache_key

        except Exception as e:
            print(f"âŒ Failed to save cache: {e}")
            import traceback
            traceback.print_exc()
            return None

    def search_command(self, config_path: str, query: str, cache_key: Optional[str] = None):
        """
        ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•˜ëŠ” ëª…ë ¹ì–´.
        ìºì‹œëœ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì§€ì •ëœ ìºì‹œ í‚¤ ì‚¬ìš©.

        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            cache_key: ì‚¬ìš©í•  ìºì‹œ í‚¤ (ì„ íƒ)
        """
        config_path = Path(config_path)
        if not config_path.exists():
            print(f"âŒ Config file not found: {config_path}")
            return

        print(f"\nğŸ” Searching with config: {config_path}")
        print(f"Query: {query}")
        print("="*60)

        # Load config
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # Determine cache key
        if not cache_key:
            cache_key = self.cache_manager.generate_cache_key(config)

        # Check if cache exists
        if not self.cache_manager.exists(cache_key):
            print(f"âš ï¸  No cached index found for cache key: {cache_key}")
            print("   Run 'python run.py index' first to create the index")

            # Suggest similar cache
            suggested = self.cache_manager.suggest_cache_key(config)
            if suggested:
                print(f"ğŸ’¡ Suggestion: Found similar cache '{suggested}'")
                print(f"   Use: python run.py search --config {config_path} --cache-key {suggested} --query \"{query}\"")
            return

        try:
            # Get cached index path directly (no copying)
            cache_path = self.cache_manager.get_cache_path(cache_key)

            # Initialize pipeline for search only
            pipeline = RAGPipeline(config, verbose=False)
            pipeline.load_index(cache_path)  # Load from cache path directly

            # Perform search
            start_time = time.time()
            results = pipeline.search(query, top_k=config.get('retrieval', {}).get('top_k', 5))
            search_time = time.time() - start_time

            print(f"\nğŸ“Š Found {len(results)} results in {search_time:.3f}s:")
            print("-"*60)

            for i, result in enumerate(results, 1):
                print(f"\n{i}. Score: {result.get('score', 0):.4f}")
                print(f"   Content: {result.get('content', '')[:200]}...")
                if result.get('metadata'):
                    print(f"   Source: {result.get('metadata')}")

        except Exception as e:
            print(f"âŒ Search failed: {e}")
            import traceback
            traceback.print_exc()

    def generate_command(self, config_path: str, query: str, cache_key: Optional[str] = None):
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê²€ìƒ‰ + ìƒì„±).
        ìºì‹œëœ ì¸ë±ìŠ¤ ì‚¬ìš©.

        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            query: ì§ˆë¬¸
            cache_key: ì‚¬ìš©í•  ìºì‹œ í‚¤ (ì„ íƒ)
        """
        config_path = Path(config_path)
        if not config_path.exists():
            print(f"âŒ Config file not found: {config_path}")
            return

        print(f"\nğŸ¤– Generating answer with config: {config_path}")
        print(f"Question: {query}")
        print("="*60)

        # Load config
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # Determine cache key
        if not cache_key:
            cache_key = self.cache_manager.generate_cache_key(config)

        # Check if cache exists
        if not self.cache_manager.exists(cache_key):
            print(f"âš ï¸  No cached index found. Creating new index...")
            cache_key = self.index_command(config_path)
            if not cache_key:
                return

        try:
            # Get cached index path directly (no copying)
            cache_path = self.cache_manager.get_cache_path(cache_key)

            # Initialize pipeline
            pipeline = RAGPipeline(config, verbose=False)
            pipeline.load_index(cache_path)  # Load from cache path directly

        except ComponentInitializationError as e:
            print(f"\nâŒ Failed to initialize pipeline: {e}")
            return
        except Exception as e:
            print(f"\nâŒ Failed to load pipeline: {e}")
            import traceback
            traceback.print_exc()
            return

        try:
            # Run full RAG pipeline
            start_time = time.time()
            result = pipeline.query(query)
            total_time = time.time() - start_time

            print(f"\nâœ¨ Answer generated in {total_time:.2f}s:")
            print("-"*60)
            print(result.get('answer', 'No answer generated'))

            print(f"\nğŸ“š Sources ({len(result.get('sources', []))} documents):")
            for i, source in enumerate(result.get('sources', []), 1):
                print(f"  {i}. {source.get('content', '')[:100]}...")

        except (RetrievalError, GenerationError) as e:
            print(f"\nâŒ Query failed: {e}")
        except Exception as e:
            print(f"\nâŒ Unexpected error: {e}")
            import traceback
            traceback.print_exc()

    def list_caches_command(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìºì‹œ ëª©ë¡ í‘œì‹œ."""
        print("\nğŸ“¦ Available Index Caches")
        print("="*60)

        caches = self.cache_manager.list_caches()

        if not caches:
            print("No caches found. Run 'python run.py index' to create one.")
            return

        print(f"Found {len(caches)} cached indices:\n")

        for cache in caches:
            cache_key = cache.get('cache_key', 'unknown')
            created = cache.get('created_at', 'unknown')
            config = cache.get('config', {})
            stats = cache.get('stats', {})

            print(f"ğŸ”‘ {cache_key}")
            print(f"   Created: {created}")
            print(f"   Chunks: {stats.get('total_chunks', 'N/A')}")
            print(f"   Config:")
            print(f"     - Chunking: {config.get('chunking', {}).get('strategy')} "
                  f"(size: {config.get('chunking', {}).get('chunk_size')})")
            print(f"     - Embedding: {config.get('embedding', {}).get('model')}")
            print(f"     - Vector DB: {config.get('vector_store', {}).get('type')}")
            print()

    def delete_cache_command(self, cache_key: str):
        """ìºì‹œ ì‚­ì œ."""
        print(f"\nğŸ—‘ï¸  Deleting cache: {cache_key}")

        if self.cache_manager.delete_cache(cache_key):
            print("âœ… Cache deleted successfully")
        else:
            print("âŒ Cache not found or deletion failed")

    def run_baseline(self):
        """Run baseline experiment (backward compatibility)."""
        print("\nğŸš€ Running Baseline RAG Experiment...")
        print("="*60)

        # Use new index command with baseline config
        baseline_config = self.experiments_dir / "baseline" / "config.yaml"
        if baseline_config.exists():
            # Index
            cache_key = self.index_command(str(baseline_config))

            if cache_key:
                # Run sample queries
                sample_queries = [
                    "ì‚´ì¸ì£„ì˜ í˜•ëŸ‰ì€?",
                    "ê³„ì•½ ìœ„ë°˜ì‹œ ì†í•´ë°°ìƒì€?"
                ]

                for query in sample_queries:
                    print(f"\n{'='*60}")
                    self.generate_command(str(baseline_config), query, cache_key)
        else:
            # Fallback to old baseline script
            baseline_script = self.experiments_dir / "baseline" / "baseline_rag.py"
            if baseline_script.exists():
                subprocess.run([sys.executable, str(baseline_script)])

    def init_experiment(self, name: str, description: str):
        """Initialize a new experiment for a team member."""
        print(f"\nğŸ”§ Initializing experiment for: {name}")

        # Create member directory and config
        member_dir = self.experiments_dir / "configs" / "members" / name
        member_dir.mkdir(parents=True, exist_ok=True)

        config_path = member_dir / f"{name}_config.yaml"

        if config_path.exists():
            print(f"âš ï¸  Config already exists: {config_path}")
            print(f"   Skipping creation. Edit existing file or delete it first.")
            return

        # Create results directory
        results_dir = self.experiments_dir / "results" / name
        results_dir.mkdir(parents=True, exist_ok=True)

        # Copy template
        template_path = self.experiments_dir / "configs" / "template_config.yaml"
        if template_path.exists():
            with open(template_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

            # Customize for this experimenter
            config['experiment_name'] = f"{name}_experiment"
            config['description'] = description
            config['author'] = name
            config['date'] = datetime.now().strftime('%Y-%m-%d')
            config['experiment']['results_dir'] = f"experiments/results/{name}/"

            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

            print(f"âœ… Experiment initialized for {name}!")
            print(f"   Config dir: {member_dir}")
            print(f"   Results dir: {results_dir}")
            print(f"\nğŸ“ Next steps:")
            print(f"   1. Edit config: vim {config_path}")
            print(f"   2. Run experiment: python run.py experiment --config {config_path}")
        else:
            print(f"âŒ Template not found: {template_path}")

    def experiment_command(self, config_path: str):
        """
        ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ (index + evaluate).
        ê¸°ì¡´ experiment ëª…ë ¹ì–´ì™€ í˜¸í™˜.
        """
        config_path = Path(config_path)
        if not config_path.exists():
            print(f"âŒ Config file not found: {config_path}")
            return

        print(f"\nğŸ”¬ Running full experiment with config: {config_path}")
        print("="*60)

        # Step 1: Index
        cache_key = self.index_command(str(config_path))
        if not cache_key:
            print("âŒ Indexing failed")
            return

        # Step 2: Load evaluation dataset
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # Load evaluation dataset if exists
        eval_set_path = config.get('evaluation', {}).get('eval_set', 'data/evaluation/test_qa.json')
        eval_dataset = self._load_eval_dataset(eval_set_path)

        if not eval_dataset:
            # Fallback: use queries from config
            eval_queries = config.get('experiment', {}).get('evaluation_queries', [
                "ì‚´ì¸ì£„ì˜ í˜•ëŸ‰ì€?",
                "ê³„ì•½ ìœ„ë°˜ì‹œ ì†í•´ë°°ìƒì€?",
                "ê°•ë„ì£„ì™€ ì ˆë„ì£„ì˜ ì°¨ì´ëŠ”?"
            ])
            eval_dataset = [{'id': f'q{i+1}', 'query': q} for i, q in enumerate(eval_queries)]

        print(f"\nğŸ“Š Running {len(eval_dataset)} evaluation queries...")
        results = []

        for eval_item in eval_dataset:
            query = eval_item['query']
            print(f"\nQuery: {query}")
            result = self._generate_and_collect(str(config_path), query, cache_key, eval_item)
            if result:
                results.append(result)

        # Save results to file
        results_dir = Path(config.get('experiment', {}).get('results_dir', 'experiments/results/'))
        results_dir.mkdir(parents=True, exist_ok=True)

        # Use config filename as part of result filename
        config_name = config_path.stem  # e.g., "wh_config_chunk_512"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = results_dir / f"{config_name}_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_name': config.get('experiment_name', 'unknown'),
                'config_path': str(config_path),
                'cache_key': cache_key,
                'timestamp': datetime.now().isoformat(),
                'results': results
            }, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… Experiment completed!")
        print(f"ğŸ“Š Results saved: {results_file}")

    def _generate_and_collect(self, config_path: str, query: str, cache_key: str, eval_item: Dict = None) -> Optional[Dict[str, Any]]:
        """
        Generate answer and collect result for saving.
        """
        config_path = Path(config_path)
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        try:
            # Get cached index path
            cache_path = self.cache_manager.get_cache_path(cache_key)

            # Initialize pipeline
            pipeline = RAGPipeline(config, verbose=False)
            pipeline.load_index(cache_path)

            # Run query with timing
            start_time = time.time()
            result = pipeline.query(query)
            total_time = time.time() - start_time

            # Extract retrieval and generation times if available
            retrieval_time = result.get('retrieval_time', None)
            generation_time = result.get('generation_time', None)

            # Print to console
            print(f"\nâœ¨ Answer generated in {total_time:.2f}s:")
            if retrieval_time and generation_time:
                print(f"   â±ï¸  Retrieval: {retrieval_time:.2f}s | Generation: {generation_time:.2f}s")
            print("-"*60)
            print(result.get('answer', 'No answer generated'))

            # Evaluate if ground truth exists
            evaluation_metrics = {}
            if eval_item and 'ground_truth' in eval_item:
                ground_truth = eval_item['ground_truth']
                answer = result.get('answer', '')

                # Basic answer quality metrics
                evaluation_metrics = evaluate_answer(ground_truth, answer)

                # Basic retrieval metrics
                k = config.get('retrieval', {}).get('top_k', 5)
                if 'relevant_docs' in eval_item:
                    retrieval_metrics = calculate_retrieval_metrics(
                        result.get('sources', []),
                        eval_item['relevant_docs'],
                        k=k
                    )
                    evaluation_metrics.update(retrieval_metrics)

                    # Advanced retrieval metrics (NDCG, Hit Rate)
                    evaluation_metrics['ndcg'] = calculate_ndcg(
                        result.get('sources', []),
                        eval_item['relevant_docs'],
                        k=k
                    )
                    evaluation_metrics['hit_rate'] = calculate_hit_rate(
                        result.get('sources', []),
                        eval_item['relevant_docs'],
                        k=k
                    )

                # Token usage
                token_info = calculate_token_usage(
                    query,
                    answer,
                    model=config.get('generation', {}).get('model', 'gpt-3.5-turbo')
                )
                evaluation_metrics['token_usage'] = token_info

                # RAGAS metrics (optional, requires OpenAI API)
                use_ragas = config.get('evaluation', {}).get('use_ragas', False)
                if use_ragas and os.getenv('OPENAI_API_KEY'):
                    contexts = [src.get('content', '') for src in result.get('sources', [])]
                    ragas_metrics = calculate_ragas_metrics(
                        query=query,
                        answer=answer,
                        contexts=contexts,
                        ground_truth=ground_truth,
                        use_ragas=True
                    )
                    evaluation_metrics['ragas'] = ragas_metrics

                print(f"ğŸ“Š Evaluation metrics:")
                print(f"   F1: {evaluation_metrics.get('f1_score', 0):.3f} | "
                      f"ROUGE-L: {evaluation_metrics.get('rouge_l', 0):.3f} | "
                      f"Semantic: {evaluation_metrics.get('semantic_similarity', 0):.3f}")
                if 'ndcg' in evaluation_metrics:
                    print(f"   NDCG@{k}: {evaluation_metrics.get('ndcg', 0):.3f} | "
                          f"Hit Rate@{k}: {evaluation_metrics.get('hit_rate', 0):.3f}")
                if 'token_usage' in evaluation_metrics and evaluation_metrics['token_usage'].get('total_tokens'):
                    print(f"   Tokens: {evaluation_metrics['token_usage']['total_tokens']}")

            print(f"ğŸ“š Sources ({len(result.get('sources', []))} documents)")

            # Return for saving
            return {
                'id': eval_item.get('id', '') if eval_item else '',
                'query': query,
                'answer': result.get('answer', ''),
                'ground_truth': eval_item.get('ground_truth', '') if eval_item else '',
                'sources': result.get('sources', []),
                'response_time': total_time,
                'num_sources': len(result.get('sources', [])),
                'evaluation_metrics': evaluation_metrics,
                'category': eval_item.get('category', '') if eval_item else '',
                'difficulty': eval_item.get('difficulty', '') if eval_item else ''
            }

        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _load_eval_dataset(self, eval_set_path: str) -> List[Dict]:
        """
        Load evaluation dataset from JSON file.
        """
        eval_path = Path(eval_set_path)

        if not eval_path.exists():
            print(f"âš ï¸ Evaluation dataset not found: {eval_path}")
            return []

        try:
            with open(eval_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)

            print(f"âœ… Loaded {len(dataset)} evaluation samples from {eval_path}")
            return dataset

        except Exception as e:
            print(f"âŒ Failed to load evaluation dataset: {e}")
            return []

    def compare_experiments(self, experiment_files: List[str]):
        """
        Compare multiple experiment results.
        """
        print(f"\nğŸ“Š Comparing {len(experiment_files)} experiments...")
        print("="*80)

        experiments = []

        # Load all experiment results
        for exp_file in experiment_files:
            exp_path = Path(exp_file)
            if not exp_path.exists():
                print(f"âš ï¸ File not found: {exp_file}")
                continue

            try:
                with open(exp_path, 'r', encoding='utf-8') as f:
                    exp_data = json.load(f)
                    experiments.append({
                        'name': exp_data.get('experiment_name', exp_path.stem),
                        'file': str(exp_path),
                        'data': exp_data
                    })
            except Exception as e:
                print(f"âŒ Failed to load {exp_file}: {e}")

        if len(experiments) < 2:
            print("âŒ Need at least 2 valid experiment files to compare")
            return

        # Calculate aggregated metrics for each experiment
        comparison = []

        for exp in experiments:
            results = exp['data'].get('results', [])

            if not results:
                continue

            # Aggregate metrics
            total_response_time = sum(r.get('response_time', 0) for r in results)
            avg_response_time = total_response_time / len(results) if results else 0

            # Average evaluation metrics
            metrics_sum = {
                'f1_score': 0,
                'bleu': 0,
                'rouge_l': 0,
                'semantic_similarity': 0,
                'exact_match': 0
            }

            for result in results:
                eval_metrics = result.get('evaluation_metrics', {})
                for key in metrics_sum:
                    metrics_sum[key] += eval_metrics.get(key, 0)

            avg_metrics = {k: v / len(results) for k, v in metrics_sum.items()} if results else metrics_sum

            comparison.append({
                'name': exp['name'],
                'num_queries': len(results),
                'avg_response_time': round(avg_response_time, 3),
                'avg_f1': round(avg_metrics['f1_score'], 3),
                'avg_bleu': round(avg_metrics['bleu'], 3),
                'avg_rouge_l': round(avg_metrics['rouge_l'], 3),
                'avg_semantic_sim': round(avg_metrics['semantic_similarity'], 3),
                'exact_match_rate': round(avg_metrics['exact_match'] / len(results) if results else 0, 3)
            })

        # Print comparison table
        print(f"\n{'Experiment':<30} {'Queries':<10} {'Avg Time':<12} {'F1':<8} {'ROUGE-L':<10} {'Semantic':<10}")
        print("-" * 80)

        for comp in comparison:
            print(f"{comp['name']:<30} {comp['num_queries']:<10} "
                  f"{comp['avg_response_time']:<12.3f} {comp['avg_f1']:<8.3f} "
                  f"{comp['avg_rouge_l']:<10.3f} {comp['avg_semantic_sim']:<10.3f}")

        # Find best experiment
        if comparison:
            best_f1 = max(comparison, key=lambda x: x['avg_f1'])
            best_speed = min(comparison, key=lambda x: x['avg_response_time'])

            print(f"\nğŸ† Best F1 Score: {best_f1['name']} ({best_f1['avg_f1']:.3f})")
            print(f"âš¡ Fastest: {best_speed['name']} ({best_speed['avg_response_time']:.3f}s)")

    def load_data_from_config(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Load data based on config with support for multiple loader types.

        Args:
            config: Configuration dictionary

        Returns:
            List of document dictionaries with 'content' and 'metadata'
        """
        data_config = config.get('data', {})
        loader_type = data_config.get('loader_type', 'default')

        # ========== [NEW] Traffic Law Loader Support ==========
        if loader_type == 'traffic':
            # Use traffic law specialized loader
            from scripts.traffic_law_data_loader import TrafficLawDataLoader
            from core.database import PostgreSQLClient

            loader = TrafficLawDataLoader(data_config.get('base_path'))

            # Get configuration options
            use_source = data_config.get('use_source', True)
            use_labeled = data_config.get('use_labeled', True)
            source_types = data_config.get('source_types', ['ë²•ë ¹', 'íŒê²°ë¬¸', 'ê²°ì •ë¡€', 'í•´ì„ë¡€'])
            labeled_types = data_config.get('labeled_types', ['ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'ê²°ì •ë¡€_QA', 'í•´ì„ë¡€_QA'])
            max_per_type = data_config.get('max_per_type', None)
            split = data_config.get('split', 'training')
            traffic_only = data_config.get('traffic_only', True)

            # Check if PostgreSQL enrichment is enabled
            use_db_enrichment = data_config.get('use_db_enrichment', False)

            if use_db_enrichment:
                # Load with PostgreSQL context enrichment
                try:
                    db_client = PostgreSQLClient()
                    dataset = loader.load_with_enrichment(
                        db_client=db_client,
                        use_source=use_source,
                        use_labeled=use_labeled,
                        source_types=source_types,
                        labeled_types=labeled_types,
                        max_per_type=max_per_type,
                        split=split,
                        traffic_only=traffic_only
                    )
                    db_client.close()
                except Exception as e:
                    print(f"âš ï¸ PostgreSQL enrichment failed: {e}")
                    print("   Falling back to basic traffic loader...")
                    dataset = loader.load_traffic_only(
                        use_source=use_source,
                        use_labeled=use_labeled,
                        source_types=source_types,
                        labeled_types=labeled_types,
                        max_per_type=max_per_type,
                        split=split
                    )
            else:
                # Load traffic data only (no DB enrichment)
                dataset = loader.load_traffic_only(
                    use_source=use_source,
                    use_labeled=use_labeled,
                    source_types=source_types,
                    labeled_types=labeled_types,
                    max_per_type=max_per_type,
                    split=split
                )

            return dataset
        # ======================================================

        # Default loader (existing criminal law loader)
        return self._load_documents_for_config(config)

    def _load_documents_for_config(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Load documents based on config (default loader)."""
        data_config = config.get('data', {})

        # Check if using real criminal law data
        if data_config.get('use_real_data', False):
            # Use criminal law data loader (supports source + labeled data)
            from scripts.criminal_law_data_loader import CriminalLawDataLoader
            loader = CriminalLawDataLoader()

            # Get configuration options
            use_source = data_config.get('use_source', True)
            use_labeled = data_config.get('use_labeled', True)
            source_types = data_config.get('source_types', ['ë²•ë ¹', 'íŒê²°ë¬¸', 'ê²°ì •ë¡€', 'í•´ì„ë¡€'])
            labeled_types = data_config.get('labeled_types', ['ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'ê²°ì •ë¡€_QA', 'í•´ì„ë¡€_QA'])
            max_per_type = data_config.get('max_per_type', None)
            split = data_config.get('split', 'training')  # 'training', 'validation', 'all'

            # Load dataset
            dataset = loader.load_dataset(
                use_source=use_source,
                use_labeled=use_labeled,
                source_types=source_types,
                labeled_types=labeled_types,
                max_per_type=max_per_type,
                split=split
            )

            documents = []
            for item in dataset:
                documents.append({
                    'content': item.get('content', item.get('text', '')),
                    'metadata': item.get('metadata', {})
                })

            return documents

        # Use data path
        data_path = Path(data_config.get('path', 'data/'))
        max_docs = data_config.get('max_documents', None)

        if data_path.exists():
            return self._load_documents(data_path, max_docs)
        else:
            # Use sample documents
            return self._get_sample_documents()

    def _load_documents(self, data_path: Path, max_docs: Optional[int] = None) -> List[Dict[str, Any]]:
        """Load documents from data directory."""
        documents = []

        # Load JSON files
        for json_file in data_path.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    documents.extend(data)
                else:
                    documents.append(data)

            if max_docs and len(documents) >= max_docs:
                break

        # Load text files
        if len(documents) < (max_docs or float('inf')):
            for txt_file in data_path.glob("*.txt"):
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    documents.append({
                        'content': content,
                        'metadata': {'source': txt_file.name}
                    })

                if max_docs and len(documents) >= max_docs:
                    break

        # If no documents found, use sample
        if not documents:
            documents = self._get_sample_documents()

        return documents[:max_docs] if max_docs else documents

    def _get_sample_documents(self) -> List[Dict[str, Any]]:
        """Get sample documents for testing."""
        return [
            {
                'content': "í˜•ë²• ì œ250ì¡° (ì‚´ì¸, ì¡´ì†ì‚´í•´) â‘  ì‚¬ëŒì„ ì‚´í•´í•œ ìëŠ” ì‚¬í˜•, ë¬´ê¸° ë˜ëŠ” 5ë…„ ì´ìƒì˜ ì§•ì—­ì— ì²˜í•œë‹¤. â‘¡ ìê¸° ë˜ëŠ” ë°°ìš°ìì˜ ì§ê³„ì¡´ì†ì„ ì‚´í•´í•œ ìëŠ” ì‚¬í˜•, ë¬´ê¸° ë˜ëŠ” 7ë…„ ì´ìƒì˜ ì§•ì—­ì— ì²˜í•œë‹¤.",
                'metadata': {'source': 'í˜•ë²•', 'article': '250', 'title': 'ì‚´ì¸ì£„'}
            },
            {
                'content': "í˜•ë²• ì œ329ì¡° (ì ˆë„) íƒ€ì¸ì˜ ì¬ë¬¼ì„ ì ˆì·¨í•œ ìëŠ” 6ë…„ ì´í•˜ì˜ ì§•ì—­ ë˜ëŠ” 1ì²œë§Œì› ì´í•˜ì˜ ë²Œê¸ˆì— ì²˜í•œë‹¤.",
                'metadata': {'source': 'í˜•ë²•', 'article': '329', 'title': 'ì ˆë„ì£„'}
            },
            {
                'content': "í˜•ë²• ì œ333ì¡° (ê°•ë„) í­í–‰ ë˜ëŠ” í˜‘ë°•ìœ¼ë¡œ íƒ€ì¸ì˜ ì¬ë¬¼ì„ ê°•ì·¨í•˜ê±°ë‚˜ ê¸°íƒ€ ì¬ì‚°ìƒì˜ ì´ìµì„ ì·¨ë“í•˜ê±°ë‚˜ ì œì‚¼ìë¡œ í•˜ì—¬ê¸ˆ ì´ë¥¼ ì·¨ë“í•˜ê²Œ í•œ ìëŠ” 3ë…„ ì´ìƒì˜ ìœ ê¸°ì§•ì—­ì— ì²˜í•œë‹¤.",
                'metadata': {'source': 'í˜•ë²•', 'article': '333', 'title': 'ê°•ë„ì£„'}
            },
            {
                'content': "ë¯¼ë²• ì œ390ì¡° (ì±„ë¬´ë¶ˆì´í–‰ê³¼ ì†í•´ë°°ìƒ) ì±„ë¬´ìê°€ ì±„ë¬´ì˜ ë‚´ìš©ì— ì¢‡ì€ ì´í–‰ì„ í•˜ì§€ ì•„ë‹ˆí•œ ë•Œì—ëŠ” ì±„ê¶ŒìëŠ” ì†í•´ë°°ìƒì„ ì²­êµ¬í•  ìˆ˜ ìˆë‹¤.",
                'metadata': {'source': 'ë¯¼ë²•', 'article': '390', 'title': 'ì†í•´ë°°ìƒ'}
            }
        ]


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="RAG Pipeline Experiment CLI v2",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        'command',
        choices=['index', 'search', 'generate', 'experiment', 'list-caches',
                 'delete-cache', 'baseline', 'init-experiment', 'compare'],
        help="""Commands:
  index         - Index documents (create or use cache)
  search        - Search in indexed documents
  generate      - Generate answer using RAG pipeline
  experiment    - Run full experiment (index + evaluate)
  list-caches   - List available index caches
  delete-cache  - Delete a specific cache
  baseline      - Run baseline experiment
  init-experiment - Initialize new experiment for team member
  compare       - Compare experiment results
"""
    )

    parser.add_argument('--config', help='Config file path')
    parser.add_argument('--query', help='Search query or question')
    parser.add_argument('--cache-key', help='Specific cache key to use')
    parser.add_argument('--force', action='store_true', help='Force re-indexing (ignore cache)')
    parser.add_argument('--name', help='Experimenter name (for init-experiment)')
    parser.add_argument('--description', help='Experiment description')
    parser.add_argument('--experiments', nargs='+', help='Experiment result files to compare')

    args = parser.parse_args()

    cli = RAGExperimentCLI()

    if args.command == 'index':
        if not args.config:
            print("âŒ --config required for index")
            sys.exit(1)
        cli.index_command(args.config, args.force)

    elif args.command == 'search':
        if not args.config or not args.query:
            print("âŒ --config and --query required for search")
            sys.exit(1)
        cli.search_command(args.config, args.query, args.cache_key)

    elif args.command == 'generate':
        if not args.config or not args.query:
            print("âŒ --config and --query required for generate")
            sys.exit(1)
        cli.generate_command(args.config, args.query, args.cache_key)

    elif args.command == 'experiment':
        if not args.config:
            print("âŒ --config required for experiment")
            sys.exit(1)
        cli.experiment_command(args.config)

    elif args.command == 'list-caches':
        cli.list_caches_command()

    elif args.command == 'delete-cache':
        if not args.cache_key:
            print("âŒ --cache-key required for delete-cache")
            sys.exit(1)
        cli.delete_cache_command(args.cache_key)

    elif args.command == 'baseline':
        cli.run_baseline()

    elif args.command == 'init-experiment':
        if not args.name:
            print("âŒ --name required for init-experiment")
            sys.exit(1)
        description = args.description or f"Experiment by {args.name}"
        cli.init_experiment(args.name, description)

    elif args.command == 'compare':
        if not args.experiments or len(args.experiments) < 2:
            print("âŒ --experiments required (at least 2 result files)")
            sys.exit(1)
        cli.compare_experiments(args.experiments)


if __name__ == "__main__":
    main()