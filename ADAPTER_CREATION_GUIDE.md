# ğŸ¯ QDoRA Adapter ì œì‘ ê°€ì´ë“œ

> **êµí†µì‚¬ê³  ì „ë¬¸ Adapter ì œì‘ ì›Œí¬í”Œë¡œìš°**

## ğŸ“Œ QDoRAë€?

**QDoRA (Quantized Domain-Specific Adapter)**ëŠ” QLoRAì˜ í™•ì¥ ê°œë…ìœ¼ë¡œ, íŠ¹ì • ë„ë©”ì¸(ë²•ë¥  ì „ë¬¸ ë¶„ì•¼)ì— íŠ¹í™”ëœ ê²½ëŸ‰ ì–´ëŒ‘í„°ì…ë‹ˆë‹¤.

### QLoRA vs QDoRA
```
QLoRA: ë²”ìš© fine-tuning (ëª¨ë“  íƒœìŠ¤í¬)
QDoRA: ë„ë©”ì¸ íŠ¹í™” fine-tuning (êµí†µì‚¬ê³  ì „ë¬¸)
  â†“
  - ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ë§Œ ì‚¬ìš©
  - ë„ë©”ì¸ íŠ¹í™” í‰ê°€ ì§€í‘œ
  - ë„ë©”ì¸ íŠ¹í™” System Prompt
```

---

## ğŸ—‚ï¸ Phase 1: ë°ì´í„° íŒŒì´í”„ë¼ì¸ (1ì£¼)

### 1.1 êµí†µì‚¬ê³  íŒë¡€ í•„í„°ë§

**ëª©ì **: ë¡œì»¬ 32,525ê°œ íŒë¡€ì—ì„œ êµí†µì‚¬ê³  ê´€ë ¨ ì¶”ì¶œ

**íŒŒì¼**: `scripts/filter_traffic_data.py`

**í‚¤ì›Œë“œ ì „ëµ**:
```python
TRAFFIC_KEYWORDS = [
    # í•µì‹¬ ë²•ë¥ 
    "êµí†µì‚¬ê³ ì²˜ë¦¬íŠ¹ë¡€ë²•",
    "ë„ë¡œêµí†µë²•",

    # ë²”ì£„ ìœ í˜•
    "ìŒì£¼ìš´ì „", "ë¬´ë©´í—ˆìš´ì „", "ëº‘ì†Œë‹ˆ",
    "ë‚œí­ìš´ì „", "ë³´ë³µìš´ì „",

    # ê²°ê³¼
    "ì¤‘ìƒí•´", "êµí†µì‚¬ê³ ", "ì¸ëª…ì‚¬ê³ ",

    # ê°€ì¤‘ ì²˜ë²Œ
    "íŠ¹ì •ë²”ì£„ê°€ì¤‘ì²˜ë²Œë²•", "íŠ¹ê°€ë²•",
]
```

**í•„í„°ë§ ë¡œì§**:
```python
1. CSV íŒë¡€ íŒŒì¼ ì½ê¸° (04.í˜•ì‚¬ë²• í´ë”)
2. "íŒì‹œì‚¬í•­" + "íŒê²°ìš”ì§€" í•„ë“œì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
3. 3ê°œ ì´ìƒ í‚¤ì›Œë“œ ë§¤ì¹­ â†’ êµí†µì‚¬ê³  íŒë¡€ë¡œ ë¶„ë¥˜
4. ì¤‘ë³µ ì œê±° (íŒë¡€ì¼ë ¨ë²ˆí˜¸ ê¸°ì¤€)
5. ê²°ê³¼ ì €ì¥: data/raw/traffic_precedents.json
```

**ëª©í‘œ ê²°ê³¼**: 3,000-4,000ê±´

---

### 1.2 Open API ì¶”ê°€ í¬ë¡¤ë§

**ëª©ì **: ìµœì‹  êµí†µì‚¬ê³  íŒë¡€ ë³´ê°•

**íŒŒì¼**: `scripts/crawl_traffic_precedents.py`

**API í™œìš©**:
```python
# êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° Open API
endpoint = "https://www.law.go.kr/DRF/lawSearch.do"
params = {
    "OC": "your_api_key",
    "target": "prec",  # íŒë¡€
    "query": "ìŒì£¼ìš´ì „+êµí†µì‚¬ê³ ",
    "display": 100,
    "sort": "date",  # ìµœì‹ ìˆœ
}
```

**í¬ë¡¤ë§ ì „ëµ**:
1. í‚¤ì›Œë“œë³„ë¡œ API í˜¸ì¶œ (ìŒì£¼ìš´ì „, ë¬´ë©´í—ˆ, ëº‘ì†Œë‹ˆ ë“±)
2. 2020ë…„ ì´í›„ ìµœì‹  íŒë¡€ ìš°ì„ 
3. ë¡œì»¬ ë°ì´í„°ì™€ ì¤‘ë³µ ì²´í¬ (íŒë¡€ë²ˆí˜¸)
4. ê²°ê³¼ ì €ì¥: `data/raw/traffic_precedents_api.json`

**ëª©í‘œ ê²°ê³¼**: 1,000-2,000ê±´

---

### 1.3 Instruction Tuning ë°ì´í„° ë³€í™˜

**ëª©ì **: QDoRA í•™ìŠµìš© Instruction Format ìƒì„±

**íŒŒì¼**: `scripts/prepare_instruction_data.py`

**ë³€í™˜ ì „ëµ**:

#### ì…ë ¥ (CSV íŒë¡€)
```csv
íŒë¡€ì¼ë ¨ë²ˆí˜¸,êµ¬ë¶„,ë¬¸ì¥ë²ˆí˜¸,ë‚´ìš©
100029,íŒì‹œì‚¬í•­,1,"ì—…ë¬´ìƒ ê³¼ì‹¤ì¹˜ì‚¬ì£„ë¡œ..."
100029,íŒê²°ìš”ì§€,2,"ì—…ë¬´ìƒ ê³¼ì‹¤ì¹˜ì‚¬ì£„ë¡œ..."
100029,íŒë¡€ë‚´ìš©,5,"ã€í”¼ ê³  ì¸ã€‘"
...
```

#### ì¶œë ¥ (Instruction Format)
```json
{
  "instruction": "ë‹¤ìŒ êµí†µì‚¬ê³  ì‚¬ê±´ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë²•ë¥ ê³¼ ì˜ˆìƒ íŒê²°ì„ ì œì‹œí•˜ì„¸ìš”.",
  "input": "í”¼ê³ ì¸ì€ ì—…ë¬´ìƒ ê³¼ì‹¤ë¡œ ì°¨ëŸ‰ì„ ìš´ì „í•˜ë‹¤ í”¼í•´ìë¥¼ ì‚¬ë§ì¼€ í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¹ì‹œ ì°¨ëŸ‰ ìœ ë¦¬ì°½ì— ì„±ì• ê°€ ë¼ì–´ ìˆì—ˆê³ , ë…¸í­ì´ 2.4mì¸ ë¹„í¬ì¥ ë„ë¡œì˜€ìŠµë‹ˆë‹¤.",
  "output": "## ë²•ì  ë¶„ì„\n\n### 1. ê´€ë ¨ ë²•ë ¹\n- êµí†µì‚¬ê³ ì²˜ë¦¬íŠ¹ë¡€ë²• ì œ3ì¡° ì œ1í•­\n- í˜•ë²• ì œ268ì¡° (ì—…ë¬´ìƒê³¼ì‹¤ì¹˜ì‚¬)\n\n### 2. ìŸì \n- ì „ë°©ì£¼ì‹œ ì˜ë¬´ ìœ„ë°˜ ì—¬ë¶€\n- ì„±ì•  ì œê±° ì˜ë¬´ ìœ„ë°˜\n\n### 3. íŒë¡€ ë¶„ì„\në³¸ ì‚¬ê±´ì—ì„œ ì›ì‹¬ì€ í”¼ê³ ì¸ì˜ ê³¼ì‹¤ ë‚´ìš©ì„ ëª…í™•íˆ ë°íˆì§€ ì•Šì•„ íŒŒê¸°ë˜ì—ˆìŠµë‹ˆë‹¤. ì—…ë¬´ìƒê³¼ì‹¤ì¹˜ì‚¬ì£„ë¥¼ ì ìš©í•˜ë ¤ë©´ êµ¬ì²´ì ì¸ ê³¼ì‹¤ ë‚´ìš©ì´ ì…ì¦ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n### 4. ê²°ë¡ \nì‹¬ë¦¬ ë¯¸ì§„ìœ¼ë¡œ ì›ì‹¬ íŒŒê¸° í™˜ì†¡"
}
```

**ë³€í™˜ ë¡œì§**:
```python
def create_instruction_data(precedent):
    # 1. íŒì‹œì‚¬í•­ â†’ instruction
    instruction = f"ë‹¤ìŒ êµí†µì‚¬ê³  ì‚¬ê±´ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë²•ë¥ ê³¼ ì˜ˆìƒ íŒê²°ì„ ì œì‹œí•˜ì„¸ìš”."

    # 2. ì‚¬ì‹¤ê´€ê³„ ì¶”ì¶œ â†’ input
    input_text = extract_facts(precedent["íŒë¡€ë‚´ìš©"])

    # 3. íŒê²°ìš”ì§€ + ë²•ë¦¬ â†’ output
    output = format_legal_analysis(
        ë²•ë ¹=precedent["ì°¸ì¡°ì¡°ë¬¸"],
        íŒì‹œì‚¬í•­=precedent["íŒì‹œì‚¬í•­"],
        íŒê²°ìš”ì§€=precedent["íŒê²°ìš”ì§€"],
    )

    return {"instruction": instruction, "input": input_text, "output": output}
```

**ëª©í‘œ ê²°ê³¼**: `data/traffic_instruction_5k.json` (5,000ê±´)

---

## ğŸš€ Phase 2: Colab QDoRA í•™ìŠµ (3-5ì¼)

### 2.1 í•™ìŠµ í™˜ê²½ êµ¬ì„±

**íŒŒì¼**: `notebooks/train_traffic_qdora.ipynb` (Colab Pro+)

**GPU**: A100 (40GB) ê¶Œì¥

---

### 2.2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

```python
# ============================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# ============================================
!pip install -q transformers peft bitsandbytes accelerate trl datasets

# ============================================
# 2. ë°ì´í„° ë¡œë“œ
# ============================================
from datasets import load_dataset

# Google Drive ë§ˆìš´íŠ¸ í›„ ë°ì´í„° ì—…ë¡œë“œ
dataset = load_dataset("json", data_files="traffic_instruction_5k.json")
train_test = dataset["train"].train_test_split(test_size=0.1)

# ============================================
# 3. Kosaul ëª¨ë¸ ë¡œë“œ (4-bit Quantization)
# ============================================
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "Bllossom/llama-3.2-Korean-Bllossom-3B"  # Kosaul ê¸°ë°˜

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# ============================================
# 4. QDoRA ì„¤ì • (Domain-Specific LoRA)
# ============================================
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ëª¨ë¸ ì¤€ë¹„
model = prepare_model_for_kbit_training(model)

# LoRA ì„¤ì • (êµí†µì‚¬ê³  ë„ë©”ì¸ íŠ¹í™”)
qdora_config = LoraConfig(
    r=16,                    # Rank (adapter í¬ê¸°)
    lora_alpha=32,           # Scaling factor
    target_modules=[         # Fine-tuneí•  ë ˆì´ì–´
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, qdora_config)
model.print_trainable_parameters()
# ì¶œë ¥ ì˜ˆ: trainable params: 42M (1.2%) || all params: 3.4B

# ============================================
# 5. í•™ìŠµ ì„¤ì •
# ============================================
from transformers import TrainingArguments
from trl import SFTTrainer

training_args = TrainingArguments(
    output_dir="./traffic_qdora_v1",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    warmup_steps=50,
    lr_scheduler_type="cosine",
)

# ============================================
# 6. SFT Trainerë¡œ í•™ìŠµ
# ============================================
trainer = SFTTrainer(
    model=model,
    train_dataset=train_test["train"],
    eval_dataset=train_test["test"],
    peft_config=qdora_config,
    dataset_text_field="text",  # instruction + input + output ê²°í•©
    max_seq_length=2048,
    tokenizer=tokenizer,
    args=training_args,
)

# í•™ìŠµ ì‹œì‘ (4-8ì‹œê°„ ì˜ˆìƒ)
trainer.train()

# ============================================
# 7. Adapter ì €ì¥
# ============================================
model.save_pretrained("./traffic_adapter_v1")
tokenizer.save_pretrained("./traffic_adapter_v1")

# Google Driveì— ë°±ì—…
!cp -r ./traffic_adapter_v1 /content/drive/MyDrive/lawlaw_adapters/
```

---

### 2.3 í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| **r** | 16 | LoRA rank (í´ìˆ˜ë¡ í‘œí˜„ë ¥â†‘, ìš©ëŸ‰â†‘) |
| **lora_alpha** | 32 | Scaling factor (ë³´í†µ rì˜ 2ë°°) |
| **learning_rate** | 2e-4 | í•™ìŠµë¥  (QLoRA ê¶Œì¥ê°’) |
| **batch_size** | 4 | A100 ê¸°ì¤€ ìµœì ê°’ |
| **epochs** | 3 | ê³¼ì í•© ë°©ì§€ |
| **max_seq_length** | 2048 | íŒë¡€ ê¸¸ì´ ê³ ë ¤ |

---

## ğŸ“¦ Phase 3: Adapter ë°°í¬ (3ì¼)

### 3.1 Adapter ì¶”ì¶œ ë° ê²€ì¦

**Colabì—ì„œ ìƒì„±ëœ íŒŒì¼**:
```
traffic_adapter_v1/
â”œâ”€â”€ adapter_model.bin      # 50-200MB (í•µì‹¬!)
â”œâ”€â”€ adapter_config.json    # LoRA ì„¤ì •
â””â”€â”€ tokenizer files
```

**ê²€ì¦ í…ŒìŠ¤íŠ¸**:
```python
# Adapter ë¡œë“œ í…ŒìŠ¤íŠ¸
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, "./traffic_adapter_v1")

# ìƒ˜í”Œ ì¶”ë¡ 
prompt = "ìŒì£¼ìš´ì „ 3íšŒ, ë¬´ë©´í—ˆ, ì¤‘ìƒí•´ ì‚¬ê±´ì˜ ì˜ˆìƒ í˜•ëŸ‰ì€?"
output = generate(model, prompt)
print(output)
```

---

### 3.2 Ollama í†µí•©

**Option 1: Adapter Merge (ê¶Œì¥)**
```python
# Adapterë¥¼ base modelì— ë³‘í•©
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, "./traffic_adapter_v1")
merged_model = model.merge_and_unload()

# GGUFë¡œ ë³€í™˜ (llama.cpp ì‚¬ìš©)
# 1. HF ì²´í¬í¬ì¸íŠ¸ ì €ì¥
merged_model.save_pretrained("./kosaul_traffic_merged")

# 2. GGUF ë³€í™˜ (ë¡œì»¬ì—ì„œ)
# python llama.cpp/convert.py ./kosaul_traffic_merged
# llama.cpp/quantize ./kosaul_traffic_merged/ggml-model-f16.gguf ./kosaul_traffic_q4.gguf Q4_K_M
```

**Modelfile ì‘ì„±**: `Modelfile_traffic_v1`
```dockerfile
FROM ./models/kosaul_traffic_q4.gguf

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"

SYSTEM """
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ë³€í˜¸ì‚¬ AIì…ë‹ˆë‹¤.

ì „ë¬¸ ë¶„ì•¼:
- ìŒì£¼ìš´ì „, ë¬´ë©´í—ˆìš´ì „, ëº‘ì†Œë‹ˆ
- êµí†µì‚¬ê³ ì²˜ë¦¬íŠ¹ë¡€ë²•, íŠ¹ê°€ë²• ì ìš© ì‚¬ê±´
- ì¸ëª…ì‚¬ê³ , ì¤‘ìƒí•´ ì‚¬ê±´

ë‹µë³€ ì›ì¹™:
1. íŒë¡€ ê¸°ë°˜ ë¶„ì„ (ì¶œì²˜ ëª…ì‹œ)
2. ìŸì  ì¤‘ì‹¬ ì„¤ëª…
3. êµ¬ì²´ì  ì–‘í˜• ì œì‹œ
4. í•©ì˜/ë³€ë¡  ì „ëµ ì œì•ˆ
"""

TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""
```

**Ollama ëª¨ë¸ ìƒì„±**:
```bash
cd /Users/jaehyungpark/Documents/libraries/lawlaw
ollama create lawlaw:traffic -f Modelfile_traffic_v1
```

---

### 3.3 ë¡œì»¬ í…ŒìŠ¤íŠ¸ ë° í‰ê°€

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: `tests/traffic_test_cases.json`

```json
[
  {
    "case": "ìŒì£¼ìš´ì „ 3íšŒ + ë¬´ë©´í—ˆ + ì¤‘ìƒí•´",
    "expected": "íŠ¹ê°€ë²• ì œ5ì¡°ì˜11, 2-5ë…„ ì§•ì—­",
    "accuracy_threshold": 0.8
  },
  {
    "case": "ëº‘ì†Œë‹ˆ + ì‚¬ë§ì‚¬ê³ ",
    "expected": "íŠ¹ê°€ë²• ì œ5ì¡°ì˜3, ë¬´ê¸° ë˜ëŠ” 3ë…„ ì´ìƒ",
    "accuracy_threshold": 0.9
  }
]
```

**í‰ê°€ ìŠ¤í¬ë¦½íŠ¸**: `scripts/evaluate_adapter.py`

```python
import ollama

def evaluate_adapter():
    baseline_model = "kosaul:latest"
    adapter_model = "lawlaw:traffic"

    test_cases = load_json("tests/traffic_test_cases.json")

    for case in test_cases:
        # Baseline ì‘ë‹µ
        baseline_response = ollama.chat(baseline_model, case["case"])

        # Adapter ì‘ë‹µ
        adapter_response = ollama.chat(adapter_model, case["case"])

        # í‰ê°€ (GPT-4 ë˜ëŠ” ì‚¬ëŒì´ ì ìˆ˜ ë§¤ê¹€)
        baseline_score = evaluate_response(baseline_response, case["expected"])
        adapter_score = evaluate_response(adapter_response, case["expected"])

        print(f"Case: {case['case']}")
        print(f"Baseline: {baseline_score:.2f}")
        print(f"Adapter: {adapter_score:.2f}")
        print(f"Improvement: {adapter_score - baseline_score:.2f}\n")
```

**ëª©í‘œ ì„±ëŠ¥**:
- Baseline ëŒ€ë¹„ +30% ì •í™•ë„ í–¥ìƒ
- íŒë¡€ ì¸ìš©ë¥  90%+
- ë²•ë¦¬ í•´ì„ ì •í™•ë„ 85%+

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

| ì§€í‘œ | Baseline Kosaul | Traffic Adapter | ê°œì„  |
|------|----------------|-----------------|------|
| **êµí†µì‚¬ê³  ì •í™•ë„** | 65% | **90%** | +38% |
| **íŒë¡€ ë§¤ì¹­** | 70% | **95%** | +36% |
| **ë²•ë¦¬ í•´ì„** | 60% | **85%** | +42% |
| **ì¶”ë¡  ì†ë„** | 20 tok/s | 20 tok/s | ë™ì¼ |
| **ëª¨ë¸ í¬ê¸°** | 4.9GB | **5.1GB** | +200MB |

---

## ğŸ“‚ ìµœì¢… íŒŒì¼ êµ¬ì¡°

```
lawlaw/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ filter_traffic_data.py           # êµí†µì‚¬ê³  íŒë¡€ í•„í„°ë§
â”‚   â”œâ”€â”€ crawl_traffic_precedents.py      # Open API í¬ë¡¤ë§
â”‚   â”œâ”€â”€ prepare_instruction_data.py      # Instruction ë³€í™˜
â”‚   â””â”€â”€ evaluate_adapter.py              # ì„±ëŠ¥ í‰ê°€
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ train_traffic_qdora.ipynb        # Colab í•™ìŠµ ë…¸íŠ¸ë¶
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ traffic_precedents.json      # ë¡œì»¬ í•„í„°ë§ ê²°ê³¼
â”‚   â”‚   â””â”€â”€ traffic_precedents_api.json  # API í¬ë¡¤ë§ ê²°ê³¼
â”‚   â””â”€â”€ traffic_instruction_5k.json      # í•™ìŠµ ë°ì´í„°
â”‚
â”œâ”€â”€ adapters/
â”‚   â””â”€â”€ traffic_v1/
â”‚       â”œâ”€â”€ adapter_model.bin            # Adapter weights (50-200MB)
â”‚       â””â”€â”€ adapter_config.json          # LoRA config
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ kosaul_traffic_q4.gguf          # Merged + quantized model
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ traffic_test_cases.json          # í‰ê°€ ì¼€ì´ìŠ¤
â”‚
â”œâ”€â”€ Modelfile_traffic_v1                 # Ollama ì„¤ì •
â””â”€â”€ ADAPTER_CREATION_GUIDE.md            # ì´ ë¬¸ì„œ
```

---

## â±ï¸ íƒ€ì„ë¼ì¸

### Week 1: ë°ì´í„° ì¤€ë¹„
- Day 1-2: í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë° ì‹¤í–‰
- Day 3-4: Open API í¬ë¡¤ë§
- Day 5-7: Instruction ë°ì´í„° ë³€í™˜ ë° ê²€ì¦

### Week 2: í•™ìŠµ ë° ìµœì í™”
- Day 1: Colab í™˜ê²½ êµ¬ì„±
- Day 2: í•™ìŠµ ì‹¤í–‰ (4-8ì‹œê°„)
- Day 3-4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- Day 5: ì¬í•™ìŠµ (í•„ìš”ì‹œ)

### Week 3: ë°°í¬ ë° í…ŒìŠ¤íŠ¸
- Day 1-2: Adapter merge ë° GGUF ë³€í™˜
- Day 3: Ollama í†µí•©
- Day 4-5: ì„±ëŠ¥ í‰ê°€ ë° ë¬¸ì„œí™”

**ì´ ì†Œìš” ê¸°ê°„**: 2-3ì£¼

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„: Phase 2 í™•ì¥

êµí†µì‚¬ê³  Adapter ì™„ì„± í›„:

1. **í˜•ì‚¬ ì¼ë°˜ Adapter** (ì ˆë„, ì‚¬ê¸°, í­í–‰)
2. **ë§ˆì•½ë²”ì£„ Adapter**
3. **ê¸°ì—…ë²”ì£„ Adapter**
4. **ë¯¼ì‚¬ Adapter**
5. **ì„±ë²”ì£„ Adapter**

ê° AdapterëŠ” ë™ì¼í•œ ì›Œí¬í”Œë¡œìš°ë¡œ 2ì£¼ ë‚´ ì œì‘ ê°€ëŠ¥.

---

## ğŸ“Œ ì°¸ê³  ìë£Œ

- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [Ollama Modelfile Syntax](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- [êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° Open API](https://open.law.go.kr/)
