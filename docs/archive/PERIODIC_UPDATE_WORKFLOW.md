# ì£¼ê¸°ì  í¬ë¡¤ë§ ë°ì´í„° ì—…ë°ì´íŠ¸ ì›Œí¬í”Œë¡œìš°

## ğŸ“‹ ê°œìš”

ë²•ì œì²˜ OpenAPIì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ìƒˆë¡œìš´ íŒë¡€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì „ì²´ DBì— ì¶”ê°€í•˜ëŠ” ìë™í™” ì‹œìŠ¤í…œ

---

## ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
1. ì£¼ê¸°ì  í¬ë¡¤ë§ (ë§¤ì¼/ë§¤ì£¼)
   â†“
2. ì‹ ê·œ ë°ì´í„° ì‹ë³„
   â†“
3. ë°ì´í„° ê²€ì¦ ë° ì¤‘ë³µ ì œê±°
   â†“
4. í†µí•© JSON íŒŒì¼ ì—…ë°ì´íŠ¸
   â†“
5. ì„ë² ë”© ìƒì„± (ì‹ ê·œ ë°ì´í„°ë§Œ)
   â†“
6. Vector DB ì—…ë°ì´íŠ¸
   â†“
7. PostgreSQL DB ë™ê¸°í™”
   â†“
8. ì•Œë¦¼ ë° ë¡œê·¸
```

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ìŠ¤ì¼€ì¤„ëŸ¬ (Cron/Celery)                     â”‚
â”‚                  ë§¤ì¼ ì˜¤ì „ 2ì‹œ ìë™ ì‹¤í–‰                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              1. í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰                          â”‚
â”‚  scripts/crawl_legal_cases.py                                â”‚
â”‚  â†’ ë²•ì œì²˜ API í˜¸ì¶œ                                           â”‚
â”‚  â†’ êµí†µ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰                                     â”‚
â”‚  â†’ ì‹ ê·œ íŒë¡€ ìˆ˜ì§‘                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. ì‹ ê·œ ë°ì´í„° ì‹ë³„                              â”‚
â”‚  scripts/identify_new_cases.py                               â”‚
â”‚  â†’ ê¸°ì¡´ DBì™€ ë¹„êµ                                            â”‚
â”‚  â†’ íŒë¡€ì¼ë ¨ë²ˆí˜¸ë¡œ ì¤‘ë³µ ì²´í¬                                    â”‚
â”‚  â†’ ì‹ ê·œ ì¼€ì´ìŠ¤ë§Œ ì¶”ì¶œ                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              3. ë°ì´í„° ë³‘í•© ë° ê²€ì¦                           â”‚
â”‚  scripts/merge_new_data.py                                   â”‚
â”‚  â†’ ë°ì´í„° í˜•ì‹ ê²€ì¦                                          â”‚
â”‚  â†’ í†µí•© JSONì— ì¶”ê°€                                          â”‚
â”‚  â†’ ë°±ì—… ìƒì„±                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4. ì„ë² ë”© ìƒì„± (ì‹ ê·œë§Œ)                          â”‚
â”‚  scripts/create_embeddings_incremental.py                    â”‚
â”‚  â†’ OpenAI APIë¡œ ì„ë² ë”© ìƒì„±                                   â”‚
â”‚  â†’ ì‹ ê·œ ë°ì´í„°ë§Œ ì²˜ë¦¬                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5. Vector DB ì—…ë°ì´íŠ¸                            â”‚
â”‚  Qdrant/Pinecone                                             â”‚
â”‚  â†’ ì‹ ê·œ ë²¡í„° ì¶”ê°€                                            â”‚
â”‚  â†’ ì¸ë±ìŠ¤ ìë™ ê°±ì‹                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              6. PostgreSQL ë™ê¸°í™”                             â”‚
â”‚  â†’ documents í…Œì´ë¸” ì¶”ê°€                                      â”‚
â”‚  â†’ document_ai_labels ì¶”ê°€                                   â”‚
â”‚  â†’ í†µê³„ ì—…ë°ì´íŠ¸                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              7. ì•Œë¦¼ ë° ë¡œê·¸                                  â”‚
â”‚  â†’ Slack/Email ì•Œë¦¼                                          â”‚
â”‚  â†’ ì—…ë°ì´íŠ¸ ë¡œê·¸ ê¸°ë¡                                         â”‚
â”‚  â†’ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ê°±ì‹                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ íŒŒì¼ êµ¬ì¡°

```
ai-camp-1st-llm-agent-service-project-2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ unified_traffic_data_latest.json        # ìµœì‹  í†µí•© ë°ì´í„°
â”‚   â”œâ”€â”€ unified_traffic_data_20251103.json      # ë°±ì—… (ë‚ ì§œë³„)
â”‚   â””â”€â”€ crawled_raw/                            # í¬ë¡¤ë§ ì›ë³¸
â”‚       â”œâ”€â”€ traffic_legal_data_20251103.json
â”‚       â””â”€â”€ traffic_legal_data_20251104.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ crawl_legal_cases.py                    # í¬ë¡¤ë§
â”‚   â”œâ”€â”€ identify_new_cases.py                   # ì‹ ê·œ ì‹ë³„ âœ… ìƒˆë¡œ ì‘ì„±
â”‚   â”œâ”€â”€ merge_new_data.py                       # ë°ì´í„° ë³‘í•© âœ… ìƒˆë¡œ ì‘ì„±
â”‚   â”œâ”€â”€ create_embeddings_incremental.py        # ì¦ë¶„ ì„ë² ë”© âœ… ìƒˆë¡œ ì‘ì„±
â”‚   â”œâ”€â”€ update_vector_db.py                     # Vector DB ì—…ë°ì´íŠ¸ âœ… ìƒˆë¡œ ì‘ì„±
â”‚   â””â”€â”€ schedule_daily_update.py                # ìŠ¤ì¼€ì¤„ëŸ¬ âœ… ìƒˆë¡œ ì‘ì„±
â”‚
â””â”€â”€ logs/
    â””â”€â”€ update_20251103.log                     # ì—…ë°ì´íŠ¸ ë¡œê·¸
```

---

## ğŸ”§ êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸

### 1. ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„

```python
# scripts/identify_new_cases.py

import json
from pathlib import Path
from typing import Set, List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_existing_case_ids(unified_file: str) -> Set[str]:
    """
    ê¸°ì¡´ í†µí•© ë°ì´í„°ì—ì„œ íŒë¡€ì¼ë ¨ë²ˆí˜¸ ì¶”ì¶œ

    Returns:
        ê¸°ì¡´ ì¼€ì´ìŠ¤ ID ì§‘í•©
    """
    with open(unified_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    existing_ids = set()

    for case_type in ['íŒë¡€', 'ê²°ì •ë¡€', 'í•´ì„ë¡€', 'ë²•ë ¹']:
        for case in data.get(case_type, []):
            case_id = case.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸')
            if case_id:
                existing_ids.add(str(case_id))

    return existing_ids


def identify_new_cases(
    crawled_file: str,
    unified_file: str
) -> Dict[str, List[Dict]]:
    """
    í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ ì‹ ê·œ ì¼€ì´ìŠ¤ë§Œ ì¶”ì¶œ

    Args:
        crawled_file: í¬ë¡¤ë§ ë°ì´í„° íŒŒì¼
        unified_file: ê¸°ì¡´ í†µí•© ë°ì´í„° íŒŒì¼

    Returns:
        ì‹ ê·œ ì¼€ì´ìŠ¤ ë”•ì…”ë„ˆë¦¬ {íŒë¡€: [...], ê²°ì •ë¡€: [...]}
    """
    logger.info("="*60)
    logger.info("ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„")
    logger.info("="*60)

    # ê¸°ì¡´ ì¼€ì´ìŠ¤ ID ë¡œë“œ
    existing_ids = load_existing_case_ids(unified_file)
    logger.info(f"ê¸°ì¡´ ì¼€ì´ìŠ¤: {len(existing_ids):,}ê±´")

    # í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ
    with open(crawled_file, 'r', encoding='utf-8') as f:
        crawled_data = json.load(f)

    new_cases = {
        'íŒë¡€': [],
        'ê²°ì •ë¡€': [],
        'í•´ì„ë¡€': [],
        'ë²•ë ¹': []
    }

    total_crawled = 0
    total_new = 0

    for case_type in ['íŒë¡€', 'ê²°ì •ë¡€', 'í•´ì„ë¡€', 'ë²•ë ¹']:
        cases = crawled_data.get(case_type, [])
        total_crawled += len(cases)

        for case in cases:
            case_id = str(case.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸', ''))

            # ì‹ ê·œ ì¼€ì´ìŠ¤ë§Œ ì¶”ê°€
            if case_id and case_id not in existing_ids:
                new_cases[case_type].append(case)
                total_new += 1

        if new_cases[case_type]:
            logger.info(f"{case_type}: {len(new_cases[case_type])}ê±´ ì‹ ê·œ")

    logger.info(f"\nì´ í¬ë¡¤ë§: {total_crawled}ê±´")
    logger.info(f"ì‹ ê·œ ì¼€ì´ìŠ¤: {total_new}ê±´")
    logger.info(f"ì¤‘ë³µ ì œì™¸: {total_crawled - total_new}ê±´")

    return new_cases


def main():
    import sys
    from datetime import datetime

    if len(sys.argv) < 3:
        print("ì‚¬ìš©ë²•: python identify_new_cases.py <crawled_file> <unified_file>")
        sys.exit(1)

    crawled_file = sys.argv[1]
    unified_file = sys.argv[2]

    # ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„
    new_cases = identify_new_cases(crawled_file, unified_file)

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"data/new_cases_{timestamp}.json"

    output_data = {
        "ìˆ˜ì§‘ì •ë³´": {
            "ì‹ë³„ì‹œê°": datetime.now().isoformat(),
            "ì›ë³¸íŒŒì¼": crawled_file,
            "ê¸°ì¤€íŒŒì¼": unified_file,
            "ì‹ ê·œê±´ìˆ˜": sum(len(cases) for cases in new_cases.values())
        },
        **new_cases
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    logger.info(f"\nì‹ ê·œ ì¼€ì´ìŠ¤ ì €ì¥: {output_file}")


if __name__ == "__main__":
    main()
```

---

### 2. ë°ì´í„° ë³‘í•©

```python
# scripts/merge_new_data.py

import json
import shutil
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def backup_unified_file(unified_file: str) -> str:
    """
    í†µí•© íŒŒì¼ ë°±ì—…

    Returns:
        ë°±ì—… íŒŒì¼ ê²½ë¡œ
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = unified_file.replace('.json', f'_backup_{timestamp}.json')

    shutil.copy2(unified_file, backup_file)
    logger.info(f"ë°±ì—… ìƒì„±: {backup_file}")

    return backup_file


def merge_new_data(
    new_cases_file: str,
    unified_file: str,
    output_file: str = None
):
    """
    ì‹ ê·œ ë°ì´í„°ë¥¼ í†µí•© íŒŒì¼ì— ë³‘í•©

    Args:
        new_cases_file: ì‹ ê·œ ì¼€ì´ìŠ¤ íŒŒì¼
        unified_file: ê¸°ì¡´ í†µí•© íŒŒì¼
        output_file: ì¶œë ¥ íŒŒì¼ (Noneì´ë©´ unified_file ë®ì–´ì“°ê¸°)
    """
    logger.info("="*60)
    logger.info("ë°ì´í„° ë³‘í•©")
    logger.info("="*60)

    # ë°±ì—… ìƒì„±
    backup_file = backup_unified_file(unified_file)

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    with open(unified_file, 'r', encoding='utf-8') as f:
        unified_data = json.load(f)

    # ì‹ ê·œ ë°ì´í„° ë¡œë“œ
    with open(new_cases_file, 'r', encoding='utf-8') as f:
        new_data = json.load(f)

    # ë³‘í•©
    stats = {}
    for case_type in ['íŒë¡€', 'ê²°ì •ë¡€', 'í•´ì„ë¡€', 'ë²•ë ¹']:
        before = len(unified_data.get(case_type, []))
        new_cases = new_data.get(case_type, [])

        if case_type not in unified_data:
            unified_data[case_type] = []

        unified_data[case_type].extend(new_cases)
        after = len(unified_data[case_type])

        stats[case_type] = {
            'ì´ì „': before,
            'ì¶”ê°€': len(new_cases),
            'ì´í›„': after
        }

        logger.info(f"{case_type}: {before} â†’ {after} (+{len(new_cases)})")

    # ìˆ˜ì§‘ì •ë³´ ì—…ë°ì´íŠ¸
    unified_data['ìˆ˜ì§‘ì •ë³´']['ìµœì¢…ê°±ì‹ '] = datetime.now().isoformat()
    unified_data['ìˆ˜ì§‘ì •ë³´']['íŒë¡€ìˆ˜'] = len(unified_data.get('íŒë¡€', []))
    unified_data['ìˆ˜ì§‘ì •ë³´']['ê²°ì •ë¡€ìˆ˜'] = len(unified_data.get('ê²°ì •ë¡€', []))
    unified_data['ìˆ˜ì§‘ì •ë³´']['í•´ì„ë¡€ìˆ˜'] = len(unified_data.get('í•´ì„ë¡€', []))
    unified_data['ìˆ˜ì§‘ì •ë³´']['ë²•ë ¹ìˆ˜'] = len(unified_data.get('ë²•ë ¹', []))
    unified_data['ìˆ˜ì§‘ì •ë³´']['ì´ê±´ìˆ˜'] = sum([
        unified_data['ìˆ˜ì§‘ì •ë³´']['íŒë¡€ìˆ˜'],
        unified_data['ìˆ˜ì§‘ì •ë³´']['ê²°ì •ë¡€ìˆ˜'],
        unified_data['ìˆ˜ì§‘ì •ë³´']['í•´ì„ë¡€ìˆ˜'],
        unified_data['ìˆ˜ì§‘ì •ë³´']['ë²•ë ¹ìˆ˜']
    ])

    # ì €ì¥
    if output_file is None:
        output_file = unified_file

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unified_data, f, ensure_ascii=False, indent=2)

    logger.info(f"\në³‘í•© ì™„ë£Œ: {output_file}")
    logger.info(f"ì´ ê±´ìˆ˜: {unified_data['ìˆ˜ì§‘ì •ë³´']['ì´ê±´ìˆ˜']:,}ê±´")

    return output_file, stats


def main():
    import sys

    if len(sys.argv) < 3:
        print("ì‚¬ìš©ë²•: python merge_new_data.py <new_cases_file> <unified_file>")
        sys.exit(1)

    new_cases_file = sys.argv[1]
    unified_file = sys.argv[2]

    merge_new_data(new_cases_file, unified_file)


if __name__ == "__main__":
    main()
```

---

### 3. ì¦ë¶„ ì„ë² ë”© ìƒì„±

```python
# scripts/create_embeddings_incremental.py

import json
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
import logging
from typing import List, Dict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IncrementalEmbeddingService:
    def __init__(self):
        self.openai_client = OpenAI()
        self.qdrant_client = QdrantClient(host="localhost", port=6333)

    def get_existing_ids(self, collection_name: str) -> set:
        """Vector DBì—ì„œ ê¸°ì¡´ ID ì¡°íšŒ"""
        # Qdrantì—ì„œ ëª¨ë“  í¬ì¸íŠ¸ ID ê°€ì ¸ì˜¤ê¸°
        result = self.qdrant_client.scroll(
            collection_name=collection_name,
            limit=100000,
            with_payload=False,
            with_vectors=False
        )

        existing_ids = {str(point.id) for point in result[0]}
        return existing_ids

    def create_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def process_new_cases(
        self,
        new_cases_file: str,
        collection_name: str = "precedents"
    ):
        """
        ì‹ ê·œ ì¼€ì´ìŠ¤ë§Œ ì„ë² ë”© ìƒì„± ë° ì €ì¥

        Args:
            new_cases_file: ì‹ ê·œ ì¼€ì´ìŠ¤ JSON íŒŒì¼
            collection_name: Vector DB ì»¬ë ‰ì…˜ ì´ë¦„
        """
        logger.info("="*60)
        logger.info("ì¦ë¶„ ì„ë² ë”© ìƒì„±")
        logger.info("="*60)

        # ì‹ ê·œ ì¼€ì´ìŠ¤ ë¡œë“œ
        with open(new_cases_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ê¸°ì¡´ ID ì¡°íšŒ
        existing_ids = self.get_existing_ids(collection_name)
        logger.info(f"ê¸°ì¡´ ë²¡í„°: {len(existing_ids):,}ê°œ")

        # ìµœëŒ€ ID ì°¾ê¸° (ì‹ ê·œ ID ìƒì„±ìš©)
        max_id = max(map(int, existing_ids)) if existing_ids else 0

        # ì‹ ê·œ ì¼€ì´ìŠ¤ ì²˜ë¦¬
        points = []
        new_count = 0

        for case_type in ['íŒë¡€', 'ê²°ì •ë¡€', 'í•´ì„ë¡€', 'ë²•ë ¹']:
            cases = data.get(case_type, [])

            for case in cases:
                case_id = str(case.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸', ''))

                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
                if case_id in existing_ids:
                    continue

                # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ êµ¬ì„±
                text_to_embed = f"""
                ì‚¬ê±´ë²ˆí˜¸: {case['ì‚¬ê±´ë²ˆí˜¸']}
                ë²•ì›: {case['ë²•ì›ëª…']}
                íŒê²°ìš”ì§€: {case['ìƒì„¸ì •ë³´'].get('íŒê²°ìš”ì§€', '')}
                ì „ë¬¸: {case['ìƒì„¸ì •ë³´'].get('ì „ë¬¸', '')[:2000]}
                """

                # ì„ë² ë”© ìƒì„±
                embedding = self.create_embedding(text_to_embed)

                # Point ìƒì„±
                max_id += 1
                point = PointStruct(
                    id=max_id,
                    vector=embedding,
                    payload={
                        "íŒë¡€ì¼ë ¨ë²ˆí˜¸": case_id,
                        "ì‚¬ê±´ë²ˆí˜¸": case['ì‚¬ê±´ë²ˆí˜¸'],
                        "ë²•ì›ëª…": case['ë²•ì›ëª…'],
                        "ì„ ê³ ì¼ì": case['ì„ ê³ ì¼ì'],
                        "ê²€ìƒ‰ì–´": case['ê²€ìƒ‰ì–´'],
                        "íŒê²°ìš”ì§€": case['ìƒì„¸ì •ë³´'].get('íŒê²°ìš”ì§€', ''),
                        "ì „ë¬¸": case['ìƒì„¸ì •ë³´'].get('ì „ë¬¸', ''),
                        "ë°ì´í„°íƒ€ì…": case['ë°ì´í„°íƒ€ì…']
                    }
                )
                points.append(point)
                new_count += 1

                # ë°°ì¹˜ ì €ì¥ (100ê°œì”©)
                if len(points) >= 100:
                    self.qdrant_client.upsert(
                        collection_name=collection_name,
                        points=points
                    )
                    logger.info(f"ì§„í–‰: {new_count}ê°œ ì¶”ê°€")
                    points = []

        # ë‚¨ì€ í¬ì¸íŠ¸ ì €ì¥
        if points:
            self.qdrant_client.upsert(
                collection_name=collection_name,
                points=points
            )

        logger.info(f"\nì´ {new_count}ê°œ ë²¡í„° ì¶”ê°€ ì™„ë£Œ")
        logger.info(f"ì „ì²´ ë²¡í„°: {len(existing_ids) + new_count:,}ê°œ")


def main():
    import sys

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python create_embeddings_incremental.py <new_cases_file>")
        sys.exit(1)

    new_cases_file = sys.argv[1]

    service = IncrementalEmbeddingService()
    service.process_new_cases(new_cases_file)


if __name__ == "__main__":
    main()
```

---

### 4. ìŠ¤ì¼€ì¤„ëŸ¬ (ìë™í™”)

```python
# scripts/schedule_daily_update.py

from datetime import datetime
import subprocess
import logging
import json
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/update_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DailyUpdatePipeline:
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.base_dir = Path(__file__).parent.parent

    def run_command(self, command: list, description: str):
        """ëª…ë ¹ì–´ ì‹¤í–‰"""
        logger.info(f"[{description}] ì‹œì‘")

        try:
            result = subprocess.run(
                command,
                cwd=self.base_dir,
                capture_output=True,
                text=True,
                check=True
            )

            logger.info(f"[{description}] ì™„ë£Œ")
            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"[{description}] ì‹¤íŒ¨: {e.stderr}")
            return False

    def execute_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("="*60)
        logger.info("ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info(f"ì‹œê°: {datetime.now().isoformat()}")
        logger.info("="*60)

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        crawled_file = f"data/crawled_raw/traffic_legal_data_{self.timestamp}.json"
        unified_file = "data/unified_traffic_data_latest.json"
        new_cases_file = f"data/new_cases_{self.timestamp}.json"

        # 1. í¬ë¡¤ë§
        if not self.run_command(
            ["python3", "scripts/crawl_legal_cases.py"],
            "í¬ë¡¤ë§"
        ):
            logger.error("í¬ë¡¤ë§ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
            return False

        # í¬ë¡¤ë§ íŒŒì¼ ì´ë™
        subprocess.run([
            "mv",
            "traffic_legal_data_*.json",
            crawled_file
        ], check=False)

        # 2. ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„
        if not self.run_command(
            ["python3", "scripts/identify_new_cases.py", crawled_file, unified_file],
            "ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„"
        ):
            logger.error("ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
            return False

        # ì‹ ê·œ ì¼€ì´ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        with open(new_cases_file, 'r', encoding='utf-8') as f:
            new_data = json.load(f)

        new_count = new_data['ìˆ˜ì§‘ì •ë³´']['ì‹ ê·œê±´ìˆ˜']

        if new_count == 0:
            logger.info("ì‹ ê·œ ì¼€ì´ìŠ¤ ì—†ìŒ. ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”")
            return True

        logger.info(f"ì‹ ê·œ ì¼€ì´ìŠ¤: {new_count}ê±´")

        # 3. ë°ì´í„° ë³‘í•©
        if not self.run_command(
            ["python3", "scripts/merge_new_data.py", new_cases_file, unified_file],
            "ë°ì´í„° ë³‘í•©"
        ):
            logger.error("ë°ì´í„° ë³‘í•© ì‹¤íŒ¨")
            return False

        # 4. ì„ë² ë”© ìƒì„± (ì‹ ê·œë§Œ)
        if not self.run_command(
            ["python3", "scripts/create_embeddings_incremental.py", new_cases_file],
            "ì„ë² ë”© ìƒì„±"
        ):
            logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False

        # 5. PostgreSQL ë™ê¸°í™” (TODO: êµ¬í˜„ í•„ìš”)
        logger.info("[PostgreSQL ë™ê¸°í™”] TODO")

        logger.info("="*60)
        logger.info("ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        logger.info(f"ì‹ ê·œ ì¶”ê°€: {new_count}ê±´")
        logger.info("="*60)

        return True


def main():
    pipeline = DailyUpdatePipeline()
    success = pipeline.execute_pipeline()

    if success:
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ")
    else:
        logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
```

---

### 5. Crontab ì„¤ì •

```bash
# crontab -e

# ë§¤ì¼ ì˜¤ì „ 2ì‹œì— ì‹¤í–‰
0 2 * * * cd /path/to/project && python3 scripts/schedule_daily_update.py

# ë˜ëŠ” Celery Beat ì‚¬ìš©
# celery -A app.celery_app beat --loglevel=info
```

---

## ğŸ“Š ë°ì´í„° íë¦„ ì˜ˆì‹œ

### Day 1 (ì´ˆê¸°)
```
í†µí•© DB: 11,769ê±´
Vector DB: 11,769ê°œ ë²¡í„°
```

### Day 2 (í¬ë¡¤ë§ í›„)
```
í¬ë¡¤ë§: 50ê±´ (íŒë¡€ 45 + í•´ì„ë¡€ 5)
ì‹ ê·œ ì‹ë³„: 12ê±´ (38ê±´ ì¤‘ë³µ ì œì™¸)
    â†“
í†µí•© DB: 11,781ê±´ (+12)
Vector DB: 11,781ê°œ ë²¡í„° (+12)
```

### Week 1 (7ì¼ í›„)
```
ëˆ„ì  ì‹ ê·œ: 84ê±´
í†µí•© DB: 11,853ê±´
Vector DB: 11,853ê°œ ë²¡í„°
```

---

## ğŸ” ì¤‘ë³µ ì œê±° ë¡œì§

```python
# íŒë¡€ì¼ë ¨ë²ˆí˜¸ë¡œ ì¤‘ë³µ ì²´í¬
existing_ids = {
    "78434",
    "79038",
    "173284",
    ...
}

# í¬ë¡¤ë§ ë°ì´í„°
crawled_case = {
    "íŒë¡€ì¼ë ¨ë²ˆí˜¸": "79038"  # ì´ë¯¸ ì¡´ì¬
}

if crawled_case['íŒë¡€ì¼ë ¨ë²ˆí˜¸'] in existing_ids:
    # ìŠ¤í‚µ
    pass
else:
    # ì‹ ê·œë¡œ ì¶”ê°€
    new_cases.append(crawled_case)
```

---

## ğŸ“‹ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ë¡œê·¸ ì˜ˆì‹œ

```
logs/update_20251103.log:

2025-11-03 02:00:00 - INFO - ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘
2025-11-03 02:00:01 - INFO - [í¬ë¡¤ë§] ì‹œì‘
2025-11-03 02:05:30 - INFO - [í¬ë¡¤ë§] ì™„ë£Œ
2025-11-03 02:05:31 - INFO - [ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„] ì‹œì‘
2025-11-03 02:05:35 - INFO - ê¸°ì¡´ ì¼€ì´ìŠ¤: 11,769ê±´
2025-11-03 02:05:35 - INFO - ì‹ ê·œ ì¼€ì´ìŠ¤: 12ê±´
2025-11-03 02:05:35 - INFO - [ì‹ ê·œ ì¼€ì´ìŠ¤ ì‹ë³„] ì™„ë£Œ
2025-11-03 02:05:36 - INFO - [ë°ì´í„° ë³‘í•©] ì‹œì‘
2025-11-03 02:05:40 - INFO - íŒë¡€: 9198 â†’ 9210 (+12)
2025-11-03 02:05:40 - INFO - [ë°ì´í„° ë³‘í•©] ì™„ë£Œ
2025-11-03 02:05:41 - INFO - [ì„ë² ë”© ìƒì„±] ì‹œì‘
2025-11-03 02:06:10 - INFO - ì´ 12ê°œ ë²¡í„° ì¶”ê°€ ì™„ë£Œ
2025-11-03 02:06:10 - INFO - [ì„ë² ë”© ìƒì„±] ì™„ë£Œ
2025-11-03 02:06:10 - INFO - âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ
```

### Slack ì•Œë¦¼ (ì„ íƒ)

```python
# scripts/schedule_daily_update.pyì— ì¶”ê°€

import requests

def send_slack_notification(message: str):
    webhook_url = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

    payload = {
        "text": message,
        "username": "íŒë¡€ ì—…ë°ì´íŠ¸ ë´‡",
        "icon_emoji": ":robot_face:"
    }

    requests.post(webhook_url, json=payload)

# íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„
send_slack_notification(f"""
âœ… íŒë¡€ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ

â€¢ ì‹ ê·œ ì¶”ê°€: {new_count}ê±´
â€¢ ì „ì²´ ê±´ìˆ˜: {total_count:,}ê±´
â€¢ ì—…ë°ì´íŠ¸ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
""")
```

---

## ğŸ¯ ìš”ì•½

### ìë™í™” ì›Œí¬í”Œë¡œìš°

```
1. Cron (ë§¤ì¼ ì˜¤ì „ 2ì‹œ)
   â†“
2. í¬ë¡¤ë§ (5ë¶„)
   â†“
3. ì‹ ê·œ ì‹ë³„ (10ì´ˆ)
   â†“
4. ë³‘í•© (10ì´ˆ)
   â†“
5. ì„ë² ë”© (ì‹ ê·œ Ã— 2ì´ˆ)
   â†“
6. Vector DB ì—…ë°ì´íŠ¸ (ì¦‰ì‹œ)
   â†“
7. ì•Œë¦¼ ë°œì†¡
```

**ì´ ì†Œìš” ì‹œê°„:** ì•½ 5~10ë¶„ (ì‹ ê·œ ë°ì´í„° ì–‘ì— ë”°ë¼)

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
  - [ ] `identify_new_cases.py`
  - [ ] `merge_new_data.py`
  - [ ] `create_embeddings_incremental.py`
  - [ ] `schedule_daily_update.py`

- [ ] ì¸í”„ë¼ ì„¤ì •
  - [ ] Qdrant ì„¤ì¹˜ ë° ì‹¤í–‰
  - [ ] Cron ì„¤ì •

- [ ] í…ŒìŠ¤íŠ¸
  - [ ] ìˆ˜ë™ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
  - [ ] ì¤‘ë³µ ì œê±° ê²€ì¦
  - [ ] ë¡¤ë°± í…ŒìŠ¤íŠ¸

- [ ] ëª¨ë‹ˆí„°ë§
  - [ ] ë¡œê·¸ í™•ì¸
  - [ ] ì•Œë¦¼ ì„¤ì •
  - [ ] ëŒ€ì‹œë³´ë“œ êµ¬ì„±

---

**ì‘ì„±ì¼:** 2025-11-03
**ì‹œìŠ¤í…œ:** ì£¼ê¸°ì  í¬ë¡¤ë§ â†’ ì¦ë¶„ ì—…ë°ì´íŠ¸ ìë™í™”
