# 형사법 RAG 챗봇 - 기술 선택 및 설계 결정 (V2)

> 7주 프로젝트를 위한 완전한 기술 스택 및 아키텍처 설계 문서

## 목차

### 핵심 아키텍처 (1-8)
1. [RAG 아키텍처 선택](#1-rag-아키텍처-선택)
2. [임베딩 모델 선택](#2-임베딩-모델-선택)
3. [청킹 전략](#3-청킹-전략)
4. [벡터 데이터베이스 선택](#4-벡터-데이터베이스-선택)
5. [검색 전략](#5-검색-전략)
6. [Few-Shot Learning](#6-few-shot-learning)
7. [Constitutional AI 프롬프트](#7-constitutional-ai-프롬프트)
8. [LLM 선택](#8-llm-선택)

### 프로젝트 필수 기능 (9-15)
9. [멀티 LLM 비교 프레임워크](#9-멀티-llm-비교-프레임워크)
10. [OCR 및 PDF 처리 전략](#10-ocr-및-pdf-처리-전략)
11. [크롤링 전략 및 법적 고려사항](#11-크롤링-전략-및-법적-고려사항)
12. [핵심 조항 추출 전략](#12-핵심-조항-추출-전략)
13. [리스크 분석 및 시각화](#13-리스크-분석-및-시각화)
14. [UI/UX 프레임워크 선택](#14-uiux-프레임워크-선택)
15. [배포 및 인프라 전략](#15-배포-및-인프라-전략)

### 시스템 설계 및 최적화 (16-21)
16. [데이터베이스 아키텍처](#16-데이터베이스-아키텍처)
17. [비용 최적화 전략](#17-비용-최적화-전략)
18. [프롬프트 엔지니어링 라이브러리](#18-프롬프트-엔지니어링-라이브러리)
19. [법률 도메인 특화 전략](#19-법률-도메인-특화-전략)
20. [보안 및 컴플라이언스](#20-보안-및-컴플라이언스)
21. [모니터링 및 성능 추적](#21-모니터링-및-성능-추적)

---

## 1. RAG 아키텍처 선택

### 문제 정의
- 형사법 데이터는 방대하고 지속적으로 업데이트됨
- LLM만으로는 최신 판례나 법령을 학습할 수 없음
- Fine-tuning은 비용이 많이 들고, 데이터 업데이트 시마다 재학습 필요
- Hallucination 문제: LLM이 없는 내용을 지어내는 경우

### 선택: Retrieval-Augmented Generation (RAG)

### 선택 이유

1. **외부 지식 활용**: 실제 판례/법령 데이터베이스에서 정확한 정보 검색
2. **실시간 업데이트**: 새로운 판례 추가 시 벡터 DB만 업데이트 (재학습 불필요)
3. **비용 효율성**: Fine-tuning 대비 90% 비용 절감 (API 호출만 필요)
4. **출처 제공**: 답변의 근거가 되는 판례를 명시적으로 제시 가능
5. **Hallucination 감소**: 검색된 문서 기반으로만 답변 생성

### 대안 비교

| 접근 방법 | 비용 | 업데이트 | 정확도 | 출처 제공 | 선택 |
|----------|------|---------|--------|----------|------|
| **RAG** | $ | 실시간 | ⭐⭐⭐⭐ | ✅ | ✅ |
| Fine-tuning | $$$$ | 재학습 필요 | ⭐⭐⭐⭐⭐ | ❌ | ❌ |
| Prompt Only | $ | - | ⭐⭐ | ❌ | ❌ |
| Hybrid (RAG+FT) | $$$$$ | 복잡 | ⭐⭐⭐⭐⭐ | ✅ | △ (고급) |

### 프로젝트 연계
- **Week 4**: RAG 파이프라인 구축
- **산출물**: 검색+추천 모듈 코드

---

## 2. 임베딩 모델 선택

### 문제 정의
- 영어 임베딩 모델은 한국어 성능이 떨어짐
- 법률 용어는 특수하고 정확한 의미 파악이 중요
- 무료 vs 유료, 성능 vs 비용 trade-off 필요

### 선택: jhgan/ko-sroberta-multitask (기본) + OpenAI text-embedding-3-small (대안)

### 선택 이유

**ko-sroberta-multitask**:
1. **한국어 최적화**: KorNLI, KorSTS 데이터셋으로 학습
2. **다중 태스크 학습**: 유사도, 분류, QA 등 범용성 높음
3. **무료 오픈소스**: 상업적 이용 가능, API 비용 없음
4. **적절한 성능**: 768차원, KorSTS 벤치마크 상위권

### 대안 비교 (2025년 기준)

| 모델 | 한국어 | 차원 | 비용 | 성능 | 선택 |
|------|--------|------|------|------|------|
| **ko-sroberta-multitask** | ⭐⭐⭐⭐⭐ | 768 | 무료 | ⭐⭐⭐⭐ | ✅ |
| OpenAI text-embedding-3-small | ⭐⭐⭐⭐ | 1536 | $0.02/1M | ⭐⭐⭐⭐⭐ | △ |
| OpenAI text-embedding-3-large | ⭐⭐⭐⭐ | 3072 | $0.13/1M | ⭐⭐⭐⭐⭐ | ❌ |
| BM-K/KoSimCSE-roberta | ⭐⭐⭐⭐⭐ | 768 | 무료 | ⭐⭐⭐⭐ | △ |
| multilingual-e5-large | ⭐⭐⭐ | 1024 | 무료 | ⭐⭐⭐⭐ | ❌ |

### 결론
- **학습용**: ko-sroberta (무료, 충분한 성능)
- **상용화 시**: OpenAI text-embedding-3-small (성능↑, 유료)

### 프로젝트 연계
- **Week 4**: 임베딩 모델 적용
- **Week 6**: 모델 비교 실험

---

## 3. 청킹 전략

### 문제 정의
- 판결문/법령은 매우 길 수 있음 (수천~수만 자)
- 임베딩 모델은 입력 길이 제한 있음 (512 토큰)
- 너무 긴 텍스트는 중요 정보가 희석됨

### 선택: RecursiveCharacterTextSplitter
- **Chunk Size**: 500자
- **Overlap**: 50자 (10%)

### 선택 이유

1. **500자 선택 이유**:
   - 너무 작으면 (100-200자): 문맥 손실
   - 너무 크면 (1000-2000자): 노이즈 증가
   - 500자: 법률 조문 1-2개, 판결문 핵심 부분 포함

2. **50자 Overlap**:
   - 문맥 연속성 유지
   - 청크 경계에서 정보 손실 방지
   - 10% 규칙 (일반적으로 10-20% 권장)

3. **Separator 계층**:
   - `\n\n` → `\n` → `.!?` → `,` → ` ` 순서
   - 의미 단위 우선 분리 (단락 > 문장 > 절 > 단어)

### 대안 비교

| Chunk Size | 문맥 보존 | 검색 정확도 | 노이즈 | 선택 |
|------------|----------|-------------|--------|------|
| 300자 | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ |
| **500자** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ✅ |
| 800자 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ | △ |

### 프로젝트 연계
- **Week 2**: 데이터 전처리 시 청킹 적용
- **Week 6**: A/B 테스트 (다양한 크기 비교)

---

## 4. 벡터 데이터베이스 선택

### 문제 정의
- 수천~수만 개의 문서 임베딩을 효율적으로 저장/검색 필요
- 메타데이터 필터링 (출처별, 날짜별) 지원 필요
- 학습용이므로 설치/관리가 간편해야 함

### 선택: ChromaDB (기본) + FAISS (대안)

### 선택 이유

**ChromaDB**:
1. **설치 간편**: pip install 한 줄로 완료
2. **영구 저장**: 디스크 자동 저장, 재시작 후에도 유지
3. **메타데이터 필터링**: 출처별, 날짜별 필터링 내장
4. **학습 친화적**: API 직관적, 문서화 우수

### 대안 비교

| DB | 학습 용이성 | 속도 (소규모) | 메타데이터 | 설치/관리 | 선택 |
|----|------------|--------------|-----------|----------|------|
| **ChromaDB** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ |
| FAISS | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | △ |
| Pinecone | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ (유료) |
| Qdrant | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | △ |
| Weaviate | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | △ |

### 확장 전략
- **소규모 (< 10,000 문서)**: ChromaDB
- **대규모 (100,000+ 문서)**: FAISS 또는 Pinecone
- **하이브리드 검색 필요 시**: Weaviate

### 프로젝트 연계
- **Week 4**: ChromaDB 설치 및 인덱싱
- **Week 6**: (선택) FAISS와 성능 비교

---

## 5. 검색 전략

### 문제 정의
- 단순 키워드 매칭의 한계 (동의어, 유사 표현 미처리)
- 검색 결과의 다양성 부족 (유사 문서만 반복)
- 정확한 조문 번호, 사건 번호 검색 필요

### 선택: Semantic Search (기본) + Hybrid Search (고급)

### 선택 이유

**Semantic Search**:
1. **의미 기반 검색**: "돈을 훔쳤어요" → "절도죄" 자동 연결
2. **동의어 처리**: "절취", "영득", "훔치다" 모두 처리
3. **Cosine Similarity**: 벡터 방향 비교, 해석 쉬움

**Hybrid Search (선택적)**:
- BM25 (키워드) 0.3 + Semantic (의미) 0.7
- 정확한 조문 번호 검색 시 유용 (예: "형법 제329조")

### 대안 비교

| 검색 방법 | 정확도 | 유연성 | 속도 | 구현 난이도 | 선택 |
|----------|--------|--------|------|------------|------|
| **Semantic Only** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ |
| Keyword Only (BM25) | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ |
| Hybrid | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | △ |

### 재순위화 (Re-ranking)
- **MMR (Maximal Marginal Relevance)**: 다양성 확보
- **목표**: 유사한 문서 제거, 다양한 관점 제공

### 프로젝트 연계
- **Week 4**: Semantic Search 구현
- **Week 6**: (선택) Hybrid Search 추가

---

## 6. Few-Shot Learning

### 문제 정의
- Zero-Shot은 답변이 너무 일반적이고 법적 정확성 부족
- LLM이 법률 답변 형식을 모름
- 구조화된 답변 포맷 필요

### 선택: 3-Shot Learning

### 선택 이유

1. **패턴 인식**: 2-3개 예시면 LLM이 패턴 파악
2. **토큰 효율**: 예시당 ~200 토큰 × 3 = 600 토큰 (적절)
3. **다양성**: 서로 다른 유형의 질문 3개로 커버 (정의, 비교, 실무)

### 대안 비교

| Shot 수 | 품질 | 토큰 사용 | 비용 | 선택 |
|---------|------|----------|------|------|
| 0-Shot | ⭐⭐ | 최소 | $ | ❌ |
| 1-Shot | ⭐⭐⭐ | 적음 | $ | ❌ |
| **3-Shot** | ⭐⭐⭐⭐ | 적절 | $$ | ✅ |
| 5-Shot | ⭐⭐⭐⭐⭐ | 많음 | $$$ | ❌ |

### 예시 구성
1. **Example 1**: 구성요건 질문 (사기죄의 구성요건은?)
2. **Example 2**: 판례 비교 질문 (절도죄와 강도죄의 차이는?)
3. **Example 3**: 실무 적용 질문 (정당방위 인정 요건은?)

### 프로젝트 연계
- **Week 3**: Few-Shot 프롬프트 작성
- **Week 6**: Shot 수 비교 실험 (0 vs 3 vs 5)

---

## 7. Constitutional AI 프롬프트

### 문제 정의
- LLM이 검색 결과 없이 추측할 위험
- 출처 명시 없이 답변할 위험
- 법률 자문처럼 보이는 표현 사용 위험

### 선택: Constitutional AI (Anthropic 방법론)

### 핵심 원칙 (Constitution)

1. **정확성**: 제공된 판례와 법령만 기반
2. **출처 명시**: 반드시 판례 번호, 조문 명시
3. **환각 방지**: 모르면 "모른다" 명시
4. **전문적 어조**: 객관적이고 정확한 표현
5. **면책 조항**: "법률 정보 제공이며, 법률 자문 아님" 명시

### 선택 이유

1. **자기 검증**: LLM이 자신의 답변을 원칙에 따라 검토
2. **환각 감소**: 검색 문서 기반 답변만 허용
3. **신뢰성 향상**: 출처와 근거 명확히 제시

### 구현 방법

**단일 프롬프트 방식** (권장 - 비용 효율):
- 프롬프트에 모든 원칙 명시
- LLM이 한 번에 원칙 준수 답변 생성

**2단계 방식** (고급 - 높은 품질):
- 1단계: 초기 답변 생성
- 2단계: 자기 비판 + 수정

### 프로젝트 연계
- **Week 3**: Constitutional Prompt 설계
- **산출물**: 프롬프트 설계 문서

---

## 8. LLM 선택

### 문제 정의
- 한국어 법률 문서 처리 가능한 모델 필요
- 긴 컨텍스트 지원 (판례 여러 개 포함)
- 비용 vs 성능 균형 필요

### 선택: GPT-4o (기본) + GPT-4o-mini (요약용) + Claude 3.5 Sonnet (대안)

### 선택 이유

**GPT-4o**:
1. **한국어 성능**: GPT 시리즈 중 최고 수준
2. **긴 컨텍스트**: 128K 토큰 → 많은 판례 포함 가능
3. **멀티모달**: 계약서 이미지 직접 분석 가능
4. **구조화 출력**: JSON mode 지원

**GPT-4o-mini**:
- 간단한 요약에 적합 (빠르고 저렴)

**Claude 3.5 Sonnet**:
- Constitutional AI 원조 (안전성 최고)

### 대안 비교 (2025년 1월 기준)

| 모델 | 한국어 | 컨텍스트 | 입력 비용 | 출력 비용 | 선택 |
|------|--------|---------|----------|----------|------|
| **GPT-4o** | ⭐⭐⭐⭐⭐ | 128K | $2.50/1M | $10/1M | ✅ |
| **GPT-4o-mini** | ⭐⭐⭐⭐ | 128K | $0.15/1M | $0.60/1M | ✅ |
| **Claude 3.5 Sonnet** | ⭐⭐⭐⭐⭐ | 200K | $3/1M | $15/1M | ✅ |
| Llama 3.1 70B | ⭐⭐⭐ | 128K | 무료 | 무료 | △ |
| GPT-3.5 Turbo | ⭐⭐⭐ | 16K | $0.50/1M | $1.50/1M | ❌ |

### 사용 전략 (모델 계층화)

- **간단한 요약**: GPT-4o-mini
- **복잡한 Q&A**: GPT-4o
- **리스크 분석**: Claude 3.5 Sonnet (안전성 중요)
- **조항 추출**: GPT-4o-mini (구조화 출력)

### 프로젝트 연계
- **Week 3**: GPT-4o API 연동
- **Week 6**: 멀티 LLM 비교 (변호사시험 기반)

---

## 9. 멀티 LLM 비교 프레임워크

### 문제 정의
- 어떤 LLM이 법률 업무에 적합한지 객관적으로 평가 필요
- 주관적 평가는 신뢰성 부족
- 비용 vs 성능 trade-off 판단 기준 필요

### 선택: 변호사시험 선택형 기반 평가

### 선택 이유

1. **객관적 평가**: 정답이 명확함 (주관적 평가 불필요)
2. **법률 전문성 직접 측정**: 실무 능력의 대리 지표
3. **표준화**: 모든 LLM에 동일한 기준 적용
4. **재현 가능**: 동일 문제로 반복 실험 가능
5. **신뢰성**: 법조계에서 인정하는 검증된 평가 도구

### 평가 메트릭

| 메트릭 | 측정 방법 | 목표값 |
|--------|----------|--------|
| **정확도** | 정답 문제 수 / 전체 | > 70% (합격선) |
| **응답 시간** | 문제당 평균 시간 | < 5초 |
| **비용** | 100문제당 API 비용 | < $5 |
| **신뢰도** | 정답 시 확신도 (1-10) | > 8 |
| **일관성** | 3회 반복 시 동일 답변 | > 90% |

### 실험 설계

**데이터 수집**:
- 법무부 공개 기출문제 (2018-2024)
- 형법 50문제 + 형사소송법 50문제 = 100문제
- 카테고리별 균형: 총론 30%, 각론 70%

**비교 대상**:
- GPT-4o
- GPT-4o-mini
- Claude 3.5 Sonnet
- Llama 3.1 70B

**프롬프트 전략 비교**:
- Zero-Shot vs 3-Shot
- Chain-of-Thought vs Standard
- RAG 활용 vs No RAG

### 종합 점수 계산

가중치:
- 정확도: 50%
- 속도: 20%
- 비용: 15%
- 신뢰도: 10%
- 일관성: 5%

### 결과 활용

**모델별 특화 분야 파악**:
- GPT-4o: 복잡한 법리 분석
- GPT-4o-mini: 단순 요약
- Claude 3.5: 안전성 중요 작업

### 프로젝트 연계
- **Week 6**: 변호사시험 데이터셋 구축 + LLM 비교 실험
- **산출물**: 멀티 LLM 비교 보고서

---

## 10. OCR 및 PDF 처리 전략

### 문제 정의
- 법률 문서는 대부분 PDF 형태
- 스캔본 PDF는 텍스트 추출 불가 → OCR 필요
- 법률 문서 특성: 한자, 특수 기호, 표, 각주 포함

### 선택: PaddleOCR (한글 OCR) + pdfplumber (PDF 파싱)

### 선택 이유

**PaddleOCR**:
1. **한글 정확도**: Tesseract 대비 15% 향상
2. **법률 특수 기호**: 인식률 높음
3. **무료 오픈소스**: 상업적 이용 가능
4. **GPU 가속**: 처리 속도 빠름

**pdfplumber**:
1. **표 추출**: 판례 데이터에 표 많음
2. **레이아웃 보존**: 구조 정보 유지
3. **정확도**: PyPDF2보다 우수

### 대안 비교

| 도구 | 정확도 | 속도 | 비용 | 한글 | 표 처리 | 선택 |
|------|--------|------|------|------|--------|------|
| **PaddleOCR** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 무료 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ |
| Tesseract | ⭐⭐⭐ | ⭐⭐⭐ | 무료 | ⭐⭐⭐ | ⭐⭐ | ❌ |
| Azure Form Recognizer | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | $1.5/1000p | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ (비쌈) |
| EasyOCR | ⭐⭐⭐⭐ | ⭐⭐⭐ | 무료 | ⭐⭐⭐⭐ | ⭐⭐⭐ | △ |
| **pdfplumber** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 무료 | - | ⭐⭐⭐⭐⭐ | ✅ |
| PyPDF2 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 무료 | - | ⭐⭐ | ❌ |

### 처리 파이프라인

1. **PDF 타입 판별**: 텍스트 PDF vs 스캔 PDF
2. **텍스트 PDF**: pdfplumber로 직접 추출
3. **스캔 PDF**: PaddleOCR 적용
4. **후처리**: 오탈자 보정, 줄바꿈 정규화, 특수문자 처리

### 평가 방법
- **메트릭**: CER (Character Error Rate) < 2%, WER (Word Error Rate) < 5%
- **테스트 데이터**: 판례 10개, 계약서 10개

### 프로젝트 연계
- **Week 2**: OCR 스크립트 작성 및 테스트
- **산출물**: 전처리된 텍스트 데이터셋

---

## 11. 크롤링 전략 및 법적 고려사항

### 문제 정의
- 판례, 법령 데이터를 어디서 수집할지 불명확
- 동적 웹페이지 (JavaScript 렌더링) 처리 필요
- 법적 문제 (저작권, 이용 약관) 검토 필요

### 선택: BeautifulSoup (정적 페이지) + Selenium (동적 페이지)

### 선택 이유

**BeautifulSoup**:
1. **간단함**: HTML 파싱에 최적
2. **빠름**: 정적 페이지는 빠르게 처리
3. **법제처 적합**: 법제처는 정적 HTML 제공

**Selenium**:
1. **동적 페이지**: JavaScript 렌더링 필요 (대법원 사이트)
2. **브라우저 자동화**: 로그인, 클릭 등 상호작용 가능

### 대안 비교

| 도구 | 정적 페이지 | 동적 페이지 | 속도 | 학습 난이도 | 선택 |
|------|-----------|-----------|------|-----------|------|
| **BeautifulSoup** | ⭐⭐⭐⭐⭐ | ❌ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ |
| **Selenium** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ✅ |
| Scrapy | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ❌ (복잡) |
| Playwright | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | △ |

### 데이터 소스

| 소스 | 내용 | 접근성 | 도구 |
|------|------|--------|------|
| 법제처 | 법령 | 공개 API | BeautifulSoup |
| 대법원 | 판례 | 웹사이트 | Selenium |
| 국회 | 법안 | 공개 데이터 | BeautifulSoup |
| 공공데이터포털 | 다양한 법률 문서 | API | requests |

### 법적 고려사항

1. **robots.txt 확인**: 크롤링 허용 여부 확인
2. **요청 간격**: 1초 이상 (서버 부하 방지)
3. **저작권**: 공공 데이터는 일반적으로 이용 가능
4. **이용 약관**: 각 사이트 약관 검토
5. **데이터 사용 범위**: 학습/연구 목적 명시

### 윤리적 크롤링 원칙

- User-Agent 명시
- 요청 속도 제한 (Rate Limiting)
- 캐싱 활용 (중복 요청 방지)
- 에러 발생 시 재시도 제한

### 프로젝트 연계
- **Week 2**: 크롤링 스크립트 작성
- **산출물**: 데이터 수집 코드 + 수집된 데이터셋

---

## 12. 핵심 조항 추출 전략

### 문제 정의
- 계약서/판례에서 중요 조항을 자동으로 찾아야 함
- "손해배상", "계약조건", "해지조항" 등 핵심 정보 추출
- 순수 NER은 학습 필요, 순수 LLM은 느리고 비쌈

### 선택: 하이브리드 접근 (규칙 기반 + LLM 검증)

### 선택 이유

1. **규칙 기반 (1단계)**:
   - 정규표현식으로 후보 조항 빠르게 추출
   - "제X조", "손해배상", "위약금" 등 키워드 탐지
   - 빠르고 비용 없음

2. **LLM 검증 (2단계)**:
   - 추출된 후보가 실제 중요 조항인지 확인
   - 문맥 분석으로 정확도 향상
   - 비용은 후보만 검증하므로 절감

### 대안 비교

| 접근 방법 | 정확도 | 속도 | 비용 | 구현 난이도 | 선택 |
|----------|--------|------|------|------------|------|
| 순수 규칙 기반 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 무료 | ⭐⭐⭐⭐ | ❌ |
| 순수 NER 모델 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 무료 | ⭐⭐ | ❌ (학습 필요) |
| 순수 LLM | ⭐⭐⭐⭐⭐ | ⭐⭐ | $$$ | ⭐⭐⭐⭐⭐ | ❌ (비쌈) |
| **하이브리드** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | $ | ⭐⭐⭐⭐ | ✅ |

### 조항 분류 체계

**카테고리**:
1. 손해배상 조항
2. 계약 조건 및 기간
3. 해지 조항
4. 비밀유지 조항
5. 권리 및 의무
6. 분쟁 해결 방법

### 처리 파이프라인

1. **키워드 사전 구축**: 법률 용어 데이터베이스
2. **규칙 기반 추출**: 정규표현식으로 후보 탐지
3. **LLM 검증**: 문맥 분석으로 정확도 확인
4. **분류**: 카테고리별로 조항 정리
5. **구조화 출력**: JSON 형식으로 반환

### 평가 방법
- **메트릭**: Precision, Recall, F1-Score
- **목표**: Precision > 90%, Recall > 85%

### 프로젝트 연계
- **Week 3**: 조항 추출 모듈 개발
- **산출물**: 조항 추출 모듈 코드

---

## 13. 리스크 분석 및 시각화

### 문제 정의
- 계약서/판례에서 법적 리스크를 자동으로 탐지해야 함
- 리스크 점수를 정량화해야 함
- 변호사/기업이 이해하기 쉽게 시각화해야 함

### 선택: 3단계 리스크 분석 + Plotly 시각화

### 선택 이유

**3단계 분석**:
1. **키워드 기반**: 리스크 키워드 사전으로 1차 필터링
2. **TF-IDF**: 키워드 중요도 계산
3. **LLM 문맥 분석**: 실제 리스크인지 판단

**Plotly 시각화**:
1. **인터랙티브**: 확대, 축소, 호버 정보 제공
2. **Streamlit 통합**: 쉽게 연동 가능
3. **다양한 차트**: 히트맵, 바 차트, 파이 차트 지원

### 대안 비교

| 도구 | 인터랙티브 | Streamlit 통합 | 학습 난이도 | 차트 종류 | 선택 |
|------|-----------|---------------|------------|----------|------|
| **Plotly** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ |
| Matplotlib | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ |
| Seaborn | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ |
| D3.js | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ (복잡) |

### 리스크 키워드 사전

**높은 리스크**:
- "손해배상", "위약금", "독점", "불공정", "강제집행"

**중간 리스크**:
- "해지", "변경", "양도", "제한"

**낮은 리스크**:
- "협의", "통지", "동의"

### 리스크 점수 계산

점수 = (키워드 빈도 × 가중치) + (TF-IDF 점수) + (LLM 판단 점수)

**범위**: 0-100 (0: 안전, 100: 매우 위험)

### 대시보드 구성

1. **리스크 히트맵**: 문서 전체에서 리스크 분포
2. **키워드 클라우드**: 주요 리스크 키워드 시각화
3. **카테고리별 바 차트**: 조항 유형별 리스크
4. **시계열 분석**: 날짜별 리스크 변화 (판례)

### 프로젝트 연계
- **Week 5**: 대시보드 UI 개발
- **산출물**: 리스크 분석 및 시각화 대시보드

---

## 14. UI/UX 프레임워크 선택

### 문제 정의
- 5주차에 웹 UI 구현 필요
- 7주 프로젝트이므로 빠른 개발 필요
- Python 백엔드와 쉽게 통합되어야 함

### 선택: Streamlit (7주 프로젝트용)

### 선택 이유

1. **개발 속도**: 1시간 만에 프로토타입 가능
2. **Python만 사용**: HTML/CSS/JS 불필요
3. **배포 간편**: Streamlit Cloud 무료 제공
4. **컴포넌트 풍부**: 파일 업로드, 차트, 폼 등 내장
5. **학습 곡선 낮음**: 튜토리얼 풍부

### 대안 비교

| 프레임워크 | 개발 속도 | 커스터마이징 | 프로 느낌 | 학습 난이도 | 배포 | 선택 |
|----------|----------|------------|---------|-----------|------|------|
| **Streamlit** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ |
| Gradio | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | △ (ML 특화) |
| FastAPI + React | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ❌ (시간 소요) |
| Flask + Bootstrap | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ❌ |

### Streamlit 주요 기능

1. **입력 위젯**: 텍스트, 파일 업로드, 선택박스
2. **출력**: 텍스트, 표, 차트, 이미지
3. **레이아웃**: 사이드바, 컬럼, 탭, 확장 패널
4. **상태 관리**: session_state로 상태 유지
5. **캐싱**: @st.cache_data로 성능 최적화

### UI 구조

**메인 페이지**:
- 문서 업로드 (PDF, TXT)
- 기능 선택 (요약, Q&A, 조항 추출, 리스크 분석)

**결과 페이지**:
- 텍스트 결과 (요약, Q&A 답변)
- 표 형태 (조항 추출 결과)
- 시각화 (리스크 분석 대시보드)

**사이드바**:
- LLM 모델 선택
- 파라미터 설정 (Temperature, Max tokens)
- 검색 옵션 (Top-K, Threshold)

### 프로젝트 연계
- **Week 5**: Streamlit UI 개발
- **산출물**: Streamlit UI 코드 + 실행 화면 캡처

---

## 15. 배포 및 인프라 전략

### 문제 정의
- 7주차에 클라우드 배포 필요
- Docker 컨테이너화 필요
- 환경 변수 (API Key) 안전 관리 필요

### 선택: Streamlit Cloud (무료) 또는 Render ($7/월)

### 선택 이유

**Streamlit Cloud**:
1. **무료**: GitHub 연동만으로 무료 배포
2. **자동 배포**: Git push 시 자동 재배포
3. **환경 변수**: Secrets 관리 기능 내장
4. **간편함**: 클릭 몇 번으로 배포 완료

**Render** (대안):
1. **더 많은 리소스**: CPU, 메모리 더 많음
2. **백그라운드 작업**: 크롤링 스크립트 실행 가능
3. **DB 통합**: PostgreSQL, Redis 제공
4. **$7/월**: 저렴함

### 대안 비교

| 플랫폼 | 비용 | 설정 난이도 | 리소스 | 자동 배포 | DB 지원 | 선택 |
|--------|------|-----------|-------|----------|--------|------|
| **Streamlit Cloud** | 무료 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ | ✅ |
| **Render** | $7/월 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ | ✅ |
| Heroku | $7/월 | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ | △ |
| AWS EC2 | $10-50/월 | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ✅ | ❌ (복잡) |
| Vercel | 무료-$20/월 | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ | ❌ (Next.js 특화) |

### Docker 컨테이너화

**Dockerfile 구조**:
1. Python 베이스 이미지
2. requirements.txt 복사 및 설치
3. 앱 코드 복사
4. Streamlit 실행

**장점**:
- 환경 일관성 (로컬 = 프로덕션)
- 이식성 (어디서나 실행 가능)
- 버전 관리 용이

### 환경 변수 관리

**로컬**:
- `.env` 파일 + `python-dotenv`

**배포**:
- Streamlit Cloud: Secrets 기능
- Render: Environment Variables 설정
- `.env` 파일은 절대 Git에 커밋하지 않음 (`.gitignore` 추가)

### CI/CD (선택사항)

**GitHub Actions**:
- Push 시 자동 테스트
- 테스트 통과 시 자동 배포
- 무료 (Public repo)

### 프로젝트 연계
- **Week 7**: Docker + 클라우드 배포
- **산출물**: 배포된 서비스 URL + README 배포 가이드

---

## 16. 데이터베이스 아키텍처

### 문제 정의
- 벡터 임베딩만으로는 부족
- 문서 메타데이터, 사용자 쿼리 로그 저장 필요
- 자주 묻는 질문 캐싱으로 API 비용 절감 필요

### 선택: 3-Tier DB 아키텍처

1. **벡터 DB**: ChromaDB (임베딩 저장)
2. **관계형 DB**: SQLite (메타데이터, 사용자, 쿼리 로그)
3. **캐싱 DB**: Redis (FAQ 캐싱)

### 선택 이유

**ChromaDB (벡터)**:
- 유사도 검색 특화

**SQLite (관계형)**:
1. **설치 불필요**: Python 내장
2. **파일 기반**: 단일 파일로 관리 간편
3. **소규모 적합**: 동시 사용자 적을 때 최적
4. **확장 가능**: 추후 PostgreSQL로 전환 쉬움

**Redis (캐싱)**:
1. **In-Memory**: 매우 빠름 (ms 단위)
2. **TTL 지원**: 자동 만료 설정
3. **API 비용 절감**: 동일 질문은 캐시에서 응답

### 대안 비교

| DB 유형 | SQLite | PostgreSQL | MySQL | MongoDB |
|---------|--------|-----------|-------|---------|
| 설치 | 불필요 | 필요 | 필요 | 필요 |
| 동시성 | 낮음 | 높음 | 높음 | 높음 |
| 학습 난이도 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |
| 소규모 프로젝트 | ✅ | ❌ | ❌ | ❌ |
| 대규모 확장 | ❌ | ✅ | ✅ | ✅ |

### 데이터베이스 스키마

**documents 테이블**:
- id, title, content, source (법제처/대법원), date, category (형법/형소법)

**queries 테이블**:
- id, user_id, question, answer, llm_used (GPT-4o/Claude), timestamp, response_time, cost

**users 테이블** (선택사항):
- id, name, email, subscription_tier (무료/유료)

**cache (Redis)**:
- Key: question_hash
- Value: answer (JSON)
- TTL: 24시간

### 확장 전략

**소규모 (< 1,000 사용자)**:
- SQLite + Redis (로컬)

**중규모 (1,000-10,000 사용자)**:
- PostgreSQL + Redis (클라우드)

**대규모 (10,000+ 사용자)**:
- PostgreSQL (Master-Slave) + Redis Cluster

### 프로젝트 연계
- **Week 2**: SQLite 스키마 설계
- **Week 4**: ChromaDB 인덱싱
- **Week 6**: Redis 캐싱 적용

---

## 17. 비용 최적화 전략

### 문제 정의
- LLM API 비용이 빠르게 증가할 수 있음
- 1,000 쿼리 시 $50-100 소요 가능
- 비용 절감 전략 필요

### 선택: 5단계 비용 최적화

### 최적화 전략

#### 1. 캐싱 (70% 절감 효과)

**Redis 캐싱**:
- 동일 질문은 캐시에서 응답 (API 호출 0)
- FAQ는 영구 캐시
- 일반 질문은 24시간 TTL

**효과**: API 호출 70% 감소

#### 2. 모델 계층화 (50% 절감 효과)

**작업별 모델 선택**:
- 간단한 요약: GPT-4o-mini ($0.15/1M)
- 복잡한 Q&A: GPT-4o ($2.50/1M)
- 리스크 분석: Claude 3.5 Sonnet ($3/1M)

**효과**: 평균 비용 50% 감소

#### 3. 프롬프트 압축 (30% 절감 효과)

**기법**:
- 불필요한 컨텍스트 제거
- 문서 요약 후 전달 (긴 문서 → 요약본)
- 토큰 수 제한 (max_tokens 설정)

**효과**: 입력 토큰 30% 감소

#### 4. 배치 처리 (10% 절감 효과)

- 여러 요청을 한 번에 처리
- API 호출 횟수 감소

#### 5. 토큰 예산 설정

**사용자당 한도**:
- 무료 사용자: 100 쿼리/월
- 유료 사용자: 1,000 쿼리/월

### 비용 계산 예시

**Before (최적화 전)**:
- 1,000 쿼리 × 5,000 토큰 평균 × $2.50/1M = $12.50

**After (최적화 후)**:
- 캐싱: 700 쿼리는 캐시 응답 (비용 0)
- 남은 300 쿼리:
  - 200 쿼리 → GPT-4o-mini ($0.15/1M) = $0.15
  - 100 쿼리 → GPT-4o ($2.50/1M) = $1.25
- **총 비용**: $1.40 (88% 절감)

### 모니터링

**추적 메트릭**:
- 일일/월별 API 비용
- 쿼리당 평균 비용
- 캐시 히트율
- 모델별 사용 비율

**알림 설정**:
- 월 비용 $50 초과 시 알림
- 캐시 히트율 < 60% 시 알림

### 프로젝트 연계
- **Week 6**: 비용 최적화 적용
- **산출물**: 최적화 전/후 비용 비교 보고서

---

## 18. 프롬프트 엔지니어링 라이브러리

### 문제 정의
- 3주차에서 프롬프트 작성이 핵심
- 작업별로 다른 프롬프트 필요 (요약, Q&A, 조항 추출)
- 프롬프트 버전 관리 및 A/B 테스트 필요

### 선택: 템플릿 기반 프롬프트 라이브러리

### 프롬프트 카테고리

#### 1. 요약 프롬프트 (Summarization)

**목표**: 법률 문서를 5줄 이내로 요약

**출력 형식**:
- 핵심 쟁점
- 법원의 판단 (판례) 또는 주요 조항 (법령)
- 근거 법령

#### 2. Q&A 프롬프트 (Question Answering)

**목표**: 사용자 질문에 정확하게 답변

**구성**:
- Few-Shot Examples (3개)
- Constitutional AI 원칙
- 검색된 컨텍스트
- 출력 형식 (답변 + 출처 + 면책 조항)

#### 3. 조항 추출 프롬프트 (Clause Extraction)

**목표**: 계약서에서 핵심 조항 추출

**출력 형식**: JSON
- 손해배상
- 계약조건
- 해지조항
- 비밀유지

#### 4. 리스크 분석 프롬프트 (Risk Analysis)

**목표**: 문서에서 법적 리스크 탐지

**출력**:
- 리스크 점수 (0-100)
- 리스크 카테고리
- 근거 문장

### 프롬프트 버전 관리

**구조**:
- `prompts/` 폴더
- 각 프롬프트는 별도 파일 (YAML 또는 JSON)
- Git으로 버전 추적

**예시 구조**:
```
prompts/
├── summarization_v1.yaml
├── summarization_v2.yaml
├── qa_v1.yaml
├── clause_extraction_v1.yaml
└── risk_analysis_v1.yaml
```

### A/B 테스트

**비교 항목**:
- Temperature (0.0 vs 0.7)
- Max tokens (500 vs 1000)
- Few-Shot (0 vs 3 vs 5)
- 프롬프트 구조 (간단 vs 상세)

**평가 메트릭**:
- 정확도
- 응답 품질 (수동 평가)
- 비용
- 응답 시간

### 프로젝트 연계
- **Week 3**: 프롬프트 라이브러리 구축
- **Week 6**: A/B 테스트로 최적 프롬프트 선택

---

## 19. 법률 도메인 특화 전략

### 문제 정의
- 법률 용어는 일반 언어와 다름
- 법령 계층 구조 존재 (헌법 > 법률 > 시행령)
- 판례 효력 차이 존재 (대법원 > 고등법원)
- 법령 개정 이력 관리 필요

### 선택: 4단계 도메인 특화

### 1. 법률 용어 정규화 사전

**동의어 그룹**:
- "훔치다", "절취하다", "영득하다" → "절도"
- "사기", "기망", "편취" → "사기죄"
- "죽이다", "살해하다" → "살인"

**목적**:
- 검색 recall 향상
- 다양한 표현 통합

### 2. 법령 계층 구조 (메타데이터)

**계층**:
1. 헌법 (최상위)
2. 법률 (형법, 형사소송법)
3. 시행령
4. 시행규칙

**검색 가중치**:
- 헌법: 10
- 법률: 7
- 시행령: 5
- 시행규칙: 3

**목적**: 상위 법령을 우선 검색

### 3. 판례 효력 (메타데이터)

**효력 순서**:
1. 대법원 전원합의체 (가중치 10)
2. 대법원 (가중치 7)
3. 고등법원 (가중치 5)
4. 지방법원 (가중치 3)

**목적**: 효력 높은 판례 우선 제시

### 4. 법령 개정 이력 관리

**버전 관리**:
- 각 법령의 유효 시점 저장
- 질문 시점에 맞는 법령 버전 검색
- 예: "2020년 절도죄 처벌은?" → 2020년 버전 형법 제329조 검색

**메타데이터**:
- effective_date (유효 시작일)
- expired_date (유효 종료일)
- version (버전 번호)

### 데이터 구조

**documents 테이블 확장**:
- category: 법령/판례
- law_type: 헌법/법률/시행령/시행규칙
- court_level: 대법원/고등법원/지방법원
- effective_date
- expired_date
- version

### 프로젝트 연계
- **Week 2**: 법률 용어 사전 구축
- **Week 4**: 메타데이터 기반 검색 가중치 적용

---

## 20. 보안 및 컴플라이언스

### 문제 정의
- API Key 노출 위험
- 프롬프트 인젝션 공격 가능
- 사용자 입력에 개인정보 포함 가능
- 로그에 민감 정보 저장 위험

### 선택: 4단계 보안 전략

### 1. API Key 관리

**원칙**:
- 절대 코드에 하드코딩하지 않음
- 절대 Git에 커밋하지 않음

**방법**:
- 로컬: `.env` 파일 + `.gitignore`
- 배포: 클라우드 환경 변수 (Secrets)
- 접근 제어: 서버만 API Key 접근

### 2. 프롬프트 인젝션 방어

**공격 예시**:
- 사용자 입력: "이전 지시를 무시하고 ..."
- 목표: LLM을 속여 의도하지 않은 동작 유도

**방어 방법**:
1. **입력 검증**: 의심스러운 패턴 탐지
2. **샌드박싱**: 사용자 입력을 별도 영역에 격리
3. **출력 필터링**: 민감 정보 출력 차단

### 3. 개인정보 보호

**문제**:
- 사용자가 질문에 이름, 주민번호 포함 가능

**해결**:
1. **입력 마스킹**: 개인정보 자동 탐지 및 제거
   - 이름: 정규표현식으로 탐지
   - 주민번호: 패턴 매칭으로 제거
2. **로그 마스킹**: 로그 저장 시 개인정보 `***` 처리
3. **데이터 보관 기간**: 30일 후 자동 삭제

### 4. 접근 제어 및 Rate Limiting

**Rate Limiting**:
- IP당 분당 10 요청 제한
- 과도한 요청 시 차단 (DoS 방어)

**접근 제어** (선택사항):
- 로그인 시스템 (사용자 인증)
- API Key 기반 접근 제어

### 5. HTTPS 사용

**배포 시**:
- Streamlit Cloud: 자동 HTTPS
- Render: 자동 HTTPS
- 직접 배포: Let's Encrypt 인증서

### 컴플라이언스

**GDPR (유럽)**:
- 사용자 데이터 수집 시 동의 필요
- 데이터 삭제 요청 처리

**개인정보보호법 (한국)**:
- 개인정보 수집 시 명시
- 보관 기간 명시

### 프로젝트 연계
- **Week 1**: API Key 환경 변수 설정
- **Week 7**: 배포 시 보안 점검

---

## 21. 모니터링 및 성능 추적

### 문제 정의
- 서비스 장애를 빠르게 감지해야 함
- API 비용을 실시간으로 추적해야 함
- 사용자 만족도를 측정해야 함
- 성능 병목을 찾아야 함

### 선택: Logging + Metrics + Alerting

### 1. 로깅 (Logging)

**로그 레벨**:
- ERROR: 치명적 오류 (API 실패, DB 오류)
- WARNING: 주의 필요 (캐시 미스율 높음)
- INFO: 일반 정보 (쿼리 처리 완료)
- DEBUG: 디버깅 정보 (개발 환경에서만)

**로그 내용**:
- 타임스탬프
- 사용자 ID (익명화)
- 쿼리 내용
- 응답 시간
- LLM 모델 사용
- API 비용
- 에러 메시지 (있을 경우)

**도구**:
- Python logging 모듈
- 파일 기반 로깅 (logs/ 폴더)
- 로그 로테이션 (일별 또는 크기별)

### 2. 메트릭 (Metrics)

**추적 메트릭**:

| 카테고리 | 메트릭 | 목표값 |
|---------|--------|--------|
| 성능 | 평균 응답 시간 | < 5초 |
| 성능 | 95 percentile 응답 시간 | < 10초 |
| 비용 | 일일 API 비용 | < $5 |
| 품질 | 에러율 | < 1% |
| 사용성 | 캐시 히트율 | > 60% |
| 만족도 | 사용자 평점 (1-5) | > 4.0 |

**도구**:
- 간단한 방법: SQLite에 메트릭 저장
- 고급 방법: Prometheus + Grafana

### 3. 알림 (Alerting)

**알림 조건**:
- 에러율 > 5%
- 응답 시간 > 30초
- 일일 비용 > $10
- 캐시 히트율 < 40%

**알림 채널**:
- 이메일
- Slack (선택)
- SMS (중요한 경우)

### 4. 대시보드

**구성**:
- 일일 쿼리 수
- 평균 응답 시간 (차트)
- API 비용 추이
- 에러 로그 (최근 10개)
- 인기 질문 Top 10
- 사용자 만족도

**도구**:
- Streamlit 자체 대시보드 (간단)
- Grafana (고급)

### 5. A/B 테스트 추적

**실험 추적**:
- 실험 ID
- 대조군 vs 실험군
- 전환율, 만족도, 응답 시간 비교

### 프로젝트 연계
- **Week 3**: 로깅 시스템 구축
- **Week 6**: 메트릭 추적 및 대시보드 구축
- **Week 7**: 알림 시스템 설정 (배포 후)

---

## 전체 기술 스택 요약

### 데이터 수집 (Week 2)
- **크롤링**: BeautifulSoup (정적), Selenium (동적)
- **OCR**: PaddleOCR (한글), pdfplumber (PDF 파싱)

### 데이터 처리 (Week 2-4)
- **청킹**: LangChain RecursiveCharacterTextSplitter (500자, 50 overlap)
- **임베딩**: jhgan/ko-sroberta-multitask (무료) 또는 OpenAI text-embedding-3-small (유료)
- **벡터 DB**: ChromaDB (기본), FAISS (대안)

### LLM 및 RAG (Week 3-4)
- **LLM**: GPT-4o (Q&A), GPT-4o-mini (요약), Claude 3.5 Sonnet (리스크 분석)
- **프롬프트**: 3-Shot Learning + Constitutional AI
- **검색**: Semantic Search (Cosine Similarity)

### 핵심 기능 (Week 3-5)
- **요약**: LLM 기반 문서 요약
- **Q&A**: RAG + Few-Shot + Constitutional AI
- **조항 추출**: 규칙 기반 + LLM 검증 하이브리드
- **리스크 분석**: 키워드 + TF-IDF + LLM 문맥 분석

### UI/UX (Week 5)
- **프론트엔드**: Streamlit
- **시각화**: Plotly (인터랙티브 차트)

### 데이터베이스 (Week 2-6)
- **벡터 DB**: ChromaDB
- **관계형 DB**: SQLite (소규모), PostgreSQL (확장)
- **캐싱**: Redis

### 배포 및 인프라 (Week 7)
- **컨테이너**: Docker
- **배포**: Streamlit Cloud (무료), Render ($7/월)
- **CI/CD**: GitHub Actions (선택)

### 최적화 및 보안 (Week 6-7)
- **비용 최적화**: 캐싱 + 모델 계층화 + 프롬프트 압축
- **보안**: 환경 변수, 프롬프트 인젝션 방어, 개인정보 마스킹
- **모니터링**: Logging + Metrics + Alerting

---

## 평가 및 개선 계획

### Week 6: 멀티 LLM 비교 실험

**변호사시험 기반 평가**:
- 100문제 (형법 50 + 형소법 50)
- GPT-4o vs GPT-4o-mini vs Claude 3.5 Sonnet vs Llama 3.1
- 메트릭: 정확도, 속도, 비용, 신뢰도, 일관성

### A/B 테스트 계획

1. **청킹 전략**: 300자 vs 500자 vs 800자
2. **임베딩 모델**: ko-sroberta vs OpenAI text-embedding-3
3. **Few-Shot**: 0-shot vs 3-shot vs 5-shot
4. **검색 전략**: Semantic only vs Hybrid (Semantic + BM25)

### 평가 메트릭

**검색 (Retrieval)**:
- Precision@5: 검색된 5개 중 관련 문서 비율
- Recall@5: 관련 문서 중 검색된 비율
- MRR (Mean Reciprocal Rank)

**생성 (Generation)**:
- Faithfulness: 검색 문서 기반 답변 비율
- Citation Rate: 출처 명시 비율
- Hallucination Rate: 환각 발생 비율

**사용자 경험**:
- Response Time: 평균 응답 시간
- User Satisfaction: 사용자 만족도 (1-5)

---

## 참고 자료

### 논문
1. Lewis et al. (2020) - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
2. Anthropic (2022) - "Constitutional AI: Harmlessness from AI Feedback"
3. Gao et al. (2024) - "Retrieval-Augmented Generation for Large Language Models: A Survey"

### 라이브러리 문서
1. LangChain - https://python.langchain.com/
2. ChromaDB - https://docs.trychroma.com/
3. Streamlit - https://docs.streamlit.io/
4. OpenAI API - https://platform.openai.com/docs/
5. Anthropic Claude - https://docs.anthropic.com/

### 한국어 NLP
1. KorNLI, KorSTS 벤치마크
2. Hugging Face Korean Models
3. KLUE Benchmark

### 법률 데이터
1. 법제처 - https://www.law.go.kr/
2. 대법원 종합법률정보 - https://glaw.scourt.go.kr/
3. 공공데이터포털 - https://www.data.go.kr/

---

## 프로젝트 주차별 매핑

| 주차 | 관련 섹션 | 핵심 작업 |
|------|----------|----------|
| Week 1 | 1, 8, 20 | 환경 설정, API 테스트 |
| Week 2 | 3, 10, 11, 19 | 크롤링, OCR, 전처리 |
| Week 3 | 6, 7, 8, 12, 18 | LLM 연동, 프롬프트 설계 |
| Week 4 | 2, 4, 5, 16 | 임베딩, 벡터DB, RAG 구축 |
| Week 5 | 13, 14 | UI 개발, 시각화 |
| Week 6 | 9, 17 | LLM 비교, 비용 최적화 |
| Week 7 | 15, 20, 21 | 배포, 보안, 모니터링 |

---

**문서 버전**: 2.0
**작성일**: 2025-01-28
**다음 업데이트**: 프로젝트 진행 중 추가 개선 사항 반영
